Creating network "docker-compose_kafka_dev_sandbox" with driver "bridge"
Creating zookeeper ... 
[1A[2KCreating zookeeper ... [32mdone[0m[1BCreating kafka1    ... 
[1A[2KCreating kafka1    ... [32mdone[0m[1BCreating schema-registry ... 
[1A[2KCreating schema-registry ... [32mdone[0m[1BCreating kafka-ui        ... 
Creating connect         ... 
[2A[2KCreating kafka-ui        ... [32mdone[0m[2B[1A[2KCreating connect         ... [32mdone[0m[1BAttaching to zookeeper, kafka1, schema-registry, kafka-ui, connect
[36mconnect            |[0m [Docker-Connect] Installing More Kafka Connect Transformations Connector ...
[32mkafka1             |[0m ===> User
[35mschema-registry    |[0m ===> User
[35mschema-registry    |[0m uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
[34mzookeeper          |[0m ===> User
[32mkafka1             |[0m uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
[34mzookeeper          |[0m uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
[32mkafka1             |[0m ===> Configuring ...
[35mschema-registry    |[0m ===> Configuring ...
[34mzookeeper          |[0m ===> Configuring ...
[36mconnect            |[0m Running in a "--no-prompt" mode 
[34mzookeeper          |[0m ===> Running preflight checks ... 
[34mzookeeper          |[0m ===> Check if /var/lib/zookeeper/data is writable ...
[34mzookeeper          |[0m ===> Check if /var/lib/zookeeper/log is writable ...
[35mschema-registry    |[0m ===> Running preflight checks ... 
[35mschema-registry    |[0m ===> Check if Zookeeper is healthy ...
[32mkafka1             |[0m ===> Running preflight checks ... 
[32mkafka1             |[0m ===> Check if /var/lib/kafka/data is writable ...
[33mkafka-ui           |[0m 15:45:57.047 [main] INFO  org.springframework.core.KotlinDetector - Kotlin reflection implementation not found at runtime, related features won't be available.
[34mzookeeper          |[0m ===> Launching ... 
[34mzookeeper          |[0m ===> Printing /var/lib/zookeeper/data/myid 
[34mzookeeper          |[0m 1===> Launching zookeeper ... 
[32mkafka1             |[0m ===> Check if Zookeeper is healthy ...
[36mconnect            |[0m Implicit acceptance of the license below:  
[36mconnect            |[0m Confluent Software License 
[36mconnect            |[0m https://www.confluent.io/software-evaluation-license 
[36mconnect            |[0m Downloading component connect-transformations 1.4.0, provided by Confluent, Inc. from Confluent Hub and installing into /usr/share/confluent-hub-components 
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=schema-registry
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.9.1
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/usr/share/java/cp-base-new/gson-2.8.6.jar:/usr/share/java/cp-base-new/netty-handler-4.1.48.Final.jar:/usr/share/java/cp-base-new/jackson-module-paranamer-2.10.5.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.10.5.jar:/usr/share/java/cp-base-new/netty-resolver-4.1.48.Final.jar:/usr/share/java/cp-base-new/netty-transport-4.1.48.Final.jar:/usr/share/java/cp-base-new/common-utils-6.0.1.jar:/usr/share/java/cp-base-new/netty-transport-native-epoll-4.1.48.Final.jar:/usr/share/java/cp-base-new/jackson-core-2.10.5.jar:/usr/share/java/cp-base-new/snakeyaml-1.26.jar:/usr/share/java/cp-base-new/jackson-databind-2.10.5.jar:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.2.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.10.5.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.30.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/zookeeper-3.5.8.jar:/usr/share/java/cp-base-new/netty-codec-4.1.48.Final.jar:/usr/share/java/cp-base-new/kafka-clients-6.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.10.5.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.10.5.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-0.9.1.jar:/usr/share/java/cp-base-new/scala-library-2.13.2.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.2.jar:/usr/share/java/cp-base-new/netty-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/jackson-annotations-2.10.5.jar:/usr/share/java/cp-base-new/netty-transport-native-unix-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.1.6.jar:/usr/share/java/cp-base-new/lz4-java-1.7.1.jar:/usr/share/java/cp-base-new/kafka_2.13-6.0.1-ccs.jar:/usr/share/java/cp-base-new/netty-buffer-4.1.48.Final.jar:/usr/share/java/cp-base-new/snappy-java-1.1.7.3.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.5.8.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/utility-belt-6.0.1.jar:/usr/share/java/cp-base-new/zstd-jni-1.4.4-7.jar
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=4.19.121-linuxkit
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=appuser
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/home/appuser
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/home/appuser
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=182MB
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=2998MB
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=188MB
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@72b6cbcc
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m   .   ____          _            __ _ _
[33mkafka-ui           |[0m  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
[33mkafka-ui           |[0m ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
[33mkafka-ui           |[0m  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
[33mkafka-ui           |[0m   '  |____| .__|_| |_|_| |_\__, | / / / /
[33mkafka-ui           |[0m  =========|_|==============|___/=/_/_/_/
[33mkafka-ui           |[0m  :: Spring Boot ::        (v2.2.4.RELEASE)
[33mkafka-ui           |[0m 
[36mconnect            |[0m Adding installation directory to plugin path in the following files: 
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
[36mconnect            |[0m   /etc/kafka/connect-distributed.properties 
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket error occurred: zookeeper/172.28.0.2:2181: Connection refused
[36mconnect            |[0m   /etc/kafka/connect-standalone.properties 
[36mconnect            |[0m   /etc/schema-registry/connect-avro-distributed.properties 
[36mconnect            |[0m   /etc/schema-registry/connect-avro-standalone.properties 
[36mconnect            |[0m  
[36mconnect            |[0m Completed 
[36mconnect            |[0m \[Docker-Connect] Start waiting for installation of Connect Transforms ... 
[36mconnect            |[0m [Docker-Connect] Installing RSS Connector ...
[33mkafka-ui           |[0m 15:45:59.615 [main] INFO  com.provectus.kafka.ui.KafkaUiApplication - Starting KafkaUiApplication on e3ce176159c4 with PID 1 (/kafka-ui-api.jar started by root in /)
[33mkafka-ui           |[0m 15:45:59.617 [main] DEBUG com.provectus.kafka.ui.KafkaUiApplication - Running with Spring Boot v2.2.4.RELEASE, Spring v5.2.3.RELEASE
[33mkafka-ui           |[0m 15:45:59.624 [main] INFO  com.provectus.kafka.ui.KafkaUiApplication - No active profile set, falling back to default profiles: default
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=kafka1
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.9.1
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/usr/share/java/cp-base-new/gson-2.8.6.jar:/usr/share/java/cp-base-new/netty-handler-4.1.48.Final.jar:/usr/share/java/cp-base-new/jackson-module-paranamer-2.10.5.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.10.5.jar:/usr/share/java/cp-base-new/netty-resolver-4.1.48.Final.jar:/usr/share/java/cp-base-new/netty-transport-4.1.48.Final.jar:/usr/share/java/cp-base-new/common-utils-6.0.1.jar:/usr/share/java/cp-base-new/netty-transport-native-epoll-4.1.48.Final.jar:/usr/share/java/cp-base-new/jackson-core-2.10.5.jar:/usr/share/java/cp-base-new/snakeyaml-1.26.jar:/usr/share/java/cp-base-new/jackson-databind-2.10.5.jar:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.2.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.10.5.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.30.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/zookeeper-3.5.8.jar:/usr/share/java/cp-base-new/netty-codec-4.1.48.Final.jar:/usr/share/java/cp-base-new/kafka-clients-6.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.10.5.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.10.5.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-0.9.1.jar:/usr/share/java/cp-base-new/scala-library-2.13.2.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.2.jar:/usr/share/java/cp-base-new/netty-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/jackson-annotations-2.10.5.jar:/usr/share/java/cp-base-new/netty-transport-native-unix-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.1.6.jar:/usr/share/java/cp-base-new/lz4-java-1.7.1.jar:/usr/share/java/cp-base-new/kafka_2.13-6.0.1-ccs.jar:/usr/share/java/cp-base-new/netty-buffer-4.1.48.Final.jar:/usr/share/java/cp-base-new/snappy-java-1.1.7.3.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.5.8.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/utility-belt-6.0.1.jar:/usr/share/java/cp-base-new/zstd-jni-1.4.4-7.jar
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=4.19.121-linuxkit
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=appuser
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/home/appuser
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/home/appuser
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=182MB
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=2998MB
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=188MB
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@72b6cbcc
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=
[32mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
[32mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket error occurred: zookeeper/172.28.0.2:2181: Connection refused
[34mzookeeper          |[0m [2021-01-30 15:46:00,090] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[34mzookeeper          |[0m [2021-01-30 15:46:00,136] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[34mzookeeper          |[0m [2021-01-30 15:46:00,136] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[34mzookeeper          |[0m [2021-01-30 15:46:00,158] ERROR Invalid configuration, only one server specified (ignoring) (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket error occurred: zookeeper/172.28.0.2:2181: Connection refused
[34mzookeeper          |[0m [2021-01-30 15:46:00,181] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[34mzookeeper          |[0m [2021-01-30 15:46:00,181] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[34mzookeeper          |[0m [2021-01-30 15:46:00,182] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[34mzookeeper          |[0m [2021-01-30 15:46:00,183] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[34mzookeeper          |[0m [2021-01-30 15:46:00,199] INFO Log4j 1.2 jmx support found and enabled. (org.apache.zookeeper.jmx.ManagedUtil)
[34mzookeeper          |[0m [2021-01-30 15:46:00,237] INFO Reading configuration from: /etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[34mzookeeper          |[0m [2021-01-30 15:46:00,238] INFO clientPortAddress is 0.0.0.0:2181 (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[34mzookeeper          |[0m [2021-01-30 15:46:00,239] INFO secureClientPort is not set (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[34mzookeeper          |[0m [2021-01-30 15:46:00,240] ERROR Invalid configuration, only one server specified (ignoring) (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[34mzookeeper          |[0m [2021-01-30 15:46:00,240] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[34mzookeeper          |[0m [2021-01-30 15:46:00,247] INFO zookeeper.snapshot.trust.empty : false (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[34mzookeeper          |[0m [2021-01-30 15:46:00,281] INFO Server environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,281] INFO Server environment:host.name=zookeeper (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,281] INFO Server environment:java.version=11.0.9.1 (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,281] INFO Server environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,281] INFO Server environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,281] INFO Server environment:java.class.path=/usr/bin/../share/java/kafka/netty-buffer-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.30.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jackson-module-paranamer-2.10.5.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/kafka-tools-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.30.jar:/usr/bin/../share/java/kafka/kafka-streams-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-core-2.10.5.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.30.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jackson-databind-2.10.5.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.2.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/kafka/zookeeper-3.5.8.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jersey-common-2.30.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-clients-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/connect-transforms-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.2.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.2.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-test.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-scaladoc.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-sources.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.18.4.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-javadoc.jar:/usr/bin/../share/java/kafka/connect-mirror-client-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/connect-file-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/usr/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/usr/bin/../share/java/kafka/jersey-client-2.30.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.30.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/javassist-3.25.0-GA.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/connect-json-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-mirror-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/kafka/netty-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-test-sources.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/connect-api-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.50.Final.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:os.version=4.19.121-linuxkit (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:user.name=appuser (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,282] INFO Server environment:user.home=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,283] INFO Server environment:user.dir=/home/appuser (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,283] INFO Server environment:os.memory.free=496MB (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,283] INFO Server environment:os.memory.max=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,283] INFO Server environment:os.memory.total=512MB (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,288] INFO minSessionTimeout set to 4000 (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,289] INFO maxSessionTimeout set to 40000 (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,290] INFO Created server with tickTime 2000 minSessionTimeout 4000 maxSessionTimeout 40000 datadir /var/lib/zookeeper/log/version-2 snapdir /var/lib/zookeeper/data/version-2 (org.apache.zookeeper.server.ZooKeeperServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,338] INFO Logging initialized @2316ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
[33mkafka-ui           |[0m 15:46:00.586 [background-preinit] WARN  org.springframework.http.converter.json.Jackson2ObjectMapperBuilder - For Jackson Kotlin classes support please add "com.fasterxml.jackson.module:jackson-module-kotlin" to the classpath
[36mconnect            |[0m Running in a "--no-prompt" mode 
[34mzookeeper          |[0m [2021-01-30 15:46:00,669] WARN o.e.j.s.ServletContextHandler@6f01b95f{/,null,UNAVAILABLE} contextPath ends with /* (org.eclipse.jetty.server.handler.ContextHandler)
[34mzookeeper          |[0m [2021-01-30 15:46:00,669] WARN Empty contextPath (org.eclipse.jetty.server.handler.ContextHandler)
[34mzookeeper          |[0m [2021-01-30 15:46:00,716] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 11.0.9.1+1-LTS (org.eclipse.jetty.server.Server)
[34mzookeeper          |[0m [2021-01-30 15:46:00,882] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
[34mzookeeper          |[0m [2021-01-30 15:46:00,882] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
[34mzookeeper          |[0m [2021-01-30 15:46:00,888] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
[34mzookeeper          |[0m [2021-01-30 15:46:00,927] INFO Started o.e.j.s.ServletContextHandler@6f01b95f{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[34mzookeeper          |[0m [2021-01-30 15:46:00,971] INFO Started ServerConnector@2cf3d63b{HTTP/1.1,[http/1.1]}{0.0.0.0:8080} (org.eclipse.jetty.server.AbstractConnector)
[34mzookeeper          |[0m [2021-01-30 15:46:00,978] INFO Started @2956ms (org.eclipse.jetty.server.Server)
[34mzookeeper          |[0m [2021-01-30 15:46:00,979] INFO Started AdminServer on address 0.0.0.0, port 8080 and command URL /commands (org.apache.zookeeper.server.admin.JettyAdminServer)
[34mzookeeper          |[0m [2021-01-30 15:46:00,990] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)
[34mzookeeper          |[0m [2021-01-30 15:46:00,999] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 12 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)
[34mzookeeper          |[0m [2021-01-30 15:46:01,005] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[34mzookeeper          |[0m [2021-01-30 15:46:01,061] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)
[34mzookeeper          |[0m [2021-01-30 15:46:01,072] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[34mzookeeper          |[0m [2021-01-30 15:46:01,081] INFO Snapshotting: 0x0 to /var/lib/zookeeper/data/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)
[32mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
[32mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.28.0.3:42178, server: zookeeper/172.28.0.2:2181
[34mzookeeper          |[0m [2021-01-30 15:46:01,159] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)
[34mzookeeper          |[0m [2021-01-30 15:46:01,166] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[32mkafka1             |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.28.0.2:2181, sessionid = 0x100044b160f0000, negotiated timeout = 40000
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.28.0.4:37244, server: zookeeper/172.28.0.2:2181
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.28.0.2:2181, sessionid = 0x100044b160f0001, negotiated timeout = 40000
[32mkafka1             |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x100044b160f0000 closed
[32mkafka1             |[0m [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100044b160f0000
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x100044b160f0001 closed
[35mschema-registry    |[0m [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100044b160f0001
[32mkafka1             |[0m ===> Launching ... 
[32mkafka1             |[0m ===> Launching kafka ... 
[35mschema-registry    |[0m ===> Check if Kafka is healthy ...
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:host.name=schema-registry
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.version=11.0.9.1
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Azul Systems, Inc.
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/lib/jvm/zulu11-ca
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=/usr/share/java/cp-base-new/gson-2.8.6.jar:/usr/share/java/cp-base-new/netty-handler-4.1.48.Final.jar:/usr/share/java/cp-base-new/jackson-module-paranamer-2.10.5.jar:/usr/share/java/cp-base-new/jackson-datatype-jdk8-2.10.5.jar:/usr/share/java/cp-base-new/netty-resolver-4.1.48.Final.jar:/usr/share/java/cp-base-new/netty-transport-4.1.48.Final.jar:/usr/share/java/cp-base-new/common-utils-6.0.1.jar:/usr/share/java/cp-base-new/netty-transport-native-epoll-4.1.48.Final.jar:/usr/share/java/cp-base-new/jackson-core-2.10.5.jar:/usr/share/java/cp-base-new/snakeyaml-1.26.jar:/usr/share/java/cp-base-new/jackson-databind-2.10.5.jar:/usr/share/java/cp-base-new/slf4j-simple-1.7.30.jar:/usr/share/java/cp-base-new/scala-logging_2.13-3.9.2.jar:/usr/share/java/cp-base-new/jackson-dataformat-yaml-2.10.5.jar:/usr/share/java/cp-base-new/slf4j-api-1.7.30.jar:/usr/share/java/cp-base-new/audience-annotations-0.5.0.jar:/usr/share/java/cp-base-new/zookeeper-3.5.8.jar:/usr/share/java/cp-base-new/netty-codec-4.1.48.Final.jar:/usr/share/java/cp-base-new/kafka-clients-6.0.1-ccs.jar:/usr/share/java/cp-base-new/jackson-dataformat-csv-2.10.5.jar:/usr/share/java/cp-base-new/jackson-module-scala_2.13-2.10.5.jar:/usr/share/java/cp-base-new/scala-java8-compat_2.13-0.9.1.jar:/usr/share/java/cp-base-new/scala-library-2.13.2.jar:/usr/share/java/cp-base-new/scala-reflect-2.13.2.jar:/usr/share/java/cp-base-new/netty-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/paranamer-2.8.jar:/usr/share/java/cp-base-new/metrics-core-2.2.0.jar:/usr/share/java/cp-base-new/jackson-annotations-2.10.5.jar:/usr/share/java/cp-base-new/netty-transport-native-unix-common-4.1.48.Final.jar:/usr/share/java/cp-base-new/scala-collection-compat_2.13-2.1.6.jar:/usr/share/java/cp-base-new/lz4-java-1.7.1.jar:/usr/share/java/cp-base-new/kafka_2.13-6.0.1-ccs.jar:/usr/share/java/cp-base-new/netty-buffer-4.1.48.Final.jar:/usr/share/java/cp-base-new/snappy-java-1.1.7.3.jar:/usr/share/java/cp-base-new/argparse4j-0.7.0.jar:/usr/share/java/cp-base-new/commons-cli-1.4.jar:/usr/share/java/cp-base-new/zookeeper-jute-3.5.8.jar:/usr/share/java/cp-base-new/jopt-simple-5.0.4.jar:/usr/share/java/cp-base-new/utility-belt-6.0.1.jar:/usr/share/java/cp-base-new/zstd-jni-1.4.4-7.jar
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.version=4.19.121-linuxkit
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.name=appuser
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.home=/home/appuser
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/home/appuser
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=182MB
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=2998MB
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=188MB
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@7c29daf3
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.common.X509Util - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.28.0.4:37278, server: zookeeper/172.28.0.2:2181
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.28.0.2:2181, sessionid = 0x100044b160f0002, negotiated timeout = 40000
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x100044b160f0002 closed
[35mschema-registry    |[0m [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100044b160f0002
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Initiating client connection, connectString=zookeeper:2181 sessionTimeout=40000 watcher=io.confluent.admin.utils.ZookeeperConnectionWatcher@fad74ee
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ClientCnxnSocket - jute.maxbuffer value is 4194304 Bytes
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ClientCnxn - zookeeper.request.timeout value is 0. feature enabled=
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error)
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Socket connection established, initiating session, client: /172.28.0.4:37284, server: zookeeper/172.28.0.2:2181
[35mschema-registry    |[0m [main-SendThread(zookeeper:2181)] INFO org.apache.zookeeper.ClientCnxn - Session establishment complete on server zookeeper/172.28.0.2:2181, sessionid = 0x100044b160f0003, negotiated timeout = 40000
[32mkafka1             |[0m [2021-01-30 15:46:03,945] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[32mkafka1             |[0m [2021-01-30 15:46:04,918] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
[32mkafka1             |[0m [2021-01-30 15:46:05,163] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
[32mkafka1             |[0m [2021-01-30 15:46:05,174] INFO starting (kafka.server.KafkaServer)
[32mkafka1             |[0m [2021-01-30 15:46:05,177] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[36mconnect            |[0m Implicit acceptance of the license below:  
[36mconnect            |[0m MIT License 
[36mconnect            |[0m https://opensource.org/licenses/MIT 
[36mconnect            |[0m Implicit confirmation of the question: You are about to install 'kafka-connect-rss' from Artur Kalimullin, as published on Confluent Hub. 
[36mconnect            |[0m Downloading component Kafka Connect RSS Source 0.1.0, provided by Artur Kalimullin from Confluent Hub and installing into /usr/share/confluent-hub-components 
[32mkafka1             |[0m [2021-01-30 15:46:05,238] INFO [ZooKeeperClient Kafka server] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[32mkafka1             |[0m [2021-01-30 15:46:05,250] INFO Client environment:zookeeper.version=3.5.8-f439ca583e70862c3068a1f2a7d4d068eec33315, built on 05/04/2020 15:53 GMT (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,250] INFO Client environment:host.name=kafka1 (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,250] INFO Client environment:java.version=11.0.9.1 (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,250] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,250] INFO Client environment:java.home=/usr/lib/jvm/zulu11-ca (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,250] INFO Client environment:java.class.path=/usr/bin/../share/java/kafka/netty-buffer-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.30.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jackson-module-paranamer-2.10.5.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/kafka-tools-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.30.jar:/usr/bin/../share/java/kafka/kafka-streams-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-core-2.10.5.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.30.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jackson-databind-2.10.5.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.2.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/kafka/zookeeper-3.5.8.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jersey-common-2.30.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-clients-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/connect-transforms-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.2.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.2.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-test.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-scaladoc.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-sources.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.18.4.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-javadoc.jar:/usr/bin/../share/java/kafka/connect-mirror-client-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/connect-file-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/usr/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/usr/bin/../share/java/kafka/jersey-client-2.30.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.30.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/javassist-3.25.0-GA.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/connect-json-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-mirror-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/kafka/netty-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-test-sources.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/connect-api-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.50.Final.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-telemetry/* (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,250] INFO Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:os.version=4.19.121-linuxkit (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:user.name=appuser (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:user.home=/home/appuser (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:user.dir=/home/appuser (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:os.memory.free=1014MB (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,251] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,257] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@134d26af (org.apache.zookeeper.ZooKeeper)
[32mkafka1             |[0m [2021-01-30 15:46:05,271] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)
[32mkafka1             |[0m [2021-01-30 15:46:05,286] INFO zookeeper.request.timeout value is 0. feature enabled= (org.apache.zookeeper.ClientCnxn)
[32mkafka1             |[0m [2021-01-30 15:46:05,298] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[33mkafka-ui           |[0m 15:46:05.358 [main] INFO  org.springframework.boot.autoconfigure.security.reactive.ReactiveUserDetailsServiceAutoConfiguration - 
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m Using generated security password: ab8e04e8-0577-4838-9de1-853f3b8252fb
[33mkafka-ui           |[0m 
[32mkafka1             |[0m [2021-01-30 15:46:05,361] INFO Opening socket connection to server zookeeper/172.28.0.2:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[32mkafka1             |[0m [2021-01-30 15:46:05,397] INFO Socket connection established, initiating session, client: /172.28.0.3:42240, server: zookeeper/172.28.0.2:2181 (org.apache.zookeeper.ClientCnxn)
[32mkafka1             |[0m [2021-01-30 15:46:05,430] INFO Session establishment complete on server zookeeper/172.28.0.2:2181, sessionid = 0x100044b160f0004, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)
[32mkafka1             |[0m [2021-01-30 15:46:05,449] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)
[32mkafka1             |[0m [2021-01-30 15:46:06,330] INFO Cluster ID = 3-wYX40qRXev0Kr60NxjUw (kafka.server.KafkaServer)
[32mkafka1             |[0m [2021-01-30 15:46:06,348] WARN No meta.properties file under dir /var/lib/kafka/data/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[36mconnect            |[0m Adding installation directory to plugin path in the following files: 
[36mconnect            |[0m   /etc/kafka/connect-distributed.properties 
[36mconnect            |[0m   /etc/kafka/connect-standalone.properties 
[36mconnect            |[0m   /etc/schema-registry/connect-avro-distributed.properties 
[36mconnect            |[0m   /etc/schema-registry/connect-avro-standalone.properties 
[36mconnect            |[0m  
[36mconnect            |[0m Completed 
[32mkafka1             |[0m [2021-01-30 15:46:06,483] INFO KafkaConfig values: 
[32mkafka1             |[0m 	advertised.host.name = null
[32mkafka1             |[0m 	advertised.listeners = PLAINTEXT://kafka1:9092, PLAINTEXT_HOST://localhost:29092
[32mkafka1             |[0m 	advertised.port = null
[32mkafka1             |[0m 	alter.config.policy.class.name = null
[32mkafka1             |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka1             |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka1             |[0m 	authorizer.class.name = 
[32mkafka1             |[0m 	auto.create.topics.enable = true
[32mkafka1             |[0m 	auto.leader.rebalance.enable = true
[32mkafka1             |[0m 	background.threads = 10
[32mkafka1             |[0m 	broker.id = 1
[32mkafka1             |[0m 	broker.id.generation.enable = true
[32mkafka1             |[0m 	broker.rack = null
[32mkafka1             |[0m 	client.quota.callback.class = null
[32mkafka1             |[0m 	compression.type = producer
[32mkafka1             |[0m 	connection.failed.authentication.delay.ms = 100
[32mkafka1             |[0m 	connections.max.idle.ms = 600000
[32mkafka1             |[0m 	connections.max.reauth.ms = 0
[32mkafka1             |[0m 	control.plane.listener.name = null
[32mkafka1             |[0m 	controlled.shutdown.enable = true
[32mkafka1             |[0m 	controlled.shutdown.max.retries = 3
[32mkafka1             |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka1             |[0m 	controller.socket.timeout.ms = 30000
[32mkafka1             |[0m 	create.topic.policy.class.name = null
[32mkafka1             |[0m 	default.replication.factor = 1
[32mkafka1             |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka1             |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka1             |[0m 	delegation.token.master.key = null
[32mkafka1             |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka1             |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka1             |[0m 	delete.topic.enable = true
[32mkafka1             |[0m 	fetch.max.bytes = 57671680
[32mkafka1             |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka1             |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka1             |[0m 	group.max.session.timeout.ms = 1800000
[32mkafka1             |[0m 	group.max.size = 2147483647
[32mkafka1             |[0m 	group.min.session.timeout.ms = 6000
[32mkafka1             |[0m 	host.name = 
[32mkafka1             |[0m 	inter.broker.listener.name = null
[32mkafka1             |[0m 	inter.broker.protocol.version = 2.6-IV0
[32mkafka1             |[0m 	kafka.metrics.polling.interval.secs = 10
[32mkafka1             |[0m 	kafka.metrics.reporters = []
[32mkafka1             |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka1             |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka1             |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
[32mkafka1             |[0m 	listeners = PLAINTEXT://0.0.0.0:9092, PLAINTEXT_HOST://0.0.0.0:29092
[32mkafka1             |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka1             |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka1             |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka1             |[0m 	log.cleaner.enable = true
[32mkafka1             |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka1             |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka1             |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka1             |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[32mkafka1             |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka1             |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka1             |[0m 	log.cleaner.threads = 1
[32mkafka1             |[0m 	log.cleanup.policy = [delete]
[32mkafka1             |[0m 	log.dir = /tmp/kafka-logs
[32mkafka1             |[0m 	log.dirs = /var/lib/kafka/data
[32mkafka1             |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka1             |[0m 	log.flush.interval.ms = null
[32mkafka1             |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka1             |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka1             |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka1             |[0m 	log.index.interval.bytes = 4096
[32mkafka1             |[0m 	log.index.size.max.bytes = 10485760
[32mkafka1             |[0m 	log.message.downconversion.enable = true
[32mkafka1             |[0m 	log.message.format.version = 2.6-IV0
[32mkafka1             |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka1             |[0m 	log.message.timestamp.type = CreateTime
[32mkafka1             |[0m 	log.preallocate = false
[32mkafka1             |[0m 	log.retention.bytes = -1
[32mkafka1             |[0m 	log.retention.check.interval.ms = 300000
[32mkafka1             |[0m 	log.retention.hours = 168
[32mkafka1             |[0m 	log.retention.minutes = null
[32mkafka1             |[0m 	log.retention.ms = null
[32mkafka1             |[0m 	log.roll.hours = 168
[32mkafka1             |[0m 	log.roll.jitter.hours = 0
[32mkafka1             |[0m 	log.roll.jitter.ms = null
[32mkafka1             |[0m 	log.roll.ms = null
[32mkafka1             |[0m 	log.segment.bytes = 1073741824
[32mkafka1             |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka1             |[0m 	max.connections = 2147483647
[32mkafka1             |[0m 	max.connections.per.ip = 2147483647
[32mkafka1             |[0m 	max.connections.per.ip.overrides = 
[32mkafka1             |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka1             |[0m 	message.max.bytes = 1048588
[32mkafka1             |[0m 	metric.reporters = []
[32mkafka1             |[0m 	metrics.num.samples = 2
[32mkafka1             |[0m 	metrics.recording.level = INFO
[32mkafka1             |[0m 	metrics.sample.window.ms = 30000
[32mkafka1             |[0m 	min.insync.replicas = 1
[32mkafka1             |[0m 	num.io.threads = 8
[32mkafka1             |[0m 	num.network.threads = 3
[32mkafka1             |[0m 	num.partitions = 1
[32mkafka1             |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka1             |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka1             |[0m 	num.replica.fetchers = 1
[32mkafka1             |[0m 	offset.metadata.max.bytes = 4096
[32mkafka1             |[0m 	offsets.commit.required.acks = -1
[32mkafka1             |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka1             |[0m 	offsets.load.buffer.size = 5242880
[32mkafka1             |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka1             |[0m 	offsets.retention.minutes = 10080
[32mkafka1             |[0m 	offsets.topic.compression.codec = 0
[32mkafka1             |[0m 	offsets.topic.num.partitions = 50
[32mkafka1             |[0m 	offsets.topic.replication.factor = 1
[32mkafka1             |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka1             |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka1             |[0m 	password.encoder.iterations = 4096
[32mkafka1             |[0m 	password.encoder.key.length = 128
[32mkafka1             |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka1             |[0m 	password.encoder.old.secret = null
[32mkafka1             |[0m 	password.encoder.secret = null
[32mkafka1             |[0m 	port = 9092
[32mkafka1             |[0m 	principal.builder.class = null
[32mkafka1             |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka1             |[0m 	queued.max.request.bytes = -1
[32mkafka1             |[0m 	queued.max.requests = 500
[32mkafka1             |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka1             |[0m 	quota.producer.default = 9223372036854775807
[32mkafka1             |[0m 	quota.window.num = 11
[32mkafka1             |[0m 	quota.window.size.seconds = 1
[32mkafka1             |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka1             |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka1             |[0m 	replica.fetch.min.bytes = 1
[32mkafka1             |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka1             |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka1             |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka1             |[0m 	replica.lag.time.max.ms = 30000
[32mkafka1             |[0m 	replica.selector.class = null
[32mkafka1             |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka1             |[0m 	replica.socket.timeout.ms = 30000
[32mkafka1             |[0m 	replication.quota.window.num = 11
[32mkafka1             |[0m 	replication.quota.window.size.seconds = 1
[32mkafka1             |[0m 	request.timeout.ms = 30000
[32mkafka1             |[0m 	reserved.broker.max.id = 1000
[32mkafka1             |[0m 	sasl.client.callback.handler.class = null
[32mkafka1             |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka1             |[0m 	sasl.jaas.config = null
[32mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka1             |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka1             |[0m 	sasl.kerberos.service.name = null
[32mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka1             |[0m 	sasl.login.callback.handler.class = null
[32mkafka1             |[0m 	sasl.login.class = null
[32mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka1             |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka1             |[0m 	sasl.server.callback.handler.class = null
[32mkafka1             |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka1             |[0m 	security.providers = null
[32mkafka1             |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka1             |[0m 	socket.request.max.bytes = 104857600
[32mkafka1             |[0m 	socket.send.buffer.bytes = 102400
[32mkafka1             |[0m 	ssl.cipher.suites = []
[32mkafka1             |[0m 	ssl.client.auth = none
[32mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[32mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka1             |[0m 	ssl.engine.factory.class = null
[32mkafka1             |[0m 	ssl.key.password = null
[32mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka1             |[0m 	ssl.keystore.location = null
[32mkafka1             |[0m 	ssl.keystore.password = null
[32mkafka1             |[0m 	ssl.keystore.type = JKS
[32mkafka1             |[0m 	ssl.principal.mapping.rules = DEFAULT
[32mkafka1             |[0m 	ssl.protocol = TLSv1.3
[32mkafka1             |[0m 	ssl.provider = null
[32mkafka1             |[0m 	ssl.secure.random.implementation = null
[32mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka1             |[0m 	ssl.truststore.location = null
[32mkafka1             |[0m 	ssl.truststore.password = null
[32mkafka1             |[0m 	ssl.truststore.type = JKS
[32mkafka1             |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[32mkafka1             |[0m 	transaction.max.timeout.ms = 900000
[32mkafka1             |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka1             |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka1             |[0m 	transaction.state.log.min.isr = 2
[32mkafka1             |[0m 	transaction.state.log.num.partitions = 50
[32mkafka1             |[0m 	transaction.state.log.replication.factor = 3
[32mkafka1             |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka1             |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka1             |[0m 	unclean.leader.election.enable = false
[32mkafka1             |[0m 	zookeeper.clientCnxnSocket = null
[32mkafka1             |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka1             |[0m 	zookeeper.connection.timeout.ms = null
[32mkafka1             |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka1             |[0m 	zookeeper.session.timeout.ms = 18000
[32mkafka1             |[0m 	zookeeper.set.acl = false
[32mkafka1             |[0m 	zookeeper.ssl.cipher.suites = null
[32mkafka1             |[0m 	zookeeper.ssl.client.enable = false
[32mkafka1             |[0m 	zookeeper.ssl.crl.enable = false
[32mkafka1             |[0m 	zookeeper.ssl.enabled.protocols = null
[32mkafka1             |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[32mkafka1             |[0m 	zookeeper.ssl.keystore.location = null
[32mkafka1             |[0m 	zookeeper.ssl.keystore.password = null
[32mkafka1             |[0m 	zookeeper.ssl.keystore.type = null
[32mkafka1             |[0m 	zookeeper.ssl.ocsp.enable = false
[32mkafka1             |[0m 	zookeeper.ssl.protocol = TLSv1.2
[32mkafka1             |[0m 	zookeeper.ssl.truststore.location = null
[32mkafka1             |[0m 	zookeeper.ssl.truststore.password = null
[32mkafka1             |[0m 	zookeeper.ssl.truststore.type = null
[32mkafka1             |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka1             |[0m  (kafka.server.KafkaConfig)
[32mkafka1             |[0m [2021-01-30 15:46:06,504] INFO KafkaConfig values: 
[32mkafka1             |[0m 	advertised.host.name = null
[32mkafka1             |[0m 	advertised.listeners = PLAINTEXT://kafka1:9092, PLAINTEXT_HOST://localhost:29092
[32mkafka1             |[0m 	advertised.port = null
[32mkafka1             |[0m 	alter.config.policy.class.name = null
[32mkafka1             |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka1             |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka1             |[0m 	authorizer.class.name = 
[32mkafka1             |[0m 	auto.create.topics.enable = true
[32mkafka1             |[0m 	auto.leader.rebalance.enable = true
[32mkafka1             |[0m 	background.threads = 10
[32mkafka1             |[0m 	broker.id = 1
[32mkafka1             |[0m 	broker.id.generation.enable = true
[32mkafka1             |[0m 	broker.rack = null
[32mkafka1             |[0m 	client.quota.callback.class = null
[32mkafka1             |[0m 	compression.type = producer
[32mkafka1             |[0m 	connection.failed.authentication.delay.ms = 100
[32mkafka1             |[0m 	connections.max.idle.ms = 600000
[32mkafka1             |[0m 	connections.max.reauth.ms = 0
[32mkafka1             |[0m 	control.plane.listener.name = null
[32mkafka1             |[0m 	controlled.shutdown.enable = true
[32mkafka1             |[0m 	controlled.shutdown.max.retries = 3
[32mkafka1             |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka1             |[0m 	controller.socket.timeout.ms = 30000
[32mkafka1             |[0m 	create.topic.policy.class.name = null
[32mkafka1             |[0m 	default.replication.factor = 1
[32mkafka1             |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka1             |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka1             |[0m 	delegation.token.master.key = null
[32mkafka1             |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka1             |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka1             |[0m 	delete.topic.enable = true
[32mkafka1             |[0m 	fetch.max.bytes = 57671680
[32mkafka1             |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka1             |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka1             |[0m 	group.max.session.timeout.ms = 1800000
[32mkafka1             |[0m 	group.max.size = 2147483647
[32mkafka1             |[0m 	group.min.session.timeout.ms = 6000
[32mkafka1             |[0m 	host.name = 
[32mkafka1             |[0m 	inter.broker.listener.name = null
[32mkafka1             |[0m 	inter.broker.protocol.version = 2.6-IV0
[32mkafka1             |[0m 	kafka.metrics.polling.interval.secs = 10
[32mkafka1             |[0m 	kafka.metrics.reporters = []
[32mkafka1             |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka1             |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka1             |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
[32mkafka1             |[0m 	listeners = PLAINTEXT://0.0.0.0:9092, PLAINTEXT_HOST://0.0.0.0:29092
[32mkafka1             |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka1             |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka1             |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka1             |[0m 	log.cleaner.enable = true
[32mkafka1             |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka1             |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka1             |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka1             |[0m 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
[32mkafka1             |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka1             |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka1             |[0m 	log.cleaner.threads = 1
[32mkafka1             |[0m 	log.cleanup.policy = [delete]
[32mkafka1             |[0m 	log.dir = /tmp/kafka-logs
[32mkafka1             |[0m 	log.dirs = /var/lib/kafka/data
[32mkafka1             |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka1             |[0m 	log.flush.interval.ms = null
[32mkafka1             |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka1             |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka1             |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka1             |[0m 	log.index.interval.bytes = 4096
[32mkafka1             |[0m 	log.index.size.max.bytes = 10485760
[32mkafka1             |[0m 	log.message.downconversion.enable = true
[32mkafka1             |[0m 	log.message.format.version = 2.6-IV0
[32mkafka1             |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka1             |[0m 	log.message.timestamp.type = CreateTime
[32mkafka1             |[0m 	log.preallocate = false
[32mkafka1             |[0m 	log.retention.bytes = -1
[32mkafka1             |[0m 	log.retention.check.interval.ms = 300000
[32mkafka1             |[0m 	log.retention.hours = 168
[32mkafka1             |[0m 	log.retention.minutes = null
[32mkafka1             |[0m 	log.retention.ms = null
[32mkafka1             |[0m 	log.roll.hours = 168
[32mkafka1             |[0m 	log.roll.jitter.hours = 0
[32mkafka1             |[0m 	log.roll.jitter.ms = null
[32mkafka1             |[0m 	log.roll.ms = null
[32mkafka1             |[0m 	log.segment.bytes = 1073741824
[32mkafka1             |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka1             |[0m 	max.connections = 2147483647
[32mkafka1             |[0m 	max.connections.per.ip = 2147483647
[32mkafka1             |[0m 	max.connections.per.ip.overrides = 
[32mkafka1             |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka1             |[0m 	message.max.bytes = 1048588
[32mkafka1             |[0m 	metric.reporters = []
[32mkafka1             |[0m 	metrics.num.samples = 2
[32mkafka1             |[0m 	metrics.recording.level = INFO
[32mkafka1             |[0m 	metrics.sample.window.ms = 30000
[32mkafka1             |[0m 	min.insync.replicas = 1
[32mkafka1             |[0m 	num.io.threads = 8
[32mkafka1             |[0m 	num.network.threads = 3
[32mkafka1             |[0m 	num.partitions = 1
[32mkafka1             |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka1             |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka1             |[0m 	num.replica.fetchers = 1
[32mkafka1             |[0m 	offset.metadata.max.bytes = 4096
[32mkafka1             |[0m 	offsets.commit.required.acks = -1
[32mkafka1             |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka1             |[0m 	offsets.load.buffer.size = 5242880
[32mkafka1             |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka1             |[0m 	offsets.retention.minutes = 10080
[32mkafka1             |[0m 	offsets.topic.compression.codec = 0
[32mkafka1             |[0m 	offsets.topic.num.partitions = 50
[32mkafka1             |[0m 	offsets.topic.replication.factor = 1
[32mkafka1             |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka1             |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka1             |[0m 	password.encoder.iterations = 4096
[32mkafka1             |[0m 	password.encoder.key.length = 128
[32mkafka1             |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka1             |[0m 	password.encoder.old.secret = null
[32mkafka1             |[0m 	password.encoder.secret = null
[32mkafka1             |[0m 	port = 9092
[32mkafka1             |[0m 	principal.builder.class = null
[32mkafka1             |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka1             |[0m 	queued.max.request.bytes = -1
[32mkafka1             |[0m 	queued.max.requests = 500
[32mkafka1             |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka1             |[0m 	quota.producer.default = 9223372036854775807
[32mkafka1             |[0m 	quota.window.num = 11
[32mkafka1             |[0m 	quota.window.size.seconds = 1
[32mkafka1             |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka1             |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka1             |[0m 	replica.fetch.min.bytes = 1
[32mkafka1             |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka1             |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka1             |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka1             |[0m 	replica.lag.time.max.ms = 30000
[32mkafka1             |[0m 	replica.selector.class = null
[32mkafka1             |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka1             |[0m 	replica.socket.timeout.ms = 30000
[32mkafka1             |[0m 	replication.quota.window.num = 11
[32mkafka1             |[0m 	replication.quota.window.size.seconds = 1
[32mkafka1             |[0m 	request.timeout.ms = 30000
[32mkafka1             |[0m 	reserved.broker.max.id = 1000
[32mkafka1             |[0m 	sasl.client.callback.handler.class = null
[32mkafka1             |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka1             |[0m 	sasl.jaas.config = null
[32mkafka1             |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka1             |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka1             |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka1             |[0m 	sasl.kerberos.service.name = null
[32mkafka1             |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka1             |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka1             |[0m 	sasl.login.callback.handler.class = null
[32mkafka1             |[0m 	sasl.login.class = null
[32mkafka1             |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka1             |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka1             |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka1             |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka1             |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka1             |[0m 	sasl.server.callback.handler.class = null
[32mkafka1             |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka1             |[0m 	security.providers = null
[32mkafka1             |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka1             |[0m 	socket.request.max.bytes = 104857600
[32mkafka1             |[0m 	socket.send.buffer.bytes = 102400
[32mkafka1             |[0m 	ssl.cipher.suites = []
[32mkafka1             |[0m 	ssl.client.auth = none
[32mkafka1             |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[32mkafka1             |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka1             |[0m 	ssl.engine.factory.class = null
[32mkafka1             |[0m 	ssl.key.password = null
[32mkafka1             |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka1             |[0m 	ssl.keystore.location = null
[32mkafka1             |[0m 	ssl.keystore.password = null
[32mkafka1             |[0m 	ssl.keystore.type = JKS
[32mkafka1             |[0m 	ssl.principal.mapping.rules = DEFAULT
[32mkafka1             |[0m 	ssl.protocol = TLSv1.3
[32mkafka1             |[0m 	ssl.provider = null
[32mkafka1             |[0m 	ssl.secure.random.implementation = null
[32mkafka1             |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka1             |[0m 	ssl.truststore.location = null
[32mkafka1             |[0m 	ssl.truststore.password = null
[32mkafka1             |[0m 	ssl.truststore.type = JKS
[32mkafka1             |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
[32mkafka1             |[0m 	transaction.max.timeout.ms = 900000
[32mkafka1             |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka1             |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka1             |[0m 	transaction.state.log.min.isr = 2
[32mkafka1             |[0m 	transaction.state.log.num.partitions = 50
[32mkafka1             |[0m 	transaction.state.log.replication.factor = 3
[32mkafka1             |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka1             |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka1             |[0m 	unclean.leader.election.enable = false
[32mkafka1             |[0m 	zookeeper.clientCnxnSocket = null
[32mkafka1             |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka1             |[0m 	zookeeper.connection.timeout.ms = null
[32mkafka1             |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka1             |[0m 	zookeeper.session.timeout.ms = 18000
[32mkafka1             |[0m 	zookeeper.set.acl = false
[32mkafka1             |[0m 	zookeeper.ssl.cipher.suites = null
[32mkafka1             |[0m 	zookeeper.ssl.client.enable = false
[32mkafka1             |[0m 	zookeeper.ssl.crl.enable = false
[32mkafka1             |[0m 	zookeeper.ssl.enabled.protocols = null
[32mkafka1             |[0m 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
[32mkafka1             |[0m 	zookeeper.ssl.keystore.location = null
[32mkafka1             |[0m 	zookeeper.ssl.keystore.password = null
[32mkafka1             |[0m 	zookeeper.ssl.keystore.type = null
[32mkafka1             |[0m 	zookeeper.ssl.ocsp.enable = false
[32mkafka1             |[0m 	zookeeper.ssl.protocol = TLSv1.2
[32mkafka1             |[0m 	zookeeper.ssl.truststore.location = null
[32mkafka1             |[0m 	zookeeper.ssl.truststore.password = null
[32mkafka1             |[0m 	zookeeper.ssl.truststore.type = null
[32mkafka1             |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka1             |[0m  (kafka.server.KafkaConfig)
[36mconnect            |[0m \[Docker-Connect] Start waiting for installation of RSS Connector to complete
[36mconnect            |[0m [Docker-Connect] Launching Kafka Connect worker
[36mconnect            |[0m ===> User
[36mconnect            |[0m uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
[36mconnect            |[0m ===> Configuring ...
[36mconnect            |[0m Sat Jan 30 15:46:06 UTC 2021  Kafka Connect listener HTTP state:  000  (waiting for 200)
[32mkafka1             |[0m [2021-01-30 15:46:06,613] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka1             |[0m [2021-01-30 15:46:06,615] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka1             |[0m [2021-01-30 15:46:06,615] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka1             |[0m [2021-01-30 15:46:06,719] INFO Loading logs from log dirs ArraySeq(/var/lib/kafka/data) (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:06,723] INFO Attempting recovery for all logs in /var/lib/kafka/data since no clean shutdown file was found (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:06,742] INFO Loaded 0 logs in 23ms. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:06,782] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:06,791] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:06,801] INFO Starting the log cleaner (kafka.log.LogCleaner)
[32mkafka1             |[0m [2021-01-30 15:46:06,906] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner)
[32mkafka1             |[0m [2021-01-30 15:46:07,963] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[32mkafka1             |[0m [2021-01-30 15:46:08,065] INFO [SocketServer brokerId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[32mkafka1             |[0m [2021-01-30 15:46:08,066] INFO Awaiting socket connections on 0.0.0.0:29092. (kafka.network.Acceptor)
[32mkafka1             |[0m [2021-01-30 15:46:08,093] INFO [SocketServer brokerId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[32mkafka1             |[0m [2021-01-30 15:46:08,180] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:08,198] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:08,217] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:08,227] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:08,289] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[32mkafka1             |[0m [2021-01-30 15:46:08,376] INFO Creating /brokers/ids/1 (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:08,452] INFO Stat of the created znode at /brokers/ids/1 is: 31,31,1612021568424,1612021568424,1,0,0,72062314577068036,246,0,31
[32mkafka1             |[0m  (kafka.zk.KafkaZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:08,456] INFO Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://kafka1:9092,PLAINTEXT_HOST://localhost:29092, czxid (broker epoch): 31 (kafka.zk.KafkaZkClient)
[35mschema-registry    |[0m [main] INFO org.apache.zookeeper.ZooKeeper - Session: 0x100044b160f0003 closed
[35mschema-registry    |[0m [main-EventThread] INFO org.apache.zookeeper.ClientCnxn - EventThread shut down for session: 0x100044b160f0003
[32mkafka1             |[0m [2021-01-30 15:46:08,784] INFO [ControllerEventThread controllerId=1] Starting (kafka.controller.ControllerEventManager$ControllerEventThread)
[35mschema-registry    |[0m [main] INFO org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
[35mschema-registry    |[0m 	bootstrap.servers = [kafka1:9092]
[35mschema-registry    |[0m 	client.dns.lookup = use_all_dns_ips
[35mschema-registry    |[0m 	client.id = 
[35mschema-registry    |[0m 	connections.max.idle.ms = 300000
[35mschema-registry    |[0m 	default.api.timeout.ms = 60000
[35mschema-registry    |[0m 	metadata.max.age.ms = 300000
[35mschema-registry    |[0m 	metric.reporters = []
[35mschema-registry    |[0m 	metrics.num.samples = 2
[35mschema-registry    |[0m 	metrics.recording.level = INFO
[35mschema-registry    |[0m 	metrics.sample.window.ms = 30000
[35mschema-registry    |[0m 	receive.buffer.bytes = 65536
[35mschema-registry    |[0m 	reconnect.backoff.max.ms = 1000
[35mschema-registry    |[0m 	reconnect.backoff.ms = 50
[35mschema-registry    |[0m 	request.timeout.ms = 30000
[35mschema-registry    |[0m 	retries = 2147483647
[35mschema-registry    |[0m 	retry.backoff.ms = 100
[35mschema-registry    |[0m 	sasl.client.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.jaas.config = null
[35mschema-registry    |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mschema-registry    |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mschema-registry    |[0m 	sasl.kerberos.service.name = null
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.login.class = null
[35mschema-registry    |[0m 	sasl.login.refresh.buffer.seconds = 300
[35mschema-registry    |[0m 	sasl.login.refresh.min.period.seconds = 60
[35mschema-registry    |[0m 	sasl.login.refresh.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.refresh.window.jitter = 0.05
[35mschema-registry    |[0m 	sasl.mechanism = GSSAPI
[35mschema-registry    |[0m 	security.protocol = PLAINTEXT
[35mschema-registry    |[0m 	security.providers = null
[35mschema-registry    |[0m 	send.buffer.bytes = 131072
[35mschema-registry    |[0m 	ssl.cipher.suites = null
[35mschema-registry    |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[35mschema-registry    |[0m 	ssl.endpoint.identification.algorithm = https
[35mschema-registry    |[0m 	ssl.engine.factory.class = null
[35mschema-registry    |[0m 	ssl.key.password = null
[35mschema-registry    |[0m 	ssl.keymanager.algorithm = SunX509
[35mschema-registry    |[0m 	ssl.keystore.location = null
[35mschema-registry    |[0m 	ssl.keystore.password = null
[35mschema-registry    |[0m 	ssl.keystore.type = JKS
[35mschema-registry    |[0m 	ssl.protocol = TLSv1.3
[35mschema-registry    |[0m 	ssl.provider = null
[35mschema-registry    |[0m 	ssl.secure.random.implementation = null
[35mschema-registry    |[0m 	ssl.trustmanager.algorithm = PKIX
[35mschema-registry    |[0m 	ssl.truststore.location = null
[35mschema-registry    |[0m 	ssl.truststore.password = null
[35mschema-registry    |[0m 	ssl.truststore.type = JKS
[35mschema-registry    |[0m 
[32mkafka1             |[0m [2021-01-30 15:46:08,830] INFO [ExpirationReaper-1-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:08,846] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:08,851] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:08,873] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:08,891] INFO [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:08,897] INFO [Controller id=1] Registering handlers (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:08,915] INFO [Controller id=1] Deleting log dir event notifications (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:08,935] INFO [Controller id=1] Deleting isr change notifications (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:08,953] INFO [Controller id=1] Initializing controller context (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,000] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:09,008] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:09,068] INFO [GroupMetadataManager brokerId=1] Removed 0 expired offsets in 47 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:09,105] INFO [ProducerId Manager 1]: Acquired new producerId block (brokerId:1,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[32mkafka1             |[0m [2021-01-30 15:46:09,127] INFO [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 31) (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,148] DEBUG [Controller id=1] Register BrokerModifications handler for Set(1) (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,169] DEBUG [Channel manager on controller 1]: Controller 1 trying to connect to broker 1 (kafka.controller.ControllerChannelManager)
[32mkafka1             |[0m [2021-01-30 15:46:09,220] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:09,228] INFO [RequestSendThread controllerId=1] Starting (kafka.controller.RequestSendThread)
[32mkafka1             |[0m [2021-01-30 15:46:09,236] INFO [Controller id=1] Currently active brokers in the cluster: Set(1) (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,236] INFO [Controller id=1] Currently shutting brokers in the cluster: HashSet() (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,237] INFO [Controller id=1] Current list of topics in the cluster: HashSet() (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,248] INFO [Transaction Marker Channel Manager 1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[32mkafka1             |[0m [2021-01-30 15:46:09,248] INFO [Controller id=1] Fetching topic deletions in progress (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,248] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:09,260] INFO [Controller id=1] List of topics to be deleted:  (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,265] INFO [Controller id=1] List of topics ineligible for deletion:  (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,267] INFO [Controller id=1] Initializing topic deletion manager (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,269] INFO [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet() (kafka.controller.TopicDeletionManager)
[32mkafka1             |[0m [2021-01-30 15:46:09,271] INFO [Controller id=1] Sending update metadata request (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,278] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:09,355] INFO [ReplicaStateMachine controllerId=1] Initializing replica state (kafka.controller.ZkReplicaStateMachine)
[32mkafka1             |[0m [2021-01-30 15:46:09,357] INFO [ReplicaStateMachine controllerId=1] Triggering online replica state changes (kafka.controller.ZkReplicaStateMachine)
[32mkafka1             |[0m [2021-01-30 15:46:09,361] INFO [RequestSendThread controllerId=1] Controller 1 connected to kafka1:9092 (id: 1 rack: null) for sending state change requests (kafka.controller.RequestSendThread)
[32mkafka1             |[0m [2021-01-30 15:46:09,401] INFO [ReplicaStateMachine controllerId=1] Triggering offline replica state changes (kafka.controller.ZkReplicaStateMachine)
[32mkafka1             |[0m [2021-01-30 15:46:09,402] DEBUG [ReplicaStateMachine controllerId=1] Started replica state machine with initial state -> HashMap() (kafka.controller.ZkReplicaStateMachine)
[32mkafka1             |[0m [2021-01-30 15:46:09,403] INFO [PartitionStateMachine controllerId=1] Initializing partition state (kafka.controller.ZkPartitionStateMachine)
[32mkafka1             |[0m [2021-01-30 15:46:09,414] INFO [PartitionStateMachine controllerId=1] Triggering online partition state changes (kafka.controller.ZkPartitionStateMachine)
[32mkafka1             |[0m [2021-01-30 15:46:09,428] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka1             |[0m [2021-01-30 15:46:09,461] DEBUG [PartitionStateMachine controllerId=1] Started partition state machine with initial state -> HashMap() (kafka.controller.ZkPartitionStateMachine)
[32mkafka1             |[0m [2021-01-30 15:46:09,462] INFO [Controller id=1] Ready to serve as the new controller with epoch 1 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,557] INFO [Controller id=1] Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,565] INFO [Controller id=1] Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,572] INFO [Controller id=1] Skipping preferred replica election for partitions due to topic deletion:  (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,579] INFO [Controller id=1] Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,593] INFO [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,659] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[35mschema-registry    |[0m [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.0.1-ccs
[35mschema-registry    |[0m [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 9c1fbb3db1e0d69d
[35mschema-registry    |[0m [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021569654
[32mkafka1             |[0m [2021-01-30 15:46:09,677] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:09,732] INFO [SocketServer brokerId=1] Starting socket server acceptors and processors (kafka.network.SocketServer)
[32mkafka1             |[0m [2021-01-30 15:46:09,774] INFO [SocketServer brokerId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[32mkafka1             |[0m [2021-01-30 15:46:09,780] INFO [SocketServer brokerId=1] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
[32mkafka1             |[0m [2021-01-30 15:46:09,782] INFO [SocketServer brokerId=1] Started socket server acceptors and processors (kafka.network.SocketServer)
[32mkafka1             |[0m [2021-01-30 15:46:09,836] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka1             |[0m [2021-01-30 15:46:09,849] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka1             |[0m [2021-01-30 15:46:09,849] INFO Kafka startTimeMs: 1612021569783 (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka1             |[0m [2021-01-30 15:46:09,861] INFO [KafkaServer id=1] started (kafka.server.KafkaServer)
[32mkafka1             |[0m [2021-01-30 15:46:10,019] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 0 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,224] INFO Creating topic marca_topics_v2 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:10,227] INFO Creating topic sport_topics_v2 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:10,228] INFO Creating topic mundo_topics_v2 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:10,343] INFO [KafkaApi-1] Auto creation of topic mundo_topics_v2 with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[32mkafka1             |[0m [2021-01-30 15:46:10,350] INFO [KafkaApi-1] Auto creation of topic marca_topics_v2 with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[32mkafka1             |[0m [2021-01-30 15:46:10,346] INFO [KafkaApi-1] Auto creation of topic sport_topics_v2 with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[33mkafka-ui           |[0m 15:46:10.397 [main] INFO  org.springframework.scheduling.concurrent.ThreadPoolTaskScheduler - Initializing ExecutorService 'taskScheduler'
[32mkafka1             |[0m [2021-01-30 15:46:10,404] INFO [Controller id=1] New topics: [Set(mundo_topics_v2, marca_topics_v2, sport_topics_v2)], deleted topics: [HashSet()], new partition replica assignment [Map(mundo_topics_v2-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), marca_topics_v2-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), sport_topics_v2-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:10,415] INFO [Controller id=1] New partition creation callback for mundo_topics_v2-0,marca_topics_v2-0,sport_topics_v2-0 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:10,464] INFO [Controller id=1 epoch=1] Changed partition mundo_topics_v2-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,465] INFO [Controller id=1 epoch=1] Changed partition marca_topics_v2-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,465] INFO [Controller id=1 epoch=1] Changed partition sport_topics_v2-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,466] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,525] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition mundo_topics_v2-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,534] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition marca_topics_v2-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,534] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition sport_topics_v2-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,535] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,680] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1), 25 -> ArrayBuffer(1), 26 -> ArrayBuffer(1), 27 -> ArrayBuffer(1), 28 -> ArrayBuffer(1), 29 -> ArrayBuffer(1), 30 -> ArrayBuffer(1), 31 -> ArrayBuffer(1), 32 -> ArrayBuffer(1), 33 -> ArrayBuffer(1), 34 -> ArrayBuffer(1), 35 -> ArrayBuffer(1), 36 -> ArrayBuffer(1), 37 -> ArrayBuffer(1), 38 -> ArrayBuffer(1), 39 -> ArrayBuffer(1), 40 -> ArrayBuffer(1), 41 -> ArrayBuffer(1), 42 -> ArrayBuffer(1), 43 -> ArrayBuffer(1), 44 -> ArrayBuffer(1), 45 -> ArrayBuffer(1), 46 -> ArrayBuffer(1), 47 -> ArrayBuffer(1), 48 -> ArrayBuffer(1), 49 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:10,696] INFO [Controller id=1 epoch=1] Changed partition mundo_topics_v2-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,696] INFO [Controller id=1 epoch=1] Changed partition marca_topics_v2-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,707] INFO [Controller id=1 epoch=1] Changed partition sport_topics_v2-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,710] INFO [KafkaApi-1] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[32mkafka1             |[0m [2021-01-30 15:46:10,717] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='mundo_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition mundo_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,725] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='sport_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition sport_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,725] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='marca_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition marca_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,728] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1), 25 -> ArrayBuffer(1), 26 -> ArrayBuffer(1), 27 -> ArrayBuffer(1), 28 -> ArrayBuffer(1), 29 -> ArrayBuffer(1), 30 -> ArrayBuffer(1), 31 -> ArrayBuffer(1), 32 -> ArrayBuffer(1), 33 -> ArrayBuffer(1), 34 -> ArrayBuffer(1), 35 -> ArrayBuffer(1), 36 -> ArrayBuffer(1), 37 -> ArrayBuffer(1), 38 -> ArrayBuffer(1), 39 -> ArrayBuffer(1), 40 -> ArrayBuffer(1), 41 -> ArrayBuffer(1), 42 -> ArrayBuffer(1), 43 -> ArrayBuffer(1), 44 -> ArrayBuffer(1), 45 -> ArrayBuffer(1), 46 -> ArrayBuffer(1), 47 -> ArrayBuffer(1), 48 -> ArrayBuffer(1), 49 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:10,728] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 3 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,729] INFO Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1), 25 -> ArrayBuffer(1), 26 -> ArrayBuffer(1), 27 -> ArrayBuffer(1), 28 -> ArrayBuffer(1), 29 -> ArrayBuffer(1), 30 -> ArrayBuffer(1), 31 -> ArrayBuffer(1), 32 -> ArrayBuffer(1), 33 -> ArrayBuffer(1), 34 -> ArrayBuffer(1), 35 -> ArrayBuffer(1), 36 -> ArrayBuffer(1), 37 -> ArrayBuffer(1), 38 -> ArrayBuffer(1), 39 -> ArrayBuffer(1), 40 -> ArrayBuffer(1), 41 -> ArrayBuffer(1), 42 -> ArrayBuffer(1), 43 -> ArrayBuffer(1), 44 -> ArrayBuffer(1), 45 -> ArrayBuffer(1), 46 -> ArrayBuffer(1), 47 -> ArrayBuffer(1), 48 -> ArrayBuffer(1), 49 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:10,754] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 3 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,801] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition mundo_topics_v2-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,809] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition marca_topics_v2-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,809] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition sport_topics_v2-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,810] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,847] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 for 3 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,852] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='mundo_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,855] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='sport_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,857] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='marca_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 1 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,892] INFO [Controller id=1] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [HashMap(__consumer_offsets-22 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-30 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-25 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-35 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-37 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-38 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-13 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-8 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-21 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-27 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-7 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-9 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-46 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-41 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-33 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-23 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-49 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-47 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-16 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-28 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-31 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-36 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-42 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-18 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-15 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-24 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-17 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-48 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-19 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-11 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-43 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-6 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-14 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-20 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-44 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-39 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-12 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-45 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-5 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-26 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-29 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-34 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-10 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-32 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-40 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:10,901] INFO [Controller id=1] New partition creation callback for __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-37,__consumer_offsets-38,__consumer_offsets-13,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:10,904] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,905] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,905] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,905] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,906] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,906] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,906] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,909] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,909] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,909] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,909] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,910] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,910] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,911] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,911] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,911] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,911] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,917] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,917] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,918] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,918] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,919] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,919] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,919] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,930] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,930] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,930] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,931] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,932] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,933] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,933] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,934] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,934] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,934] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,935] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,935] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,935] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,935] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,935] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,936] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,936] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,937] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,938] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,938] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,944] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,945] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,946] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,946] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,947] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,947] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,951] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,957] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition sport_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,957] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,971] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition mundo_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,972] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 epoch 1 starting the become-leader transition for partition marca_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,971] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,973] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,973] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,974] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,978] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(sport_topics_v2-0, mundo_topics_v2-0, marca_topics_v2-0) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:10,978] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,979] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,980] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,982] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,982] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,983] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,984] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,985] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,987] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,987] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,990] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,991] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,989] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 1 epoch 1 as part of the become-leader transition for 3 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,993] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,995] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:10,997] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,000] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,003] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,004] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,004] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,008] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,009] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,018] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,019] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,021] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,022] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,022] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,023] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,025] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,027] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,029] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,035] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,035] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,040] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,040] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,041] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,041] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,042] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,042] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,043] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,044] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,044] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,045] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,048] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[33mkafka-ui           |[0m 15:46:11.116 [parallel-1] DEBUG com.provectus.kafka.ui.cluster.service.MetricsUpdateService - Start getting metrics for kafkaCluster: Broker1
[33mkafka-ui           |[0m 15:46:11.236 [parallel-1] INFO  org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
[33mkafka-ui           |[0m 	bootstrap.servers = [kafka1:9092]
[33mkafka-ui           |[0m 	client.dns.lookup = default
[33mkafka-ui           |[0m 	client.id = 
[33mkafka-ui           |[0m 	connections.max.idle.ms = 300000
[33mkafka-ui           |[0m 	metadata.max.age.ms = 300000
[33mkafka-ui           |[0m 	metric.reporters = []
[33mkafka-ui           |[0m 	metrics.num.samples = 2
[33mkafka-ui           |[0m 	metrics.recording.level = INFO
[33mkafka-ui           |[0m 	metrics.sample.window.ms = 30000
[33mkafka-ui           |[0m 	receive.buffer.bytes = 65536
[33mkafka-ui           |[0m 	reconnect.backoff.max.ms = 1000
[33mkafka-ui           |[0m 	reconnect.backoff.ms = 50
[33mkafka-ui           |[0m 	request.timeout.ms = 5000
[33mkafka-ui           |[0m 	retries = 5
[33mkafka-ui           |[0m 	retry.backoff.ms = 100
[33mkafka-ui           |[0m 	sasl.client.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.jaas.config = null
[33mkafka-ui           |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mkafka-ui           |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mkafka-ui           |[0m 	sasl.kerberos.service.name = null
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.login.class = null
[33mkafka-ui           |[0m 	sasl.login.refresh.buffer.seconds = 300
[33mkafka-ui           |[0m 	sasl.login.refresh.min.period.seconds = 60
[33mkafka-ui           |[0m 	sasl.login.refresh.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.refresh.window.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.mechanism = GSSAPI
[33mkafka-ui           |[0m 	security.protocol = PLAINTEXT
[33mkafka-ui           |[0m 	security.providers = null
[33mkafka-ui           |[0m 	send.buffer.bytes = 131072
[33mkafka-ui           |[0m 	ssl.cipher.suites = null
[33mkafka-ui           |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mkafka-ui           |[0m 	ssl.endpoint.identification.algorithm = https
[33mkafka-ui           |[0m 	ssl.key.password = null
[33mkafka-ui           |[0m 	ssl.keymanager.algorithm = SunX509
[33mkafka-ui           |[0m 	ssl.keystore.location = null
[33mkafka-ui           |[0m 	ssl.keystore.password = null
[33mkafka-ui           |[0m 	ssl.keystore.type = JKS
[33mkafka-ui           |[0m 	ssl.protocol = TLS
[33mkafka-ui           |[0m 	ssl.provider = null
[33mkafka-ui           |[0m 	ssl.secure.random.implementation = null
[33mkafka-ui           |[0m 	ssl.trustmanager.algorithm = PKIX
[33mkafka-ui           |[0m 	ssl.truststore.location = null
[33mkafka-ui           |[0m 	ssl.truststore.password = null
[33mkafka-ui           |[0m 	ssl.truststore.type = JKS
[33mkafka-ui           |[0m 
[32mkafka1             |[0m [2021-01-30 15:46:11,529] INFO [Log partition=sport_topics_v2-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:11,603] INFO Created log for partition sport_topics_v2-0 in /var/lib/kafka/data/sport_topics_v2-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[35mschema-registry    |[0m ===> Launching ... 
[32mkafka1             |[0m [2021-01-30 15:46:11,621] INFO [Partition sport_topics_v2-0 broker=1] No checkpointed highwatermark is found for partition sport_topics_v2-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:11,624] INFO [Partition sport_topics_v2-0 broker=1] Log loaded for partition sport_topics_v2-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:11,632] INFO [Broker id=1] Leader sport_topics_v2-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[35mschema-registry    |[0m ===> Launching schema-registry ... 
[32mkafka1             |[0m [2021-01-30 15:46:11,658] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,660] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-30 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,663] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-25 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,663] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-35 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,665] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-37 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,665] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-38 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,666] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,666] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,671] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,671] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,672] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-27 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,672] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,673] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,674] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-46 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,675] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-41 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,675] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-33 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,676] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,676] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-49 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,677] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-47 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,677] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,682] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-28 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,683] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-31 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,685] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-36 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,685] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-42 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,686] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,689] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,689] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,690] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,691] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,691] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-48 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,693] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,693] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,696] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,697] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-43 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,697] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,703] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,703] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,704] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,705] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-44 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,706] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-39 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,707] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,708] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-45 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,709] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,709] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,710] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-26 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,711] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-29 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,711] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-34 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,712] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,713] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-32 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,715] INFO [Controller id=1 epoch=1] Changed partition __consumer_offsets-40 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,715] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-13 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,715] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-46 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,715] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-9 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,718] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-42 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,718] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-21 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,721] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-17 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,721] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-30 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,729] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-26 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,736] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-5 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,737] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-38 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,737] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,738] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-34 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,739] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,739] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-45 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,739] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,740] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-41 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,740] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-24 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,741] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-20 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,741] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-49 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,746] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,747] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-29 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,747] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-25 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,748] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,749] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-37 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,750] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-33 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,752] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-15 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,756] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-48 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,758] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-11 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,759] INFO [Log partition=mundo_topics_v2-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:11,759] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-44 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,760] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-23 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,761] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-19 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,762] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-32 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,763] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-28 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,765] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-7 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,765] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-40 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,767] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,772] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-36 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,773] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-47 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,773] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,773] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-43 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,774] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,774] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-22 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,775] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-18 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,775] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-31 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,776] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-27 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,777] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-39 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,779] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,780] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-35 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,785] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition __consumer_offsets-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,785] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 50 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,787] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 50 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,798] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-32 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,800] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-5 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,801] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-44 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,803] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-48 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,803] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-46 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,804] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-20 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,806] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-43 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,808] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-24 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,808] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-6 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,809] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-18 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,809] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-21 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,809] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-1 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,810] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-14 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,810] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-34 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,811] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-16 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,812] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-29 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,819] INFO Created log for partition mundo_topics_v2-0 in /var/lib/kafka/data/mundo_topics_v2-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:11,824] INFO [Partition mundo_topics_v2-0 broker=1] No checkpointed highwatermark is found for partition mundo_topics_v2-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:11,824] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-11 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,842] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,831] INFO [Partition mundo_topics_v2-0 broker=1] Log loaded for partition mundo_topics_v2-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:11,843] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-22 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,843] INFO [Broker id=1] Leader mundo_topics_v2-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,844] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-47 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,848] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-36 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,854] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-28 from NewReplica to OnlineReplica (state.change.logger)
[33mkafka-ui           |[0m 15:46:11.851 [parallel-1] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.4.0
[32mkafka1             |[0m [2021-01-30 15:46:11,855] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-42 from NewReplica to OnlineReplica (state.change.logger)
[33mkafka-ui           |[0m 15:46:11.856 [parallel-1] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 77a89fcf8d7fa018
[32mkafka1             |[0m [2021-01-30 15:46:11,857] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-9 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,859] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-37 from NewReplica to OnlineReplica (state.change.logger)
[33mkafka-ui           |[0m 15:46:11.858 [parallel-1] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021571815
[32mkafka1             |[0m [2021-01-30 15:46:11,864] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-13 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,867] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-30 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,874] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-35 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,874] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-39 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,876] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-12 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,877] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-27 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,877] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-45 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,878] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-19 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,878] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-49 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,879] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-40 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,886] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-41 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,888] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-38 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,905] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-8 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,906] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-7 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,907] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-33 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,909] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-25 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,910] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-31 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,912] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-23 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,912] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-10 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,918] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-2 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,919] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-17 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,919] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-4 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,922] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-15 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,923] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-26 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,925] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition __consumer_offsets-3 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,930] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,928] INFO [Log partition=marca_topics_v2-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:11,953] INFO Created log for partition marca_topics_v2-0 in /var/lib/kafka/data/marca_topics_v2-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:11,954] INFO [Partition marca_topics_v2-0 broker=1] No checkpointed highwatermark is found for partition marca_topics_v2-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:11,954] INFO [Partition marca_topics_v2-0 broker=1] Log loaded for partition marca_topics_v2-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:11,954] INFO [Broker id=1] Leader marca_topics_v2-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,993] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition sport_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,993] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition mundo_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:11,993] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 1 from controller 1 epoch 1 for the become-leader transition for partition marca_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,107] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=mundo_topics_v2,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=sport_topics_v2,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=marca_topics_v2,partition_index=0,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 1 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,181] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='mundo_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition mundo_topics_v2-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,191] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='sport_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition sport_topics_v2-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,191] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='marca_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition marca_topics_v2-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,196] INFO [Broker id=1] Add 3 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,202] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 2 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,275] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 for 50 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,275] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,275] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,275] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,276] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,276] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,276] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,276] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,276] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,279] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,279] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,279] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,284] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,285] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,285] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,285] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,285] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,286] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,287] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,288] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,288] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,288] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,288] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,288] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,292] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,292] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,292] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,292] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,292] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,293] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,293] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,293] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,301] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,301] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,301] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,302] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,302] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,304] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,305] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,305] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,305] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,305] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,305] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,306] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,306] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,306] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,306] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,306] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,306] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,306] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,307] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 3 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,573] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,573] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,573] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,574] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,574] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,574] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,574] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,574] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,575] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,575] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,575] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,575] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,575] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,576] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,584] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,584] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,585] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,585] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,586] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,587] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,587] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,587] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,587] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,587] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,588] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,588] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,589] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,590] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,590] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,590] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,592] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,594] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,594] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,594] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,595] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,595] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,596] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,598] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,599] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,600] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,600] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,600] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,600] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,600] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,601] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,602] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,602] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,602] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,603] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,605] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 epoch 1 starting the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,610] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-30, __consumer_offsets-25, __consumer_offsets-35, __consumer_offsets-37, __consumer_offsets-38, __consumer_offsets-13, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, __consumer_offsets-27, __consumer_offsets-7, __consumer_offsets-9, __consumer_offsets-46, __consumer_offsets-41, __consumer_offsets-33, __consumer_offsets-23, __consumer_offsets-49, __consumer_offsets-47, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-31, __consumer_offsets-36, __consumer_offsets-42, __consumer_offsets-3, __consumer_offsets-18, __consumer_offsets-15, __consumer_offsets-24, __consumer_offsets-17, __consumer_offsets-48, __consumer_offsets-19, __consumer_offsets-11, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, __consumer_offsets-39, __consumer_offsets-12, __consumer_offsets-45, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, __consumer_offsets-29, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,610] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 1 epoch 1 as part of the become-leader transition for 50 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,623] INFO [Log partition=__consumer_offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,626] INFO Created log for partition __consumer_offsets-3 in /var/lib/kafka/data/__consumer_offsets-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,628] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,630] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,630] INFO [Broker id=1] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,642] INFO [Log partition=__consumer_offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,645] INFO Created log for partition __consumer_offsets-18 in /var/lib/kafka/data/__consumer_offsets-18 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,645] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,645] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,645] INFO [Broker id=1] Leader __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,661] INFO [Log partition=__consumer_offsets-41, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,667] INFO Created log for partition __consumer_offsets-41 in /var/lib/kafka/data/__consumer_offsets-41 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,667] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,668] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,668] INFO [Broker id=1] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,680] INFO [Log partition=__consumer_offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,690] INFO Created log for partition __consumer_offsets-10 in /var/lib/kafka/data/__consumer_offsets-10 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,691] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,691] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,692] INFO [Broker id=1] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,710] INFO [Log partition=__consumer_offsets-33, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,713] INFO Created log for partition __consumer_offsets-33 in /var/lib/kafka/data/__consumer_offsets-33 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,713] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,713] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,719] INFO [Broker id=1] Leader __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,735] INFO [Log partition=__consumer_offsets-48, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,738] INFO Created log for partition __consumer_offsets-48 in /var/lib/kafka/data/__consumer_offsets-48 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,738] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,738] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,738] INFO [Broker id=1] Leader __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,752] INFO [Log partition=__consumer_offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,818] INFO Created log for partition __consumer_offsets-19 in /var/lib/kafka/data/__consumer_offsets-19 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,818] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,820] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,820] INFO [Broker id=1] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,846] INFO [Log partition=__consumer_offsets-34, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,863] INFO Created log for partition __consumer_offsets-34 in /var/lib/kafka/data/__consumer_offsets-34 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,876] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,877] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,877] INFO [Broker id=1] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,894] INFO [Log partition=__consumer_offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,904] INFO Created log for partition __consumer_offsets-4 in /var/lib/kafka/data/__consumer_offsets-4 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,904] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,904] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,904] INFO [Broker id=1] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:12,927] INFO [Log partition=__consumer_offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,941] INFO Created log for partition __consumer_offsets-11 in /var/lib/kafka/data/__consumer_offsets-11 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,941] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,941] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,942] INFO [Broker id=1] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[33mkafka-ui           |[0m 15:46:12.944 [main] INFO  org.springframework.boot.web.embedded.netty.NettyWebServer - Netty started on port(s): 8080
[32mkafka1             |[0m [2021-01-30 15:46:12,955] INFO [Log partition=__consumer_offsets-26, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,958] INFO Created log for partition __consumer_offsets-26 in /var/lib/kafka/data/__consumer_offsets-26 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,959] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,960] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,960] INFO [Broker id=1] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[33mkafka-ui           |[0m 15:46:12.966 [main] INFO  com.provectus.kafka.ui.KafkaUiApplication - Started KafkaUiApplication in 15.718 seconds (JVM running for 21.596)
[32mkafka1             |[0m [2021-01-30 15:46:12,991] INFO [Log partition=__consumer_offsets-49, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:12,994] INFO Created log for partition __consumer_offsets-49 in /var/lib/kafka/data/__consumer_offsets-49 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:12,994] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:12,994] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,006] INFO [Broker id=1] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,023] INFO [Log partition=__consumer_offsets-39, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,027] INFO Created log for partition __consumer_offsets-39 in /var/lib/kafka/data/__consumer_offsets-39 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,028] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,028] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,028] INFO [Broker id=1] Leader __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,055] INFO [Log partition=__consumer_offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,071] INFO Created log for partition __consumer_offsets-9 in /var/lib/kafka/data/__consumer_offsets-9 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,072] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,073] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,073] INFO [Broker id=1] Leader __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,139] INFO [Log partition=__consumer_offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,143] INFO Created log for partition __consumer_offsets-24 in /var/lib/kafka/data/__consumer_offsets-24 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,143] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,143] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,144] INFO [Broker id=1] Leader __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,178] INFO [Log partition=__consumer_offsets-31, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,181] INFO Created log for partition __consumer_offsets-31 in /var/lib/kafka/data/__consumer_offsets-31 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,181] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,181] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,182] INFO [Broker id=1] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,208] INFO [Log partition=__consumer_offsets-46, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,217] INFO Created log for partition __consumer_offsets-46 in /var/lib/kafka/data/__consumer_offsets-46 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,217] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,219] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,220] INFO [Broker id=1] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,237] INFO [Log partition=__consumer_offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,242] INFO Created log for partition __consumer_offsets-1 in /var/lib/kafka/data/__consumer_offsets-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,242] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,244] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,245] INFO [Broker id=1] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,265] INFO [Log partition=__consumer_offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,268] INFO Created log for partition __consumer_offsets-16 in /var/lib/kafka/data/__consumer_offsets-16 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,268] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,270] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,270] INFO [Broker id=1] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,280] INFO [Log partition=__consumer_offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,284] INFO Created log for partition __consumer_offsets-2 in /var/lib/kafka/data/__consumer_offsets-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,285] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,285] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,285] INFO [Broker id=1] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,317] INFO [Log partition=__consumer_offsets-25, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,330] INFO Created log for partition __consumer_offsets-25 in /var/lib/kafka/data/__consumer_offsets-25 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,330] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,331] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,331] INFO [Broker id=1] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,342] INFO [Log partition=__consumer_offsets-40, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,352] INFO Created log for partition __consumer_offsets-40 in /var/lib/kafka/data/__consumer_offsets-40 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,352] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,353] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,355] INFO [Broker id=1] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,374] INFO [Log partition=__consumer_offsets-47, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,377] INFO Created log for partition __consumer_offsets-47 in /var/lib/kafka/data/__consumer_offsets-47 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,377] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,378] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,379] INFO [Broker id=1] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,399] INFO [Log partition=__consumer_offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,403] INFO Created log for partition __consumer_offsets-17 in /var/lib/kafka/data/__consumer_offsets-17 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,403] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,404] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,404] INFO [Broker id=1] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,413] INFO [Log partition=__consumer_offsets-32, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,424] INFO Created log for partition __consumer_offsets-32 in /var/lib/kafka/data/__consumer_offsets-32 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,424] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,431] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,431] INFO [Broker id=1] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,441] INFO [Log partition=__consumer_offsets-37, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,444] INFO Created log for partition __consumer_offsets-37 in /var/lib/kafka/data/__consumer_offsets-37 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,444] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,445] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,446] INFO [Broker id=1] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,463] INFO [Log partition=__consumer_offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,465] INFO Created log for partition __consumer_offsets-7 in /var/lib/kafka/data/__consumer_offsets-7 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,465] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,471] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,471] INFO [Broker id=1] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,490] INFO [Log partition=__consumer_offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,492] INFO Created log for partition __consumer_offsets-22 in /var/lib/kafka/data/__consumer_offsets-22 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,493] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,493] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,494] INFO [Broker id=1] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,510] INFO [Log partition=__consumer_offsets-29, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,515] INFO Created log for partition __consumer_offsets-29 in /var/lib/kafka/data/__consumer_offsets-29 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,515] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,515] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,515] INFO [Broker id=1] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,530] INFO [Log partition=__consumer_offsets-44, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,532] INFO Created log for partition __consumer_offsets-44 in /var/lib/kafka/data/__consumer_offsets-44 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,533] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,536] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,536] INFO [Broker id=1] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,555] INFO [Log partition=__consumer_offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,558] INFO Created log for partition __consumer_offsets-14 in /var/lib/kafka/data/__consumer_offsets-14 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,558] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,559] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,560] INFO [Broker id=1] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,569] INFO [Log partition=__consumer_offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,580] INFO Created log for partition __consumer_offsets-23 in /var/lib/kafka/data/__consumer_offsets-23 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,581] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,581] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,581] INFO [Broker id=1] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,598] INFO [Log partition=__consumer_offsets-38, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,605] INFO Created log for partition __consumer_offsets-38 in /var/lib/kafka/data/__consumer_offsets-38 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,605] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,606] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,607] INFO [Broker id=1] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,632] INFO [Log partition=__consumer_offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,636] INFO Created log for partition __consumer_offsets-8 in /var/lib/kafka/data/__consumer_offsets-8 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,636] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,647] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,648] INFO [Broker id=1] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,670] INFO [Log partition=__consumer_offsets-45, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,692] INFO Created log for partition __consumer_offsets-45 in /var/lib/kafka/data/__consumer_offsets-45 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,693] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,693] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,694] INFO [Broker id=1] Leader __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,713] INFO [Log partition=__consumer_offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,742] INFO Created log for partition __consumer_offsets-15 in /var/lib/kafka/data/__consumer_offsets-15 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,743] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,743] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,744] INFO [Broker id=1] Leader __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,758] INFO [Log partition=__consumer_offsets-30, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,762] INFO Created log for partition __consumer_offsets-30 in /var/lib/kafka/data/__consumer_offsets-30 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,772] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,773] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,773] INFO [Broker id=1] Leader __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,791] INFO [Log partition=__consumer_offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,796] INFO Created log for partition __consumer_offsets-0 in /var/lib/kafka/data/__consumer_offsets-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,796] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,797] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,797] INFO [Broker id=1] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,825] INFO [Log partition=__consumer_offsets-35, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,842] INFO Created log for partition __consumer_offsets-35 in /var/lib/kafka/data/__consumer_offsets-35 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,842] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,842] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,842] INFO [Broker id=1] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,860] INFO [Log partition=__consumer_offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,869] INFO Created log for partition __consumer_offsets-5 in /var/lib/kafka/data/__consumer_offsets-5 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,869] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,875] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,875] INFO [Broker id=1] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,895] INFO [Log partition=__consumer_offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,915] INFO Created log for partition __consumer_offsets-20 in /var/lib/kafka/data/__consumer_offsets-20 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,923] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,923] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,924] INFO [Broker id=1] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,951] INFO [Log partition=__consumer_offsets-27, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,961] INFO Created log for partition __consumer_offsets-27 in /var/lib/kafka/data/__consumer_offsets-27 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,961] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,963] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,968] INFO [Broker id=1] Leader __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:13,983] INFO [Log partition=__consumer_offsets-42, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:13,991] INFO Created log for partition __consumer_offsets-42 in /var/lib/kafka/data/__consumer_offsets-42 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:13,991] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,993] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:13,994] INFO [Broker id=1] Leader __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,012] INFO [Log partition=__consumer_offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:14,028] INFO Created log for partition __consumer_offsets-12 in /var/lib/kafka/data/__consumer_offsets-12 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,028] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,028] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,028] INFO [Broker id=1] Leader __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,051] INFO [Log partition=__consumer_offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:14,060] INFO Created log for partition __consumer_offsets-21 in /var/lib/kafka/data/__consumer_offsets-21 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,060] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,066] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,066] INFO [Broker id=1] Leader __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,079] INFO [Log partition=__consumer_offsets-36, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:14,088] INFO Created log for partition __consumer_offsets-36 in /var/lib/kafka/data/__consumer_offsets-36 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,088] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,092] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,092] INFO [Broker id=1] Leader __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,109] INFO [Log partition=__consumer_offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:14,114] INFO Created log for partition __consumer_offsets-6 in /var/lib/kafka/data/__consumer_offsets-6 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,115] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,115] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,116] INFO [Broker id=1] Leader __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,130] INFO [Log partition=__consumer_offsets-43, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:14,133] INFO Created log for partition __consumer_offsets-43 in /var/lib/kafka/data/__consumer_offsets-43 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,133] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,133] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,134] INFO [Broker id=1] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,173] INFO [Log partition=__consumer_offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:14,188] INFO Created log for partition __consumer_offsets-13 in /var/lib/kafka/data/__consumer_offsets-13 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,188] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,189] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,189] INFO [Broker id=1] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,250] INFO [Log partition=__consumer_offsets-28, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:14,260] INFO Created log for partition __consumer_offsets-28 in /var/lib/kafka/data/__consumer_offsets-28 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 104857600, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,260] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,266] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:14,266] INFO [Broker id=1] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-18 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-41 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-33 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-48 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-19 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-34 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,277] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-11 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,278] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-26 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,279] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-49 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,279] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-39 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,279] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-9 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,280] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-24 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,280] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-31 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,280] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-46 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,280] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,280] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,280] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,280] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-25 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,281] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-40 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,281] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-47 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,281] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-17 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,282] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-32 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,282] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-37 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,283] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-7 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,283] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-22 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,283] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-29 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,283] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-44 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,283] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,283] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-23 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,284] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-38 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,285] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,285] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-45 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-15 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-30 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-35 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-5 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-20 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-27 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,293] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-42 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,294] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,294] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-21 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,294] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-36 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,294] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,295] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-43 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,295] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-13 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,295] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 3 from controller 1 epoch 1 for the become-leader transition for partition __consumer_offsets-28 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,302] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,315] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,315] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,316] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,317] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,317] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,317] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,317] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,318] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,318] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,318] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,319] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,320] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,322] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,322] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,322] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,322] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,322] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,329] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,329] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,329] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,330] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,330] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,330] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,330] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,330] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,331] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,331] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,331] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,332] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,332] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,332] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,339] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,340] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,340] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,342] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,342] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 35 milliseconds, of which 12 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,348] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,354] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,355] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,356] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,355] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 40 milliseconds, of which 39 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,357] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,357] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,358] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,358] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,359] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,359] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 42 milliseconds, of which 41 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,360] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 44 milliseconds, of which 44 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,360] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,362] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,365] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 48 milliseconds, of which 48 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,366] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,369] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 52 milliseconds, of which 52 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,370] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,371] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,371] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 54 milliseconds, of which 54 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,374] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 57 milliseconds, of which 57 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,375] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 57 milliseconds, of which 56 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,376] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 58 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,377] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 59 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,377] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 58 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,379] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 58 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 58 milliseconds, of which 57 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,380] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 58 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,381] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=__consumer_offsets,partition_index=13,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=46,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=9,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=42,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=21,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=17,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=30,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=26,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=5,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=38,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=34,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=16,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=45,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=12,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=41,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=24,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=20,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=49,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=29,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=25,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=8,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=37,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=33,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=15,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=48,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=11,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=44,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=23,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=19,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=32,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=28,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=7,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=40,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=36,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=47,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=14,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=43,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=10,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=22,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=18,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=31,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=27,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=39,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=6,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=35,error_code=0,_tagged_fields={}},{topic_name=__consumer_offsets,partition_index=2,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 3 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,383] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 61 milliseconds, of which 61 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,387] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 65 milliseconds, of which 61 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,388] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 59 milliseconds, of which 58 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,389] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 60 milliseconds, of which 59 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,389] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 60 milliseconds, of which 60 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,390] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 61 milliseconds, of which 61 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,391] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,395] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 64 milliseconds, of which 61 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,396] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=46, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-46 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=42, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-42 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=30, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-30 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 67 milliseconds, of which 66 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=26, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-26 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,397] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=38, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-38 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=34, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-34 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 68 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=45, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-45 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=41, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-41 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,398] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=49, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-49 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=29, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-29 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=25, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-25 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=37, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-37 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,399] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=33, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-33 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,400] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 70 milliseconds, of which 68 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,401] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 69 milliseconds, of which 69 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,401] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=48, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-48 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=44, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-44 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 70 milliseconds, of which 70 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=32, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-32 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=28, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-28 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=40, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-40 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,402] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=36, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-36 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=47, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-47 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=43, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-43 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 72 milliseconds, of which 71 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,408] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 77 milliseconds, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,403] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,409] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 77 milliseconds, of which 77 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,410] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=31, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-31 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,410] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=27, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-27 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,410] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=39, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-39 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,410] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,410] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=35, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-35 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,411] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 78 milliseconds, of which 78 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,411] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='__consumer_offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition __consumer_offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,411] INFO [Broker id=1] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,411] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 79 milliseconds, of which 79 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,412] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 72 milliseconds, of which 72 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,413] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 73 milliseconds, of which 72 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,414] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 73 milliseconds, of which 72 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,415] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 4 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:14,416] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 74 milliseconds, of which 74 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,421] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 67 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 67 milliseconds, of which 66 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,422] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 66 milliseconds, of which 66 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,423] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 67 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,424] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 67 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,425] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 67 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,426] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 67 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,430] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 72 milliseconds, of which 72 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,431] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 72 milliseconds, of which 72 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,432] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 71 milliseconds, of which 70 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,432] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 70 milliseconds, of which 70 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,433] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 67 milliseconds, of which 67 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,434] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 64 milliseconds, of which 64 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,439] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 68 milliseconds, of which 64 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka1             |[0m [2021-01-30 15:46:14,684] INFO [Controller id=1] Processing automatic preferred replica leader election (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:14,686] TRACE [Controller id=1] Checking need to trigger auto leader balancing (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:14,695] DEBUG [Controller id=1] Topics not in preferred replica for broker 1 HashMap() (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:14,698] TRACE [Controller id=1] Leader imbalance ratio for broker 1 is 0.0 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:14,931] INFO [GroupCoordinator 1]: Preparing to rebalance group group-string-id2 in state PreparingRebalance with old generation 0 (__consumer_offsets-17) (reason: Adding new member cons-string-id2-36da344c-20f5-4a2e-8cfc-663897445d0a with group instance id None) (kafka.coordinator.group.GroupCoordinator)
[33mkafka-ui           |[0m 15:46:15.009 [ZkClient-EventThread-96-null] INFO  org.I0Itec.zkclient.ZkEventThread - Starting ZkClient event thread.
[32mkafka1             |[0m [2021-01-30 15:46:15,035] INFO [GroupCoordinator 1]: Stabilized group group-string-id2 generation 1 (__consumer_offsets-17) (kafka.coordinator.group.GroupCoordinator)
[33mkafka-ui           |[0m 15:46:15.068 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT
[33mkafka-ui           |[0m 15:46:15.069 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:host.name=e3ce176159c4
[33mkafka-ui           |[0m 15:46:15.070 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.version=13.0.2
[33mkafka-ui           |[0m 15:46:15.071 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.vendor=Oracle Corporation
[33mkafka-ui           |[0m 15:46:15.072 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.home=/usr/java/openjdk-13
[33mkafka-ui           |[0m 15:46:15.075 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.class.path=kafka-ui-api.jar
[33mkafka-ui           |[0m 15:46:15.077 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
[33mkafka-ui           |[0m 15:46:15.079 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.io.tmpdir=/tmp
[33mkafka-ui           |[0m 15:46:15.084 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:java.compiler=<NA>
[33mkafka-ui           |[0m 15:46:15.087 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.name=Linux
[33mkafka-ui           |[0m 15:46:15.087 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.arch=amd64
[33mkafka-ui           |[0m 15:46:15.089 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.version=4.19.121-linuxkit
[33mkafka-ui           |[0m 15:46:15.090 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.name=root
[33mkafka-ui           |[0m 15:46:15.091 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.home=/root
[33mkafka-ui           |[0m 15:46:15.092 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:user.dir=/
[33mkafka-ui           |[0m 15:46:15.093 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.memory.free=79MB
[33mkafka-ui           |[0m 15:46:15.093 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.memory.max=2998MB
[33mkafka-ui           |[0m 15:46:15.094 [kafka-admin-client-thread | adminclient-1] INFO  org.apache.zookeeper.ZooKeeper - Client environment:os.memory.total=130MB
[33mkafka-ui           |[0m 15:46:15.101 [ZkClient-EventThread-96-null] INFO  org.I0Itec.zkclient.ZkEventThread - Terminate ZkClient event thread.
[33mkafka-ui           |[0m 15:46:15.101 [kafka-admin-client-thread | adminclient-1] ERROR com.provectus.kafka.ui.zookeeper.ZookeeperService - Error while creating zookeeper client for cluster Broker1
[33mkafka-ui           |[0m 15:46:15.102 [kafka-admin-client-thread | adminclient-1] DEBUG com.provectus.kafka.ui.zookeeper.ZookeeperService - Start getting Zookeeper metrics for kafkaCluster: Broker1
[35mschema-registry    |[0m [2021-01-30 15:46:15,227] INFO SchemaRegistryConfig values: 
[35mschema-registry    |[0m 	access.control.allow.headers = 
[35mschema-registry    |[0m 	access.control.allow.methods = 
[35mschema-registry    |[0m 	access.control.allow.origin = 
[35mschema-registry    |[0m 	access.control.skip.options = true
[35mschema-registry    |[0m 	authentication.method = NONE
[35mschema-registry    |[0m 	authentication.realm = 
[35mschema-registry    |[0m 	authentication.roles = [*]
[35mschema-registry    |[0m 	authentication.skip.paths = []
[35mschema-registry    |[0m 	avro.compatibility.level = 
[35mschema-registry    |[0m 	compression.enable = true
[35mschema-registry    |[0m 	debug = false
[35mschema-registry    |[0m 	host.name = schema-registry
[35mschema-registry    |[0m 	idle.timeout.ms = 30000
[35mschema-registry    |[0m 	inter.instance.headers.whitelist = []
[35mschema-registry    |[0m 	inter.instance.protocol = http
[35mschema-registry    |[0m 	kafkastore.bootstrap.servers = [PLAINTEXT://kafka1:9092]
[35mschema-registry    |[0m 	kafkastore.connection.url = zookeeper:2181
[35mschema-registry    |[0m 	kafkastore.group.id = 
[35mschema-registry    |[0m 	kafkastore.init.timeout.ms = 60000
[35mschema-registry    |[0m 	kafkastore.sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mschema-registry    |[0m 	kafkastore.sasl.kerberos.min.time.before.relogin = 60000
[35mschema-registry    |[0m 	kafkastore.sasl.kerberos.service.name = 
[35mschema-registry    |[0m 	kafkastore.sasl.kerberos.ticket.renew.jitter = 0.05
[35mschema-registry    |[0m 	kafkastore.sasl.kerberos.ticket.renew.window.factor = 0.8
[35mschema-registry    |[0m 	kafkastore.sasl.mechanism = GSSAPI
[35mschema-registry    |[0m 	kafkastore.security.protocol = PLAINTEXT
[35mschema-registry    |[0m 	kafkastore.ssl.cipher.suites = 
[35mschema-registry    |[0m 	kafkastore.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
[35mschema-registry    |[0m 	kafkastore.ssl.endpoint.identification.algorithm = 
[35mschema-registry    |[0m 	kafkastore.ssl.key.password = [hidden]
[35mschema-registry    |[0m 	kafkastore.ssl.keymanager.algorithm = SunX509
[35mschema-registry    |[0m 	kafkastore.ssl.keystore.location = 
[35mschema-registry    |[0m 	kafkastore.ssl.keystore.password = [hidden]
[35mschema-registry    |[0m 	kafkastore.ssl.keystore.type = JKS
[35mschema-registry    |[0m 	kafkastore.ssl.protocol = TLS
[35mschema-registry    |[0m 	kafkastore.ssl.provider = 
[35mschema-registry    |[0m 	kafkastore.ssl.trustmanager.algorithm = PKIX
[35mschema-registry    |[0m 	kafkastore.ssl.truststore.location = 
[35mschema-registry    |[0m 	kafkastore.ssl.truststore.password = [hidden]
[35mschema-registry    |[0m 	kafkastore.ssl.truststore.type = JKS
[35mschema-registry    |[0m 	kafkastore.timeout.ms = 500
[35mschema-registry    |[0m 	kafkastore.topic = _schemas
[35mschema-registry    |[0m 	kafkastore.topic.replication.factor = 3
[35mschema-registry    |[0m 	kafkastore.update.handlers = []
[35mschema-registry    |[0m 	kafkastore.write.max.retries = 5
[35mschema-registry    |[0m 	kafkastore.zk.session.timeout.ms = 30000
[35mschema-registry    |[0m 	leader.eligibility = true
[35mschema-registry    |[0m 	listeners = []
[35mschema-registry    |[0m 	master.eligibility = null
[35mschema-registry    |[0m 	metric.reporters = []
[35mschema-registry    |[0m 	metrics.jmx.prefix = kafka.schema.registry
[35mschema-registry    |[0m 	metrics.num.samples = 2
[35mschema-registry    |[0m 	metrics.sample.window.ms = 30000
[35mschema-registry    |[0m 	metrics.tag.map = []
[35mschema-registry    |[0m 	mode.mutability = false
[35mschema-registry    |[0m 	port = 8081
[35mschema-registry    |[0m 	request.logger.name = io.confluent.rest-utils.requests
[35mschema-registry    |[0m 	request.queue.capacity = 2147483647
[35mschema-registry    |[0m 	request.queue.capacity.growby = 64
[35mschema-registry    |[0m 	request.queue.capacity.init = 128
[35mschema-registry    |[0m 	resource.extension.class = []
[35mschema-registry    |[0m 	resource.extension.classes = []
[35mschema-registry    |[0m 	resource.static.locations = []
[35mschema-registry    |[0m 	response.http.headers.config = 
[35mschema-registry    |[0m 	response.mediatype.default = application/vnd.schemaregistry.v1+json
[35mschema-registry    |[0m 	response.mediatype.preferred = [application/vnd.schemaregistry.v1+json, application/vnd.schemaregistry+json, application/json]
[35mschema-registry    |[0m 	rest.servlet.initializor.classes = []
[35mschema-registry    |[0m 	schema.compatibility.level = backward
[35mschema-registry    |[0m 	schema.providers = []
[35mschema-registry    |[0m 	schema.registry.group.id = schema-registry
[35mschema-registry    |[0m 	schema.registry.inter.instance.protocol = 
[35mschema-registry    |[0m 	schema.registry.resource.extension.class = []
[35mschema-registry    |[0m 	schema.registry.zk.namespace = schema_registry
[35mschema-registry    |[0m 	shutdown.graceful.ms = 1000
[35mschema-registry    |[0m 	ssl.cipher.suites = []
[35mschema-registry    |[0m 	ssl.client.auth = false
[35mschema-registry    |[0m 	ssl.client.authentication = NONE
[35mschema-registry    |[0m 	ssl.enabled.protocols = []
[35mschema-registry    |[0m 	ssl.endpoint.identification.algorithm = null
[35mschema-registry    |[0m 	ssl.key.password = [hidden]
[35mschema-registry    |[0m 	ssl.keymanager.algorithm = 
[35mschema-registry    |[0m 	ssl.keystore.location = 
[35mschema-registry    |[0m 	ssl.keystore.password = [hidden]
[35mschema-registry    |[0m 	ssl.keystore.reload = false
[35mschema-registry    |[0m 	ssl.keystore.type = JKS
[35mschema-registry    |[0m 	ssl.keystore.watch.location = 
[35mschema-registry    |[0m 	ssl.protocol = TLS
[35mschema-registry    |[0m 	ssl.provider = 
[35mschema-registry    |[0m 	ssl.trustmanager.algorithm = 
[35mschema-registry    |[0m 	ssl.truststore.location = 
[35mschema-registry    |[0m 	ssl.truststore.password = [hidden]
[35mschema-registry    |[0m 	ssl.truststore.type = JKS
[35mschema-registry    |[0m 	thread.pool.max = 200
[35mschema-registry    |[0m 	thread.pool.min = 8
[35mschema-registry    |[0m 	websocket.path.prefix = /ws
[35mschema-registry    |[0m 	websocket.servlet.initializor.classes = []
[35mschema-registry    |[0m 	zookeeper.set.acl = false
[35mschema-registry    |[0m  (io.confluent.kafka.schemaregistry.rest.SchemaRegistryConfig)
[32mkafka1             |[0m [2021-01-30 15:46:15,263] INFO [GroupCoordinator 1]: Assignment received from leader for group group-string-id2 for generation 1 (kafka.coordinator.group.GroupCoordinator)
[35mschema-registry    |[0m [2021-01-30 15:46:15,350] INFO Logging initialized @3666ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
[35mschema-registry    |[0m [2021-01-30 15:46:15,381] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
[35mschema-registry    |[0m [2021-01-30 15:46:15,544] WARN DEPRECATION warning: `listeners` configuration is not configured. Falling back to the deprecated `port` configuration. (io.confluent.rest.ApplicationServer)
[35mschema-registry    |[0m [2021-01-30 15:46:15,545] INFO Adding listener: http://0.0.0.0:8081 (io.confluent.rest.ApplicationServer)
[35mschema-registry    |[0m [2021-01-30 15:46:15,928] WARN DEPRECATION warning: `listeners` configuration is not configured. Falling back to the deprecated `port` configuration. (io.confluent.rest.ApplicationServer)
[35mschema-registry    |[0m [2021-01-30 15:46:15,957] INFO AdminClientConfig values: 
[35mschema-registry    |[0m 	bootstrap.servers = [PLAINTEXT://kafka1:9092]
[35mschema-registry    |[0m 	client.dns.lookup = use_all_dns_ips
[35mschema-registry    |[0m 	client.id = 
[35mschema-registry    |[0m 	connections.max.idle.ms = 300000
[35mschema-registry    |[0m 	default.api.timeout.ms = 60000
[35mschema-registry    |[0m 	metadata.max.age.ms = 300000
[35mschema-registry    |[0m 	metric.reporters = []
[35mschema-registry    |[0m 	metrics.num.samples = 2
[35mschema-registry    |[0m 	metrics.recording.level = INFO
[35mschema-registry    |[0m 	metrics.sample.window.ms = 30000
[35mschema-registry    |[0m 	receive.buffer.bytes = 65536
[35mschema-registry    |[0m 	reconnect.backoff.max.ms = 1000
[35mschema-registry    |[0m 	reconnect.backoff.ms = 50
[35mschema-registry    |[0m 	request.timeout.ms = 30000
[35mschema-registry    |[0m 	retries = 2147483647
[35mschema-registry    |[0m 	retry.backoff.ms = 100
[35mschema-registry    |[0m 	sasl.client.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.jaas.config = null
[35mschema-registry    |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mschema-registry    |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mschema-registry    |[0m 	sasl.kerberos.service.name = null
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.login.class = null
[35mschema-registry    |[0m 	sasl.login.refresh.buffer.seconds = 300
[35mschema-registry    |[0m 	sasl.login.refresh.min.period.seconds = 60
[35mschema-registry    |[0m 	sasl.login.refresh.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.refresh.window.jitter = 0.05
[35mschema-registry    |[0m 	sasl.mechanism = GSSAPI
[35mschema-registry    |[0m 	security.protocol = PLAINTEXT
[35mschema-registry    |[0m 	security.providers = null
[35mschema-registry    |[0m 	send.buffer.bytes = 131072
[35mschema-registry    |[0m 	ssl.cipher.suites = null
[35mschema-registry    |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[35mschema-registry    |[0m 	ssl.endpoint.identification.algorithm = https
[35mschema-registry    |[0m 	ssl.engine.factory.class = null
[35mschema-registry    |[0m 	ssl.key.password = null
[35mschema-registry    |[0m 	ssl.keymanager.algorithm = SunX509
[35mschema-registry    |[0m 	ssl.keystore.location = null
[35mschema-registry    |[0m 	ssl.keystore.password = null
[35mschema-registry    |[0m 	ssl.keystore.type = JKS
[35mschema-registry    |[0m 	ssl.protocol = TLSv1.3
[35mschema-registry    |[0m 	ssl.provider = null
[35mschema-registry    |[0m 	ssl.secure.random.implementation = null
[35mschema-registry    |[0m 	ssl.trustmanager.algorithm = PKIX
[35mschema-registry    |[0m 	ssl.truststore.location = null
[35mschema-registry    |[0m 	ssl.truststore.password = null
[35mschema-registry    |[0m 	ssl.truststore.type = JKS
[35mschema-registry    |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:16,072] WARN The configuration 'connection.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:16,075] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,076] INFO Kafka commitId: 5e516110bd85c6e3 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,076] INFO Kafka startTimeMs: 1612021576073 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m Sat Jan 30 15:46:16 UTC 2021  Kafka Connect listener HTTP state:  000  (waiting for 200)
[35mschema-registry    |[0m [2021-01-30 15:46:16,693] INFO Registering schema provider for AVRO: io.confluent.kafka.schemaregistry.avro.AvroSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
[35mschema-registry    |[0m [2021-01-30 15:46:16,693] INFO Registering schema provider for JSON: io.confluent.kafka.schemaregistry.json.JsonSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
[35mschema-registry    |[0m [2021-01-30 15:46:16,694] INFO Registering schema provider for PROTOBUF: io.confluent.kafka.schemaregistry.protobuf.ProtobufSchemaProvider (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
[35mschema-registry    |[0m [2021-01-30 15:46:16,701] WARN DEPRECATION warning: `listeners` configuration is not configured. Falling back to the deprecated `port` configuration. (io.confluent.rest.ApplicationServer)
[35mschema-registry    |[0m [2021-01-30 15:46:16,702] INFO Initializing KafkaStore with broker endpoints: PLAINTEXT://kafka1:9092 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[35mschema-registry    |[0m [2021-01-30 15:46:16,703] INFO AdminClientConfig values: 
[35mschema-registry    |[0m 	bootstrap.servers = [PLAINTEXT://kafka1:9092]
[35mschema-registry    |[0m 	client.dns.lookup = use_all_dns_ips
[35mschema-registry    |[0m 	client.id = 
[35mschema-registry    |[0m 	connections.max.idle.ms = 300000
[35mschema-registry    |[0m 	default.api.timeout.ms = 60000
[35mschema-registry    |[0m 	metadata.max.age.ms = 300000
[35mschema-registry    |[0m 	metric.reporters = []
[35mschema-registry    |[0m 	metrics.num.samples = 2
[35mschema-registry    |[0m 	metrics.recording.level = INFO
[35mschema-registry    |[0m 	metrics.sample.window.ms = 30000
[35mschema-registry    |[0m 	receive.buffer.bytes = 65536
[35mschema-registry    |[0m 	reconnect.backoff.max.ms = 1000
[35mschema-registry    |[0m 	reconnect.backoff.ms = 50
[35mschema-registry    |[0m 	request.timeout.ms = 30000
[35mschema-registry    |[0m 	retries = 2147483647
[35mschema-registry    |[0m 	retry.backoff.ms = 100
[35mschema-registry    |[0m 	sasl.client.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.jaas.config = null
[35mschema-registry    |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mschema-registry    |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mschema-registry    |[0m 	sasl.kerberos.service.name = null
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.login.class = null
[35mschema-registry    |[0m 	sasl.login.refresh.buffer.seconds = 300
[35mschema-registry    |[0m 	sasl.login.refresh.min.period.seconds = 60
[35mschema-registry    |[0m 	sasl.login.refresh.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.refresh.window.jitter = 0.05
[35mschema-registry    |[0m 	sasl.mechanism = GSSAPI
[35mschema-registry    |[0m 	security.protocol = PLAINTEXT
[35mschema-registry    |[0m 	security.providers = null
[35mschema-registry    |[0m 	send.buffer.bytes = 131072
[35mschema-registry    |[0m 	ssl.cipher.suites = null
[35mschema-registry    |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[35mschema-registry    |[0m 	ssl.endpoint.identification.algorithm = https
[35mschema-registry    |[0m 	ssl.engine.factory.class = null
[35mschema-registry    |[0m 	ssl.key.password = null
[35mschema-registry    |[0m 	ssl.keymanager.algorithm = SunX509
[35mschema-registry    |[0m 	ssl.keystore.location = null
[35mschema-registry    |[0m 	ssl.keystore.password = null
[35mschema-registry    |[0m 	ssl.keystore.type = JKS
[35mschema-registry    |[0m 	ssl.protocol = TLSv1.3
[35mschema-registry    |[0m 	ssl.provider = null
[35mschema-registry    |[0m 	ssl.secure.random.implementation = null
[35mschema-registry    |[0m 	ssl.trustmanager.algorithm = PKIX
[35mschema-registry    |[0m 	ssl.truststore.location = null
[35mschema-registry    |[0m 	ssl.truststore.password = null
[35mschema-registry    |[0m 	ssl.truststore.type = JKS
[35mschema-registry    |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:16,713] WARN The configuration 'connection.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:16,713] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,713] INFO Kafka commitId: 5e516110bd85c6e3 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,713] INFO Kafka startTimeMs: 1612021576713 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,741] INFO Creating schemas topic _schemas (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[35mschema-registry    |[0m [2021-01-30 15:46:16,744] WARN Creating the schema topic _schemas using a replication factor of 1, which is less than the desired one of 3. If this is a production environment, it's crucial to add more brokers and increase the replication factor of the topic. (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[32mkafka1             |[0m [2021-01-30 15:46:16,770] INFO Creating topic _schemas with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:16,784] INFO [Controller id=1] New topics: [HashSet(_schemas)], deleted topics: [HashSet()], new partition replica assignment [Map(_schemas-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:16,784] INFO [Controller id=1] New partition creation callback for _schemas-0 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:16,784] INFO [Controller id=1 epoch=1] Changed partition _schemas-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,785] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,785] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _schemas-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,785] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,796] INFO [Controller id=1 epoch=1] Changed partition _schemas-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,796] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition _schemas-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,797] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,797] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,798] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _schemas-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,798] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,799] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,799] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 5 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,800] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 1 epoch 1 starting the become-leader transition for partition _schemas-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,800] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_schemas-0) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:16,800] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,805] INFO [Log partition=_schemas-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:16,806] INFO Created log for partition _schemas-0 in /var/lib/kafka/data/_schemas-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:16,807] INFO [Partition _schemas-0 broker=1] No checkpointed highwatermark is found for partition _schemas-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:16,807] INFO [Partition _schemas-0 broker=1] Log loaded for partition _schemas-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:16,807] INFO [Broker id=1] Leader _schemas-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,810] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 5 from controller 1 epoch 1 for the become-leader transition for partition _schemas-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,812] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=_schemas,partition_index=0,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 5 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,815] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='_schemas', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _schemas-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,815] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:16,825] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 6 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[35mschema-registry    |[0m [2021-01-30 15:46:16,853] INFO ProducerConfig values: 
[35mschema-registry    |[0m 	acks = -1
[35mschema-registry    |[0m 	batch.size = 16384
[35mschema-registry    |[0m 	bootstrap.servers = [PLAINTEXT://kafka1:9092]
[35mschema-registry    |[0m 	buffer.memory = 33554432
[35mschema-registry    |[0m 	client.dns.lookup = use_all_dns_ips
[35mschema-registry    |[0m 	client.id = producer-1
[35mschema-registry    |[0m 	compression.type = none
[35mschema-registry    |[0m 	connections.max.idle.ms = 540000
[35mschema-registry    |[0m 	delivery.timeout.ms = 120000
[35mschema-registry    |[0m 	enable.idempotence = false
[35mschema-registry    |[0m 	interceptor.classes = []
[35mschema-registry    |[0m 	internal.auto.downgrade.txn.commit = false
[35mschema-registry    |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[35mschema-registry    |[0m 	linger.ms = 0
[35mschema-registry    |[0m 	max.block.ms = 60000
[35mschema-registry    |[0m 	max.in.flight.requests.per.connection = 5
[35mschema-registry    |[0m 	max.request.size = 1048576
[35mschema-registry    |[0m 	metadata.max.age.ms = 300000
[35mschema-registry    |[0m 	metadata.max.idle.ms = 300000
[35mschema-registry    |[0m 	metric.reporters = []
[35mschema-registry    |[0m 	metrics.num.samples = 2
[35mschema-registry    |[0m 	metrics.recording.level = INFO
[35mschema-registry    |[0m 	metrics.sample.window.ms = 30000
[35mschema-registry    |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[35mschema-registry    |[0m 	receive.buffer.bytes = 32768
[35mschema-registry    |[0m 	reconnect.backoff.max.ms = 1000
[35mschema-registry    |[0m 	reconnect.backoff.ms = 50
[35mschema-registry    |[0m 	request.timeout.ms = 30000
[35mschema-registry    |[0m 	retries = 0
[35mschema-registry    |[0m 	retry.backoff.ms = 100
[35mschema-registry    |[0m 	sasl.client.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.jaas.config = null
[35mschema-registry    |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mschema-registry    |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mschema-registry    |[0m 	sasl.kerberos.service.name = null
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.login.class = null
[35mschema-registry    |[0m 	sasl.login.refresh.buffer.seconds = 300
[35mschema-registry    |[0m 	sasl.login.refresh.min.period.seconds = 60
[35mschema-registry    |[0m 	sasl.login.refresh.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.refresh.window.jitter = 0.05
[35mschema-registry    |[0m 	sasl.mechanism = GSSAPI
[35mschema-registry    |[0m 	security.protocol = PLAINTEXT
[35mschema-registry    |[0m 	security.providers = null
[35mschema-registry    |[0m 	send.buffer.bytes = 131072
[35mschema-registry    |[0m 	ssl.cipher.suites = null
[35mschema-registry    |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[35mschema-registry    |[0m 	ssl.endpoint.identification.algorithm = https
[35mschema-registry    |[0m 	ssl.engine.factory.class = null
[35mschema-registry    |[0m 	ssl.key.password = null
[35mschema-registry    |[0m 	ssl.keymanager.algorithm = SunX509
[35mschema-registry    |[0m 	ssl.keystore.location = null
[35mschema-registry    |[0m 	ssl.keystore.password = null
[35mschema-registry    |[0m 	ssl.keystore.type = JKS
[35mschema-registry    |[0m 	ssl.protocol = TLSv1.3
[35mschema-registry    |[0m 	ssl.provider = null
[35mschema-registry    |[0m 	ssl.secure.random.implementation = null
[35mschema-registry    |[0m 	ssl.trustmanager.algorithm = PKIX
[35mschema-registry    |[0m 	ssl.truststore.location = null
[35mschema-registry    |[0m 	ssl.truststore.password = null
[35mschema-registry    |[0m 	ssl.truststore.type = JKS
[35mschema-registry    |[0m 	transaction.timeout.ms = 60000
[35mschema-registry    |[0m 	transactional.id = null
[35mschema-registry    |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[35mschema-registry    |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:16,893] WARN The configuration 'connection.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:16,896] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,896] INFO Kafka commitId: 5e516110bd85c6e3 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,896] INFO Kafka startTimeMs: 1612021576893 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:16,911] INFO [Producer clientId=producer-1] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[35mschema-registry    |[0m [2021-01-30 15:46:16,940] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[35mschema-registry    |[0m [2021-01-30 15:46:16,941] INFO Kafka store reader thread starting consumer (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
[35mschema-registry    |[0m [2021-01-30 15:46:16,950] INFO ConsumerConfig values: 
[35mschema-registry    |[0m 	allow.auto.create.topics = true
[35mschema-registry    |[0m 	auto.commit.interval.ms = 5000
[35mschema-registry    |[0m 	auto.offset.reset = earliest
[35mschema-registry    |[0m 	bootstrap.servers = [PLAINTEXT://kafka1:9092]
[35mschema-registry    |[0m 	check.crcs = true
[35mschema-registry    |[0m 	client.dns.lookup = use_all_dns_ips
[35mschema-registry    |[0m 	client.id = KafkaStore-reader-_schemas
[35mschema-registry    |[0m 	client.rack = 
[35mschema-registry    |[0m 	connections.max.idle.ms = 540000
[35mschema-registry    |[0m 	default.api.timeout.ms = 60000
[35mschema-registry    |[0m 	enable.auto.commit = false
[35mschema-registry    |[0m 	exclude.internal.topics = true
[35mschema-registry    |[0m 	fetch.max.bytes = 52428800
[35mschema-registry    |[0m 	fetch.max.wait.ms = 500
[35mschema-registry    |[0m 	fetch.min.bytes = 1
[35mschema-registry    |[0m 	group.id = schema-registry-schema-registry-8081
[35mschema-registry    |[0m 	group.instance.id = null
[35mschema-registry    |[0m 	heartbeat.interval.ms = 3000
[35mschema-registry    |[0m 	interceptor.classes = []
[35mschema-registry    |[0m 	internal.leave.group.on.close = true
[35mschema-registry    |[0m 	internal.throw.on.fetch.stable.offset.unsupported = false
[35mschema-registry    |[0m 	isolation.level = read_uncommitted
[35mschema-registry    |[0m 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[35mschema-registry    |[0m 	max.partition.fetch.bytes = 1048576
[35mschema-registry    |[0m 	max.poll.interval.ms = 300000
[35mschema-registry    |[0m 	max.poll.records = 500
[35mschema-registry    |[0m 	metadata.max.age.ms = 300000
[35mschema-registry    |[0m 	metric.reporters = []
[35mschema-registry    |[0m 	metrics.num.samples = 2
[35mschema-registry    |[0m 	metrics.recording.level = INFO
[35mschema-registry    |[0m 	metrics.sample.window.ms = 30000
[35mschema-registry    |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[35mschema-registry    |[0m 	receive.buffer.bytes = 65536
[35mschema-registry    |[0m 	reconnect.backoff.max.ms = 1000
[35mschema-registry    |[0m 	reconnect.backoff.ms = 50
[35mschema-registry    |[0m 	request.timeout.ms = 30000
[35mschema-registry    |[0m 	retry.backoff.ms = 100
[35mschema-registry    |[0m 	sasl.client.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.jaas.config = null
[35mschema-registry    |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mschema-registry    |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mschema-registry    |[0m 	sasl.kerberos.service.name = null
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mschema-registry    |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.callback.handler.class = null
[35mschema-registry    |[0m 	sasl.login.class = null
[35mschema-registry    |[0m 	sasl.login.refresh.buffer.seconds = 300
[35mschema-registry    |[0m 	sasl.login.refresh.min.period.seconds = 60
[35mschema-registry    |[0m 	sasl.login.refresh.window.factor = 0.8
[35mschema-registry    |[0m 	sasl.login.refresh.window.jitter = 0.05
[35mschema-registry    |[0m 	sasl.mechanism = GSSAPI
[35mschema-registry    |[0m 	security.protocol = PLAINTEXT
[35mschema-registry    |[0m 	security.providers = null
[35mschema-registry    |[0m 	send.buffer.bytes = 131072
[35mschema-registry    |[0m 	session.timeout.ms = 10000
[35mschema-registry    |[0m 	ssl.cipher.suites = null
[35mschema-registry    |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[35mschema-registry    |[0m 	ssl.endpoint.identification.algorithm = https
[35mschema-registry    |[0m 	ssl.engine.factory.class = null
[35mschema-registry    |[0m 	ssl.key.password = null
[35mschema-registry    |[0m 	ssl.keymanager.algorithm = SunX509
[35mschema-registry    |[0m 	ssl.keystore.location = null
[35mschema-registry    |[0m 	ssl.keystore.password = null
[35mschema-registry    |[0m 	ssl.keystore.type = JKS
[35mschema-registry    |[0m 	ssl.protocol = TLSv1.3
[35mschema-registry    |[0m 	ssl.provider = null
[35mschema-registry    |[0m 	ssl.secure.random.implementation = null
[35mschema-registry    |[0m 	ssl.trustmanager.algorithm = PKIX
[35mschema-registry    |[0m 	ssl.truststore.location = null
[35mschema-registry    |[0m 	ssl.truststore.password = null
[35mschema-registry    |[0m 	ssl.truststore.type = JKS
[35mschema-registry    |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[35mschema-registry    |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:17,018] WARN The configuration 'connection.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[35mschema-registry    |[0m [2021-01-30 15:46:17,018] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:17,019] INFO Kafka commitId: 5e516110bd85c6e3 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:17,019] INFO Kafka startTimeMs: 1612021577018 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:17,034] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[35mschema-registry    |[0m [2021-01-30 15:46:17,045] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Subscribed to partition(s): _schemas-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[35mschema-registry    |[0m [2021-01-30 15:46:17,052] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Seeking to EARLIEST offset of partition _schemas-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[35mschema-registry    |[0m [2021-01-30 15:46:17,053] INFO Initialized last consumed offset to -1 (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
[35mschema-registry    |[0m [2021-01-30 15:46:17,057] INFO [kafka-store-reader-thread-_schemas]: Starting (io.confluent.kafka.schemaregistry.storage.KafkaStoreReaderThread)
[35mschema-registry    |[0m [2021-01-30 15:46:17,122] INFO [Consumer clientId=KafkaStore-reader-_schemas, groupId=schema-registry-schema-registry-8081] Resetting offset for partition _schemas-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[35mschema-registry    |[0m [2021-01-30 15:46:17,248] INFO Wait to catch up until the offset at 0 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[35mschema-registry    |[0m [2021-01-30 15:46:17,330] INFO Joining schema registry with Kafka-based coordination (io.confluent.kafka.schemaregistry.storage.KafkaSchemaRegistry)
[35mschema-registry    |[0m [2021-01-30 15:46:17,343] INFO Kafka version: 6.0.1-ce (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:17,343] INFO Kafka commitId: 5e516110bd85c6e3 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:17,344] INFO Kafka startTimeMs: 1612021577343 (org.apache.kafka.common.utils.AppInfoParser)
[35mschema-registry    |[0m [2021-01-30 15:46:17,364] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[35mschema-registry    |[0m [2021-01-30 15:46:17,365] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Discovered group coordinator kafka1:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[35mschema-registry    |[0m [2021-01-30 15:46:17,368] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[35mschema-registry    |[0m [2021-01-30 15:46:17,393] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[35mschema-registry    |[0m org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
[35mschema-registry    |[0m [2021-01-30 15:46:17,397] INFO [Schema registry clientId=sr-1, groupId=schema-registry] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:17,400] INFO [GroupCoordinator 1]: Preparing to rebalance group schema-registry in state PreparingRebalance with old generation 0 (__consumer_offsets-29) (reason: Adding new member sr-1-0642ad4c-f054-4f3c-94e0-5227c94a846e with group instance id None) (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:17,403] INFO [GroupCoordinator 1]: Stabilized group schema-registry generation 1 (__consumer_offsets-29) (kafka.coordinator.group.GroupCoordinator)
[35mschema-registry    |[0m [2021-01-30 15:46:17,405] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Successfully joined group with generation Generation{generationId=1, memberId='sr-1-0642ad4c-f054-4f3c-94e0-5227c94a846e', protocol='v0'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:17,429] INFO [GroupCoordinator 1]: Assignment received from leader for group schema-registry for generation 1 (kafka.coordinator.group.GroupCoordinator)
[35mschema-registry    |[0m [2021-01-30 15:46:17,436] INFO [Schema registry clientId=sr-1, groupId=schema-registry] Successfully synced group in generation Generation{generationId=1, memberId='sr-1-0642ad4c-f054-4f3c-94e0-5227c94a846e', protocol='v0'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[35mschema-registry    |[0m [2021-01-30 15:46:17,442] INFO Finished rebalance with leader election result: Assignment{version=1, error=0, leader='sr-1-0642ad4c-f054-4f3c-94e0-5227c94a846e', leaderIdentity=SchemaRegistryIdentity{version=1,host=schema-registry,port=8081,scheme=http,leaderEligibility=true}, nodes=[SchemaRegistryIdentity{version=1,host=schema-registry,port=8081,scheme=http,leaderEligibility=true}]} (io.confluent.kafka.schemaregistry.leaderelector.kafka.KafkaGroupLeaderElector)
[35mschema-registry    |[0m [2021-01-30 15:46:17,482] INFO Wait to catch up until the offset at 1 (io.confluent.kafka.schemaregistry.storage.KafkaStore)
[35mschema-registry    |[0m [2021-01-30 15:46:17,864] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 11.0.9.1+1-LTS (org.eclipse.jetty.server.Server)
[35mschema-registry    |[0m [2021-01-30 15:46:17,968] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
[35mschema-registry    |[0m [2021-01-30 15:46:17,969] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
[35mschema-registry    |[0m [2021-01-30 15:46:17,973] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session)
[36mconnect            |[0m ===> Running preflight checks ... 
[36mconnect            |[0m ===> Check if Kafka is healthy ...
[35mschema-registry    |[0m Jan 30, 2021 3:46:18 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[35mschema-registry    |[0m WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.ConfigResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.ConfigResource will be ignored. 
[35mschema-registry    |[0m Jan 30, 2021 3:46:18 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[35mschema-registry    |[0m WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.CompatibilityResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.CompatibilityResource will be ignored. 
[35mschema-registry    |[0m Jan 30, 2021 3:46:18 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[35mschema-registry    |[0m WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SchemasResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SchemasResource will be ignored. 
[35mschema-registry    |[0m Jan 30, 2021 3:46:18 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[35mschema-registry    |[0m WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SubjectVersionsResource will be ignored. 
[35mschema-registry    |[0m Jan 30, 2021 3:46:18 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[35mschema-registry    |[0m WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.ModeResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.ModeResource will be ignored. 
[35mschema-registry    |[0m Jan 30, 2021 3:46:18 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[35mschema-registry    |[0m WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.ServerMetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.ServerMetadataResource will be ignored. 
[35mschema-registry    |[0m Jan 30, 2021 3:46:18 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[35mschema-registry    |[0m WARNING: A provider io.confluent.kafka.schemaregistry.rest.resources.SubjectsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.kafka.schemaregistry.rest.resources.SubjectsResource will be ignored. 
[35mschema-registry    |[0m [2021-01-30 15:46:19,210] INFO HV000001: Hibernate Validator 6.0.17.Final (org.hibernate.validator.internal.util.Version)
[34mzookeeper          |[0m [2021-01-30 15:46:19,350] INFO The list of known four letter word commands is : [{1936881266=srvr, 1937006964=stat, 2003003491=wchc, 1685417328=dump, 1668445044=crst, 1936880500=srst, 1701738089=envi, 1668247142=conf, -720899=telnet close, 2003003507=wchs, 2003003504=wchp, 1684632179=dirs, 1668247155=cons, 1835955314=mntr, 1769173615=isro, 1920298859=ruok, 1735683435=gtmk, 1937010027=stmk}] (org.apache.zookeeper.server.command.FourLetterCommands)
[34mzookeeper          |[0m [2021-01-30 15:46:19,350] INFO The list of enabled four letter word commands is : [[ruok, srvr]] (org.apache.zookeeper.server.command.FourLetterCommands)
[34mzookeeper          |[0m [2021-01-30 15:46:19,350] INFO Processing ruok command from /127.0.0.1:50564 (org.apache.zookeeper.server.NIOServerCnxn)
[36mconnect            |[0m [main] INFO org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 30000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 100
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 
[35mschema-registry    |[0m [2021-01-30 15:46:19,770] INFO Started o.e.j.s.ServletContextHandler@60921b21{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[35mschema-registry    |[0m [2021-01-30 15:46:19,810] INFO Started o.e.j.s.ServletContextHandler@1eea9d2d{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[35mschema-registry    |[0m [2021-01-30 15:46:19,837] INFO Started NetworkTrafficServerConnector@4f4c4b1a{HTTP/1.1,[http/1.1]}{0.0.0.0:8081} (org.eclipse.jetty.server.AbstractConnector)
[35mschema-registry    |[0m [2021-01-30 15:46:19,838] INFO Started @8201ms (org.eclipse.jetty.server.Server)
[35mschema-registry    |[0m [2021-01-30 15:46:19,838] INFO Server started, listening for requests... (io.confluent.kafka.schemaregistry.rest.SchemaRegistryMain)
[36mconnect            |[0m [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.0.1-ccs
[36mconnect            |[0m [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 9c1fbb3db1e0d69d
[36mconnect            |[0m [main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021580132
[35mschema-registry    |[0m [2021-01-30 15:46:20,885] INFO 127.0.0.1 - - [30/Jan/2021:15:46:20 +0000] "GET / HTTP/1.1" 200 2  293 (io.confluent.rest-utils.requests)
[36mconnect            |[0m ===> Launching ... 
[36mconnect            |[0m ===> Launching kafka-connect ... 
[36mconnect            |[0m [2021-01-30 15:46:22,175] INFO WorkerInfo values: 
[36mconnect            |[0m 	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/var/log/kafka, -Dlog4j.configuration=file:/etc/kafka/connect-log4j.properties
[36mconnect            |[0m 	jvm.spec = Azul Systems, Inc., OpenJDK 64-Bit Server VM, 11.0.9.1, 11.0.9.1+1-LTS
[36mconnect            |[0m 	jvm.classpath = /etc/kafka-connect/jars/*:/usr/share/java/kafka/netty-buffer-4.1.50.Final.jar:/usr/share/java/kafka/jersey-container-servlet-2.30.jar:/usr/share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/share/java/kafka/jackson-module-paranamer-2.10.5.jar:/usr/share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/usr/share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/share/java/kafka/kafka-tools-6.0.1-ccs.jar:/usr/share/java/kafka/kafka-streams-test-utils-6.0.1-ccs.jar:/usr/share/java/kafka/jersey-hk2-2.30.jar:/usr/share/java/kafka/kafka-streams-6.0.1-ccs.jar:/usr/share/java/kafka/kafka-streams-examples-6.0.1-ccs.jar:/usr/share/java/kafka/jackson-core-2.10.5.jar:/usr/share/java/kafka/jersey-container-servlet-core-2.30.jar:/usr/share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/usr/share/java/kafka/netty-codec-4.1.50.Final.jar:/usr/share/java/kafka/jetty-util-9.4.24.v20191120.jar:/usr/share/java/kafka/jackson-databind-2.10.5.jar:/usr/share/java/kafka/scala-logging_2.13-3.9.2.jar:/usr/share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/share/java/kafka/jetty-http-9.4.24.v20191120.jar:/usr/share/java/kafka/maven-artifact-3.6.3.jar:/usr/share/java/kafka/jetty-io-9.4.24.v20191120.jar:/usr/share/java/kafka/slf4j-api-1.7.30.jar:/usr/share/java/kafka/audience-annotations-0.5.0.jar:/usr/share/java/kafka/netty-transport-4.1.50.Final.jar:/usr/share/java/kafka/jetty-server-9.4.24.v20191120.jar:/usr/share/java/kafka/jersey-media-jaxb-2.30.jar:/usr/share/java/kafka/zookeeper-3.5.8.jar:/usr/share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/usr/share/java/kafka/jersey-common-2.30.jar:/usr/share/java/kafka/kafka-log4j-appender-6.0.1-ccs.jar:/usr/share/java/kafka/kafka-clients-6.0.1-ccs.jar:/usr/share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/usr/share/java/kafka/commons-lang3-3.8.1.jar:/usr/share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/usr/share/java/kafka/jetty-client-9.4.24.v20191120.jar:/usr/share/java/kafka/connect-transforms-6.0.1-ccs.jar:/usr/share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/usr/share/java/kafka/kafka-streams-scala_2.13-6.0.1-ccs.jar:/usr/share/java/kafka/jakarta.inject-2.6.1.jar:/usr/share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/usr/share/java/kafka/scala-library-2.13.2.jar:/usr/share/java/kafka/scala-reflect-2.13.2.jar:/usr/share/java/kafka/connect-basic-auth-extension-6.0.1-ccs.jar:/usr/share/java/kafka/paranamer-2.8.jar:/usr/share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/share/java/kafka/hk2-locator-2.6.1.jar:/usr/share/java/kafka/metrics-core-2.2.0.jar:/usr/share/java/kafka/kafka_2.13-6.0.1-ccs-test.jar:/usr/share/java/kafka/jackson-annotations-2.10.5.jar:/usr/share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/share/java/kafka/kafka_2.13-6.0.1-ccs-scaladoc.jar:/usr/share/java/kafka/activation-1.1.1.jar:/usr/share/java/kafka/kafka_2.13-6.0.1-ccs-sources.jar:/usr/share/java/kafka/scala-collection-compat_2.13-2.1.6.jar:/usr/share/java/kafka/connect-runtime-6.0.1-ccs.jar:/usr/share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/share/java/kafka/netty-handler-4.1.50.Final.jar:/usr/share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/usr/share/java/kafka/lz4-java-1.7.1.jar:/usr/share/java/kafka/rocksdbjni-5.18.4.jar:/usr/share/java/kafka/jaxb-api-2.3.0.jar:/usr/share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/usr/share/java/kafka/hk2-utils-2.6.1.jar:/usr/share/java/kafka/kafka.jar:/usr/share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/share/java/kafka/plexus-utils-3.2.1.jar:/usr/share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/share/java/kafka/kafka_2.13-6.0.1-ccs.jar:/usr/share/java/kafka/kafka_2.13-6.0.1-ccs-javadoc.jar:/usr/share/java/kafka/connect-mirror-client-6.0.1-ccs.jar:/usr/share/java/kafka/jetty-security-9.4.24.v20191120.jar:/usr/share/java/kafka/connect-file-6.0.1-ccs.jar:/usr/share/java/kafka/snappy-java-1.1.7.3.jar:/usr/share/java/kafka/argparse4j-0.7.0.jar:/usr/share/java/kafka/commons-cli-1.4.jar:/usr/share/java/kafka/zookeeper-jute-3.5.8.jar:/usr/share/java/kafka/javassist-3.26.0-GA.jar:/usr/share/java/kafka/jersey-client-2.30.jar:/usr/share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/usr/share/java/kafka/jersey-server-2.30.jar:/usr/share/java/kafka/netty-resolver-4.1.50.Final.jar:/usr/share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/share/java/kafka/javassist-3.25.0-GA.jar:/usr/share/java/kafka/jopt-simple-5.0.4.jar:/usr/share/java/kafka/reflections-0.9.12.jar:/usr/share/java/kafka/connect-json-6.0.1-ccs.jar:/usr/share/java/kafka/connect-mirror-6.0.1-ccs.jar:/usr/share/java/kafka/zstd-jni-1.4.4-7.jar:/usr/share/java/kafka/netty-common-4.1.50.Final.jar:/usr/share/java/kafka/kafka_2.13-6.0.1-ccs-test-sources.jar:/usr/share/java/kafka/netty-transport-native-unix-common-4.1.50.Final.jar:/usr/share/java/kafka/connect-api-6.0.1-ccs.jar:/usr/share/java/kafka/netty-transport-native-epoll-4.1.50.Final.jar:/usr/share/java/kafka/hk2-api-2.6.1.jar:/usr/share/java/confluent-common/common-config-6.0.1.jar:/usr/share/java/confluent-common/common-utils-6.0.1.jar:/usr/share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/share/java/confluent-common/common-metrics-6.0.1.jar:/usr/share/java/confluent-common/build-tools-6.0.1.jar:/usr/share/java/kafka-serde-tools/gson-2.8.6.jar:/usr/share/java/kafka-serde-tools/jakarta.ws.rs-api-2.1.6.jar:/usr/share/java/kafka-serde-tools/wire-runtime-3.2.2.jar:/usr/share/java/kafka-serde-tools/j2objc-annotations-1.3.jar:/usr/share/java/kafka-serde-tools/commons-validator-1.6.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jdk8-2.10.5.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-jvm-1.3.50.jar:/usr/share/java/kafka-serde-tools/handy-uri-templates-2.1.8.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-serializer-6.0.1.jar:/usr/share/java/kafka-serde-tools/kafka-streams-6.0.1-ccs.jar:/usr/share/java/kafka-serde-tools/jackson-core-2.10.5.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-guava-2.10.5.jar:/usr/share/java/kafka-serde-tools/protobuf-java-3.11.4.jar:/usr/share/java/kafka-serde-tools/kotlin-reflect-1.3.50.jar:/usr/share/java/kafka-serde-tools/kafka-connect-protobuf-converter-6.0.1.jar:/usr/share/java/kafka-serde-tools/jackson-databind-2.10.5.jar:/usr/share/java/kafka-serde-tools/kafka-connect-json-schema-converter-6.0.1.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-serializer-6.0.1.jar:/usr/share/java/kafka-serde-tools/json-20190722.jar:/usr/share/java/kafka-serde-tools/guava-28.1-jre.jar:/usr/share/java/kafka-serde-tools/jsr305-3.0.2.jar:/usr/share/java/kafka-serde-tools/animal-sniffer-annotations-1.18.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-joda-2.10.5.jar:/usr/share/java/kafka-serde-tools/kafka-streams-json-schema-serde-6.0.1.jar:/usr/share/java/kafka-serde-tools/kotlin-script-runtime-1.3.50.jar:/usr/share/java/kafka-serde-tools/classgraph-4.8.21.jar:/usr/share/java/kafka-serde-tools/commons-logging-1.2.jar:/usr/share/java/kafka-serde-tools/joda-time-2.9.9.jar:/usr/share/java/kafka-serde-tools/jersey-common-2.30.jar:/usr/share/java/kafka-serde-tools/error_prone_annotations-2.3.4.jar:/usr/share/java/kafka-serde-tools/kotlinx-coroutines-core-1.1.1.jar:/usr/share/java/kafka-serde-tools/kafka-json-serializer-6.0.1.jar:/usr/share/java/kafka-serde-tools/kafka-protobuf-provider-6.0.1.jar:/usr/share/java/kafka-serde-tools/avro-1.9.2.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-converter-6.0.1.jar:/usr/share/java/kafka-serde-tools/jakarta.inject-2.6.1.jar:/usr/share/java/kafka-serde-tools/checker-qual-2.8.1.jar:/usr/share/java/kafka-serde-tools/scala-library-2.13.2.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk7-1.3.71.jar:/usr/share/java/kafka-serde-tools/validation-api-2.0.1.Final.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-jdk8-1.3.71.jar:/usr/share/java/kafka-serde-tools/kafka-streams-avro-serde-6.0.1.jar:/usr/share/java/kafka-serde-tools/jackson-annotations-2.10.5.jar:/usr/share/java/kafka-serde-tools/re2j-1.3.jar:/usr/share/java/kafka-serde-tools/kafka-avro-serializer-6.0.1.jar:/usr/share/java/kafka-serde-tools/jackson-datatype-jsr310-2.10.5.jar:/usr/share/java/kafka-serde-tools/okio-2.5.0.jar:/usr/share/java/kafka-serde-tools/commons-digester-1.8.1.jar:/usr/share/java/kafka-serde-tools/osgi-resource-locator-1.0.3.jar:/usr/share/java/kafka-serde-tools/swagger-annotations-1.6.2.jar:/usr/share/java/kafka-serde-tools/org.everit.json.schema-1.12.1.jar:/usr/share/java/kafka-serde-tools/kafka-streams-protobuf-serde-6.0.1.jar:/usr/share/java/kafka-serde-tools/rocksdbjni-5.18.4.jar:/usr/share/java/kafka-serde-tools/commons-collections-3.2.2.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/jakarta.annotation-api-1.3.5.jar:/usr/share/java/kafka-serde-tools/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/share/java/kafka-serde-tools/annotations-13.0.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-1.4.0.jar:/usr/share/java/kafka-serde-tools/kotlin-stdlib-common-1.3.71.jar:/usr/share/java/kafka-serde-tools/protobuf-java-util-3.11.4.jar:/usr/share/java/kafka-serde-tools/kafka-connect-avro-data-6.0.1.jar:/usr/share/java/kafka-serde-tools/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/share/java/kafka-serde-tools/jackson-module-parameter-names-2.10.5.jar:/usr/share/java/kafka-serde-tools/kafka-json-schema-provider-6.0.1.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/share/java/kafka-serde-tools/kafka-schema-serializer-6.0.1.jar:/usr/share/java/kafka-serde-tools/failureaccess-1.0.1.jar:/usr/share/java/kafka-serde-tools/kafka-schema-registry-client-6.0.1.jar:/usr/share/java/kafka-serde-tools/kotlin-scripting-common-1.3.50.jar:/usr/share/java/kafka-serde-tools/kotlinx-coroutines-core-common-1.1.1.jar:/usr/share/java/kafka-serde-tools/wire-schema-3.2.2.jar:/usr/share/java/kafka-serde-tools/commons-compress-1.19.jar:/usr/share/java/monitoring-interceptors/monitoring-interceptors-6.0.1.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.30.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jackson-module-paranamer-2.10.5.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.10.5.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/kafka-tools-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.30.jar:/usr/bin/../share/java/kafka/kafka-streams-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-core-2.10.5.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.30.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jackson-databind-2.10.5.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.2.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/maven-artifact-3.6.3.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jersey-media-jaxb-2.30.jar:/usr/bin/../share/java/kafka/zookeeper-3.5.8.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/jersey-common-2.30.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka-clients-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.10.5.jar:/usr/bin/../share/java/kafka/commons-lang3-3.8.1.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.10.5.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/connect-transforms-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.10.5.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-0.9.1.jar:/usr/bin/../share/java/kafka/scala-library-2.13.2.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.2.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp2.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-test.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-scaladoc.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-sources.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.1.6.jar:/usr/bin/../share/java/kafka/connect-runtime-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/lz4-java-1.7.1.jar:/usr/bin/../share/java/kafka/rocksdbjni-5.18.4.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.10.5.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-javadoc.jar:/usr/bin/../share/java/kafka/connect-mirror-client-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.24.v20191120.jar:/usr/bin/../share/java/kafka/connect-file-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.7.3.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.5.8.jar:/usr/bin/../share/java/kafka/javassist-3.26.0-GA.jar:/usr/bin/../share/java/kafka/jersey-client-2.30.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.10.5.jar:/usr/bin/../share/java/kafka/jersey-server-2.30.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.50.Final.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/javassist-3.25.0-GA.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/connect-json-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/connect-mirror-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/zstd-jni-1.4.4-7.jar:/usr/bin/../share/java/kafka/netty-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/kafka_2.13-6.0.1-ccs-test-sources.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.50.Final.jar:/usr/bin/../share/java/kafka/connect-api-6.0.1-ccs.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.50.Final.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-telemetry/confluent-metrics-6.0.1-ce.jar
[36mconnect            |[0m 	os.spec = Linux, amd64, 4.19.121-linuxkit
[36mconnect            |[0m 	os.vcpus = 6
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.WorkerInfo)
[36mconnect            |[0m [2021-01-30 15:46:22,182] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.ConnectDistributed)
[36mconnect            |[0m [2021-01-30 15:46:22,212] INFO Loading plugin from: /usr/share/confluent-hub-components/confluentinc-connect-transforms (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,513] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/confluent-hub-components/confluentinc-connect-transforms/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,513] INFO Added plugin 'io.confluent.connect.transforms.Drop$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,514] INFO Added plugin 'io.confluent.connect.transforms.Filter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,514] INFO Added plugin 'io.confluent.connect.transforms.TombstoneHandler' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,515] INFO Added plugin 'io.confluent.connect.transforms.MessageTimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,515] INFO Added plugin 'io.confluent.connect.transforms.ExtractTopic$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,515] INFO Added plugin 'io.confluent.connect.transforms.Filter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,516] INFO Added plugin 'io.confluent.connect.transforms.Drop$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,516] INFO Added plugin 'io.confluent.connect.transforms.ExtractTopic$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,517] INFO Added plugin 'io.confluent.connect.transforms.ExtractTopic$Header' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,518] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,518] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,518] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,522] INFO Loading plugin from: /usr/share/confluent-hub-components/kaliy-kafka-connect-rss (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,649] INFO Registered loader: PluginClassLoader{pluginLocation=file:/usr/share/confluent-hub-components/kaliy-kafka-connect-rss/} (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:22,649] INFO Added plugin 'org.kaliy.kafka.connect.rss.RssSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,375] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@9e89d68 (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,375] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,376] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,376] INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,377] INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,377] INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,377] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,377] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,377] INFO Added plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,377] INFO Added plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,378] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,378] INFO Added plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,378] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,378] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,378] INFO Added plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,379] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,379] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,379] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,379] INFO Added plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,379] INFO Added plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,379] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,379] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,380] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,380] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,380] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,380] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,380] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,381] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,381] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,381] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,381] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,381] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,381] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,382] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,382] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,382] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,382] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,382] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,383] INFO Added plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,383] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,383] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,383] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,383] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,383] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,383] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,384] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,384] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,384] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,384] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,384] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,385] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,385] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,387] INFO Added aliases 'FileStreamSinkConnector' and 'FileStreamSink' to plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,387] INFO Added aliases 'FileStreamSourceConnector' and 'FileStreamSource' to plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,387] INFO Added aliases 'MirrorCheckpointConnector' and 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,387] INFO Added aliases 'MirrorHeartbeatConnector' and 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,387] INFO Added aliases 'MirrorSourceConnector' and 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,388] INFO Added aliases 'MockConnector' and 'Mock' to plugin 'org.apache.kafka.connect.tools.MockConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,388] INFO Added aliases 'MockSinkConnector' and 'MockSink' to plugin 'org.apache.kafka.connect.tools.MockSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,388] INFO Added aliases 'MockSourceConnector' and 'MockSource' to plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,388] INFO Added aliases 'SchemaSourceConnector' and 'SchemaSource' to plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,389] INFO Added aliases 'VerifiableSinkConnector' and 'VerifiableSink' to plugin 'org.apache.kafka.connect.tools.VerifiableSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,389] INFO Added aliases 'VerifiableSourceConnector' and 'VerifiableSource' to plugin 'org.apache.kafka.connect.tools.VerifiableSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,389] INFO Added aliases 'RssSourceConnector' and 'RssSource' to plugin 'org.kaliy.kafka.connect.rss.RssSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,389] INFO Added aliases 'AvroConverter' and 'Avro' to plugin 'io.confluent.connect.avro.AvroConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,390] INFO Added aliases 'JsonSchemaConverter' and 'JsonSchema' to plugin 'io.confluent.connect.json.JsonSchemaConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,390] INFO Added aliases 'ProtobufConverter' and 'Protobuf' to plugin 'io.confluent.connect.protobuf.ProtobufConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,390] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,390] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,391] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,391] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,391] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,391] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,392] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,392] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,392] INFO Added aliases 'ByteArrayConverter' and 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,392] INFO Added aliases 'DoubleConverter' and 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,393] INFO Added aliases 'FloatConverter' and 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,393] INFO Added aliases 'IntegerConverter' and 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,393] INFO Added aliases 'LongConverter' and 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,393] INFO Added aliases 'ShortConverter' and 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,394] INFO Added aliases 'JsonConverter' and 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,394] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,394] INFO Added aliases 'StringConverter' and 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,397] INFO Added alias 'Header' to plugin 'io.confluent.connect.transforms.ExtractTopic$Header' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,397] INFO Added alias 'MessageTimestampRouter' to plugin 'io.confluent.connect.transforms.MessageTimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,398] INFO Added alias 'TombstoneHandler' to plugin 'io.confluent.connect.transforms.TombstoneHandler' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,398] INFO Added aliases 'PredicatedTransformation' and 'Predicated' to plugin 'org.apache.kafka.connect.runtime.PredicatedTransformation' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,399] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,400] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,400] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,400] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,401] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,401] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,401] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,401] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,401] INFO Added aliases 'AllConnectorClientConfigOverridePolicy' and 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,401] INFO Added aliases 'NoneConnectorClientConfigOverridePolicy' and 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,401] INFO Added aliases 'PrincipalConnectorClientConfigOverridePolicy' and 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader)
[36mconnect            |[0m [2021-01-30 15:46:25,509] INFO DistributedConfig values: 
[36mconnect            |[0m 	access.control.allow.methods = 
[36mconnect            |[0m 	access.control.allow.origin = 
[36mconnect            |[0m 	admin.listeners = null
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	config.providers = []
[36mconnect            |[0m 	config.storage.replication.factor = 1
[36mconnect            |[0m 	config.storage.topic = connect-configs
[36mconnect            |[0m 	connect.protocol = sessioned
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	connector.client.config.override.policy = None
[36mconnect            |[0m 	group.id = sandbox
[36mconnect            |[0m 	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
[36mconnect            |[0m 	heartbeat.interval.ms = 3000
[36mconnect            |[0m 	inter.worker.key.generation.algorithm = HmacSHA256
[36mconnect            |[0m 	inter.worker.key.size = null
[36mconnect            |[0m 	inter.worker.key.ttl.ms = 3600000
[36mconnect            |[0m 	inter.worker.signature.algorithm = HmacSHA256
[36mconnect            |[0m 	inter.worker.verification.algorithms = [HmacSHA256]
[36mconnect            |[0m 	internal.key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	internal.value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	key.converter = class io.confluent.connect.avro.AvroConverter
[36mconnect            |[0m 	listeners = null
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	offset.flush.interval.ms = 60000
[36mconnect            |[0m 	offset.flush.timeout.ms = 5000
[36mconnect            |[0m 	offset.storage.partitions = 25
[36mconnect            |[0m 	offset.storage.replication.factor = 1
[36mconnect            |[0m 	offset.storage.topic = connect-offsets
[36mconnect            |[0m 	plugin.path = [/usr/share/confluent-hub-components]
[36mconnect            |[0m 	rebalance.timeout.ms = 60000
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	response.http.headers.config = 
[36mconnect            |[0m 	rest.advertised.host.name = connect
[36mconnect            |[0m 	rest.advertised.listener = null
[36mconnect            |[0m 	rest.advertised.port = null
[36mconnect            |[0m 	rest.extension.classes = []
[36mconnect            |[0m 	rest.host.name = null
[36mconnect            |[0m 	rest.port = 8083
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	scheduled.rebalance.max.delay.ms = 300000
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	session.timeout.ms = 10000
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.client.auth = none
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	status.storage.partitions = 5
[36mconnect            |[0m 	status.storage.replication.factor = 1
[36mconnect            |[0m 	status.storage.topic = connect-status
[36mconnect            |[0m 	task.shutdown.graceful.timeout.ms = 5000
[36mconnect            |[0m 	topic.creation.enable = true
[36mconnect            |[0m 	topic.tracking.allow.reset = true
[36mconnect            |[0m 	topic.tracking.enable = true
[36mconnect            |[0m 	value.converter = class io.confluent.connect.avro.AvroConverter
[36mconnect            |[0m 	worker.sync.timeout.ms = 3000
[36mconnect            |[0m 	worker.unsync.backoff.ms = 300000
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.distributed.DistributedConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,509] INFO Worker configuration property 'internal.key.converter' is deprecated and may be removed in an upcoming release. The specified value 'org.apache.kafka.connect.json.JsonConverter' matches the default, so this property can be safely removed from the worker configuration. (org.apache.kafka.connect.runtime.WorkerConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,509] INFO Worker configuration property 'internal.key.converter.schemas.enable' (along with all configuration for 'internal.key.converter') is deprecated and may be removed in an upcoming release. The specified value 'false' matches the default, so this property can be safely removed from the worker configuration. (org.apache.kafka.connect.runtime.WorkerConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,509] INFO Worker configuration property 'internal.value.converter' is deprecated and may be removed in an upcoming release. The specified value 'org.apache.kafka.connect.json.JsonConverter' matches the default, so this property can be safely removed from the worker configuration. (org.apache.kafka.connect.runtime.WorkerConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,510] INFO Worker configuration property 'internal.value.converter.schemas.enable' (along with all configuration for 'internal.value.converter') is deprecated and may be removed in an upcoming release. The specified value 'false' matches the default, so this property can be safely removed from the worker configuration. (org.apache.kafka.connect.runtime.WorkerConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,515] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:25,518] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,655] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,655] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,655] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,656] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,657] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,658] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,658] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,658] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:25,659] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:25,659] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:25,659] INFO Kafka startTimeMs: 1612021585658 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,145] INFO Kafka cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,179] INFO Logging initialized @4995ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
[36mconnect            |[0m [2021-01-30 15:46:26,284] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,285] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,296] INFO jetty-9.4.24.v20191120; built: 2019-11-20T21:37:49.771Z; git: 363d5f2df3a8a28de40604320230664b9c793c16; jvm 11.0.9.1+1-LTS (org.eclipse.jetty.server.Server)
[36mconnect            |[0m [2021-01-30 15:46:26,345] INFO Started http_8083@6a0094c9{HTTP/1.1,[http/1.1]}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector)
[36mconnect            |[0m [2021-01-30 15:46:26,346] INFO Started @5162ms (org.eclipse.jetty.server.Server)
[36mconnect            |[0m [2021-01-30 15:46:26,442] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,442] INFO REST server listening at http://172.28.0.6:8083/, advertising URL http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,443] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,443] INFO REST admin endpoints at http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,443] INFO Advertised URI: http://connect:8083/ (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,446] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,447] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,456] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,456] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,456] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,456] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,457] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,458] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,459] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,459] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,459] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,459] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,459] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,459] INFO Kafka startTimeMs: 1612021586459 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,484] INFO Kafka cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,502] INFO Setting up None Policy for ConnectorClientConfigOverride. This will disallow any client configuration to be overridden (org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy)
[36mconnect            |[0m [2021-01-30 15:46:26,510] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,511] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,523] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,524] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,524] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,524] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,524] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,525] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,525] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,525] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,525] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,525] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,525] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,525] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,526] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,526] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,527] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,527] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,527] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,527] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,527] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,527] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,527] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,528] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,528] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,528] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,528] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,528] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,528] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,529] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,529] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,529] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,529] INFO Kafka startTimeMs: 1612021586529 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,550] INFO Kafka cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,560] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,560] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,560] INFO Kafka startTimeMs: 1612021586560 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [Docker-Connect] Checking connection to Kafka Connect, port 8083 
[36mconnect            |[0m Ncat: Version 7.70 ( https://nmap.org/ncat )
[36mconnect            |[0m Ncat: Connected to 172.28.0.6:8083.
[36mconnect            |[0m Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds.
[36mconnect            |[0m 
[36mconnect            |[0m --
[36mconnect            |[0m +> [Docker-Connect] Creating Kafka Connect RSS Source. Lauching specific configuration
[36mconnect            |[0m <html>
[36mconnect            |[0m <head>
[36mconnect            |[0m <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
[36mconnect            |[0m <title>Error 404 Not Found</title>
[36mconnect            |[0m </head>
[36mconnect            |[0m <body><h2>HTTP ERROR 404 Not Found</h2>
[36mconnect            |[0m <table>
[36mconnect            |[0m <tr><th>URI:</th><td>/connectors</td></tr>
[36mconnect            |[0m <tr><th>STATUS:</th><td>404</td></tr>
[36mconnect            |[0m <tr><th>MESSAGE:</th><td>Not Found</td></tr>
[36mconnect            |[0m <tr><th>SERVLET:</th><td>-</td></tr>
[36mconnect            |[0m </table>
[36mconnect            |[0m <hr><a href="http://eclipse.org/jetty">Powered by Jetty:// 9.4.24.v20191120</a><hr/>
[36mconnect            |[0m 
[36mconnect            |[0m </body>
[36mconnect            |[0m </html>
[36mconnect            |[0m 
[36mconnect            |[0m --
[36mconnect            |[0m +> [Docker-Connect] Deployed specific configuration present in script: rss-post.sh 
[36mconnect            |[0m [2021-01-30 15:46:26,782] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = key
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,785] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = value
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,785] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,786] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,793] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,793] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,794] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,794] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,794] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,795] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,795] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,795] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,795] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,796] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,796] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,796] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,799] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,800] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,800] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,800] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,800] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,800] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,800] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,801] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,801] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,801] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,801] INFO Kafka startTimeMs: 1612021586801 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,818] INFO Kafka cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,834] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,835] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,840] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,841] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,842] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,842] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,842] INFO Kafka startTimeMs: 1612021586842 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,856] INFO Kafka cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,865] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,866] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,871] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,872] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,873] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,873] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,873] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,873] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,873] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,873] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,873] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,874] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,874] INFO Kafka startTimeMs: 1612021586873 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,888] INFO Kafka cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,910] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,911] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,915] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,915] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,915] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,915] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,916] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,916] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,916] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,916] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,917] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,917] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,917] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,917] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,917] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,918] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,919] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,919] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,919] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,919] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,919] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,919] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,920] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,920] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,920] INFO Kafka startTimeMs: 1612021586919 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,934] INFO Kafka cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.connect.util.ConnectUtils)
[36mconnect            |[0m [2021-01-30 15:46:26,974] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,974] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,974] INFO Kafka startTimeMs: 1612021586973 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,979] INFO Kafka Connect distributed worker initialization took 4797ms (org.apache.kafka.connect.cli.ConnectDistributed)
[36mconnect            |[0m [2021-01-30 15:46:26,979] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect)
[36mconnect            |[0m [2021-01-30 15:46:26,982] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:26,983] INFO [Worker clientId=connect-1, groupId=sandbox] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:26,985] INFO Worker starting (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:26,985] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
[36mconnect            |[0m [2021-01-30 15:46:26,985] INFO Starting KafkaBasedLog with topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:26,985] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,990] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,991] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:26,992] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,993] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:26,993] INFO Kafka startTimeMs: 1612021586992 (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka1             |[0m [2021-01-30 15:46:27,013] INFO Creating topic connect-offsets with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1), 5 -> ArrayBuffer(1), 6 -> ArrayBuffer(1), 7 -> ArrayBuffer(1), 8 -> ArrayBuffer(1), 9 -> ArrayBuffer(1), 10 -> ArrayBuffer(1), 11 -> ArrayBuffer(1), 12 -> ArrayBuffer(1), 13 -> ArrayBuffer(1), 14 -> ArrayBuffer(1), 15 -> ArrayBuffer(1), 16 -> ArrayBuffer(1), 17 -> ArrayBuffer(1), 18 -> ArrayBuffer(1), 19 -> ArrayBuffer(1), 20 -> ArrayBuffer(1), 21 -> ArrayBuffer(1), 22 -> ArrayBuffer(1), 23 -> ArrayBuffer(1), 24 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:27,033] INFO [Controller id=1] New topics: [HashSet(connect-offsets)], deleted topics: [HashSet()], new partition replica assignment [HashMap(connect-offsets-20 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-6 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-17 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-15 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-7 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-24 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-16 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-21 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-11 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-10 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-18 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-23 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-12 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-5 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-13 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-14 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-8 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-9 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-19 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-offsets-22 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:27,033] INFO [Controller id=1] New partition creation callback for connect-offsets-20,connect-offsets-6,connect-offsets-17,connect-offsets-15,connect-offsets-7,connect-offsets-24,connect-offsets-16,connect-offsets-3,connect-offsets-21,connect-offsets-11,connect-offsets-10,connect-offsets-2,connect-offsets-18,connect-offsets-23,connect-offsets-4,connect-offsets-12,connect-offsets-5,connect-offsets-13,connect-offsets-14,connect-offsets-0,connect-offsets-8,connect-offsets-9,connect-offsets-1,connect-offsets-19,connect-offsets-22 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:27,033] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-20 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-6 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-17 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-15 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-7 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-24 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-16 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-21 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-11 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-10 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-18 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-23 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-12 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,034] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-5 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-13 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-14 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-8 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-9 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-19 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-22 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,035] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-22 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-13 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-24 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-14 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-10 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-17 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-23 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-18 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,036] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-6 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-4 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-16 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-7 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-3 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-1 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-11 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-19 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-2 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-21 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-20 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-12 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-9 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-5 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-8 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-15 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,037] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:27,102] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-20 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-6 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-17 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-15 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-7 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-24 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-16 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-21 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-11 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-10 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-18 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-23 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,105] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-12 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-5 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-13 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-14 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-8 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-9 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,106] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,107] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-19 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,107] INFO [Controller id=1 epoch=1] Changed partition connect-offsets-22 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,107] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,107] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-24 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-18 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-22 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,108] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-20 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-9 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-7 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-13 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-11 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-5 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-23 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-17 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-15 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-21 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-offsets-19 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,109] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 25 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,110] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 25 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,111] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-22 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,111] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-13 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-24 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-14 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-10 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-17 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-23 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-18 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-6 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-4 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-16 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-7 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-3 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,112] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-1 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-11 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-19 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-2 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-21 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-20 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-12 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-9 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-5 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-8 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-offsets-15 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,113] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,114] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 for 25 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,115] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,116] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,117] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,117] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,117] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,117] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,118] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,118] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,118] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,118] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,118] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,119] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,119] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,119] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,119] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,120] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,120] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,120] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,120] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,120] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,120] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,121] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,121] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,121] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,121] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 7 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,135] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-24 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,135] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-9 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,136] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-20 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,136] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-5 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,136] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-17 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,137] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,137] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-13 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,137] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,137] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,137] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-23 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,138] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,138] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,138] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,138] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-11 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,139] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-22 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,139] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-7 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,139] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,139] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-19 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,139] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-15 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,140] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,140] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-21 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,140] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,140] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-18 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,140] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,140] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 1 epoch 1 starting the become-leader transition for partition connect-offsets-14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,141] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(connect-offsets-20, connect-offsets-6, connect-offsets-17, connect-offsets-15, connect-offsets-7, connect-offsets-24, connect-offsets-16, connect-offsets-3, connect-offsets-21, connect-offsets-11, connect-offsets-10, connect-offsets-2, connect-offsets-18, connect-offsets-23, connect-offsets-4, connect-offsets-12, connect-offsets-5, connect-offsets-13, connect-offsets-14, connect-offsets-0, connect-offsets-8, connect-offsets-9, connect-offsets-1, connect-offsets-19, connect-offsets-22) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,141] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 1 epoch 1 as part of the become-leader transition for 25 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,145] INFO [Log partition=connect-offsets-24, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,147] INFO Created log for partition connect-offsets-24 in /var/lib/kafka/data/connect-offsets-24 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,149] INFO [Partition connect-offsets-24 broker=1] No checkpointed highwatermark is found for partition connect-offsets-24 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,149] INFO [Partition connect-offsets-24 broker=1] Log loaded for partition connect-offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,150] INFO [Broker id=1] Leader connect-offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,157] INFO [Log partition=connect-offsets-9, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,158] INFO Created log for partition connect-offsets-9 in /var/lib/kafka/data/connect-offsets-9 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,158] INFO [Partition connect-offsets-9 broker=1] No checkpointed highwatermark is found for partition connect-offsets-9 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,158] INFO [Partition connect-offsets-9 broker=1] Log loaded for partition connect-offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,159] INFO [Broker id=1] Leader connect-offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,165] INFO [Log partition=connect-offsets-20, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,167] INFO Created log for partition connect-offsets-20 in /var/lib/kafka/data/connect-offsets-20 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,168] INFO [Partition connect-offsets-20 broker=1] No checkpointed highwatermark is found for partition connect-offsets-20 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,168] INFO [Partition connect-offsets-20 broker=1] Log loaded for partition connect-offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,168] INFO [Broker id=1] Leader connect-offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,174] INFO [Log partition=connect-offsets-5, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,175] INFO Created log for partition connect-offsets-5 in /var/lib/kafka/data/connect-offsets-5 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,176] INFO [Partition connect-offsets-5 broker=1] No checkpointed highwatermark is found for partition connect-offsets-5 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,176] INFO [Partition connect-offsets-5 broker=1] Log loaded for partition connect-offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,176] INFO [Broker id=1] Leader connect-offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,184] INFO [Log partition=connect-offsets-17, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,186] INFO Created log for partition connect-offsets-17 in /var/lib/kafka/data/connect-offsets-17 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,187] INFO [Partition connect-offsets-17 broker=1] No checkpointed highwatermark is found for partition connect-offsets-17 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,187] INFO [Partition connect-offsets-17 broker=1] Log loaded for partition connect-offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,187] INFO [Broker id=1] Leader connect-offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,194] INFO [Log partition=connect-offsets-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,195] INFO Created log for partition connect-offsets-2 in /var/lib/kafka/data/connect-offsets-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,196] INFO [Partition connect-offsets-2 broker=1] No checkpointed highwatermark is found for partition connect-offsets-2 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,196] INFO [Partition connect-offsets-2 broker=1] Log loaded for partition connect-offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,197] INFO [Broker id=1] Leader connect-offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,204] INFO [Log partition=connect-offsets-13, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,206] INFO Created log for partition connect-offsets-13 in /var/lib/kafka/data/connect-offsets-13 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,207] INFO [Partition connect-offsets-13 broker=1] No checkpointed highwatermark is found for partition connect-offsets-13 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,207] INFO [Partition connect-offsets-13 broker=1] Log loaded for partition connect-offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,208] INFO [Broker id=1] Leader connect-offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,216] INFO [Log partition=connect-offsets-8, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[36mconnect            |[0m [2021-01-30 15:46:27,217] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
[36mconnect            |[0m [2021-01-30 15:46:27,218] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
[32mkafka1             |[0m [2021-01-30 15:46:27,218] INFO Created log for partition connect-offsets-8 in /var/lib/kafka/data/connect-offsets-8 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,218] INFO [Partition connect-offsets-8 broker=1] No checkpointed highwatermark is found for partition connect-offsets-8 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,218] INFO [Partition connect-offsets-8 broker=1] Log loaded for partition connect-offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,219] INFO [Broker id=1] Leader connect-offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:27,221] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
[32mkafka1             |[0m [2021-01-30 15:46:27,226] INFO [Log partition=connect-offsets-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,227] INFO Created log for partition connect-offsets-4 in /var/lib/kafka/data/connect-offsets-4 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,227] INFO [Partition connect-offsets-4 broker=1] No checkpointed highwatermark is found for partition connect-offsets-4 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,227] INFO [Partition connect-offsets-4 broker=1] Log loaded for partition connect-offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,227] INFO [Broker id=1] Leader connect-offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,235] INFO [Log partition=connect-offsets-23, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,237] INFO Created log for partition connect-offsets-23 in /var/lib/kafka/data/connect-offsets-23 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,238] INFO [Partition connect-offsets-23 broker=1] No checkpointed highwatermark is found for partition connect-offsets-23 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,238] INFO [Partition connect-offsets-23 broker=1] Log loaded for partition connect-offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,238] INFO [Broker id=1] Leader connect-offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,244] INFO [Log partition=connect-offsets-16, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,246] INFO Created log for partition connect-offsets-16 in /var/lib/kafka/data/connect-offsets-16 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,246] INFO [Partition connect-offsets-16 broker=1] No checkpointed highwatermark is found for partition connect-offsets-16 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,246] INFO [Partition connect-offsets-16 broker=1] Log loaded for partition connect-offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,246] INFO [Broker id=1] Leader connect-offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,253] INFO [Log partition=connect-offsets-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,255] INFO Created log for partition connect-offsets-1 in /var/lib/kafka/data/connect-offsets-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,255] INFO [Partition connect-offsets-1 broker=1] No checkpointed highwatermark is found for partition connect-offsets-1 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,255] INFO [Partition connect-offsets-1 broker=1] Log loaded for partition connect-offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,255] INFO [Broker id=1] Leader connect-offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,261] INFO [Log partition=connect-offsets-12, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,263] INFO Created log for partition connect-offsets-12 in /var/lib/kafka/data/connect-offsets-12 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,263] INFO [Partition connect-offsets-12 broker=1] No checkpointed highwatermark is found for partition connect-offsets-12 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,263] INFO [Partition connect-offsets-12 broker=1] Log loaded for partition connect-offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,263] INFO [Broker id=1] Leader connect-offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,270] INFO [Log partition=connect-offsets-11, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,271] INFO Created log for partition connect-offsets-11 in /var/lib/kafka/data/connect-offsets-11 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,271] INFO [Partition connect-offsets-11 broker=1] No checkpointed highwatermark is found for partition connect-offsets-11 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,271] INFO [Partition connect-offsets-11 broker=1] Log loaded for partition connect-offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,271] INFO [Broker id=1] Leader connect-offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,277] INFO [Log partition=connect-offsets-22, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,278] INFO Created log for partition connect-offsets-22 in /var/lib/kafka/data/connect-offsets-22 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,278] INFO [Partition connect-offsets-22 broker=1] No checkpointed highwatermark is found for partition connect-offsets-22 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,279] INFO [Partition connect-offsets-22 broker=1] Log loaded for partition connect-offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,279] INFO [Broker id=1] Leader connect-offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,286] INFO [Log partition=connect-offsets-7, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,287] INFO Created log for partition connect-offsets-7 in /var/lib/kafka/data/connect-offsets-7 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,287] INFO [Partition connect-offsets-7 broker=1] No checkpointed highwatermark is found for partition connect-offsets-7 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,287] INFO [Partition connect-offsets-7 broker=1] Log loaded for partition connect-offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,288] INFO [Broker id=1] Leader connect-offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,294] INFO [Log partition=connect-offsets-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,295] INFO Created log for partition connect-offsets-0 in /var/lib/kafka/data/connect-offsets-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,295] INFO [Partition connect-offsets-0 broker=1] No checkpointed highwatermark is found for partition connect-offsets-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,295] INFO [Partition connect-offsets-0 broker=1] Log loaded for partition connect-offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,296] INFO [Broker id=1] Leader connect-offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,303] INFO [Log partition=connect-offsets-19, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,305] INFO Created log for partition connect-offsets-19 in /var/lib/kafka/data/connect-offsets-19 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,305] INFO [Partition connect-offsets-19 broker=1] No checkpointed highwatermark is found for partition connect-offsets-19 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,305] INFO [Partition connect-offsets-19 broker=1] Log loaded for partition connect-offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,305] INFO [Broker id=1] Leader connect-offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,312] INFO [Log partition=connect-offsets-15, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,313] INFO Created log for partition connect-offsets-15 in /var/lib/kafka/data/connect-offsets-15 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,313] INFO [Partition connect-offsets-15 broker=1] No checkpointed highwatermark is found for partition connect-offsets-15 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,313] INFO [Partition connect-offsets-15 broker=1] Log loaded for partition connect-offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,314] INFO [Broker id=1] Leader connect-offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,325] INFO [Log partition=connect-offsets-10, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,326] INFO Created log for partition connect-offsets-10 in /var/lib/kafka/data/connect-offsets-10 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,326] INFO [Partition connect-offsets-10 broker=1] No checkpointed highwatermark is found for partition connect-offsets-10 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,326] INFO [Partition connect-offsets-10 broker=1] Log loaded for partition connect-offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,327] INFO [Broker id=1] Leader connect-offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,336] INFO [Log partition=connect-offsets-21, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,337] INFO Created log for partition connect-offsets-21 in /var/lib/kafka/data/connect-offsets-21 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,337] INFO [Partition connect-offsets-21 broker=1] No checkpointed highwatermark is found for partition connect-offsets-21 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,337] INFO [Partition connect-offsets-21 broker=1] Log loaded for partition connect-offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,337] INFO [Broker id=1] Leader connect-offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,346] INFO [Log partition=connect-offsets-6, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,348] INFO Created log for partition connect-offsets-6 in /var/lib/kafka/data/connect-offsets-6 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,348] INFO [Partition connect-offsets-6 broker=1] No checkpointed highwatermark is found for partition connect-offsets-6 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,348] INFO [Partition connect-offsets-6 broker=1] Log loaded for partition connect-offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,348] INFO [Broker id=1] Leader connect-offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,356] INFO [Log partition=connect-offsets-18, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,358] INFO Created log for partition connect-offsets-18 in /var/lib/kafka/data/connect-offsets-18 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,358] INFO [Partition connect-offsets-18 broker=1] No checkpointed highwatermark is found for partition connect-offsets-18 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,358] INFO [Partition connect-offsets-18 broker=1] Log loaded for partition connect-offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,358] INFO [Broker id=1] Leader connect-offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,365] INFO [Log partition=connect-offsets-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,367] INFO Created log for partition connect-offsets-3 in /var/lib/kafka/data/connect-offsets-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,367] INFO [Partition connect-offsets-3 broker=1] No checkpointed highwatermark is found for partition connect-offsets-3 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,368] INFO [Partition connect-offsets-3 broker=1] Log loaded for partition connect-offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,369] INFO [Broker id=1] Leader connect-offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,391] INFO [Log partition=connect-offsets-14, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,393] INFO Created log for partition connect-offsets-14 in /var/lib/kafka/data/connect-offsets-14 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,393] INFO [Partition connect-offsets-14 broker=1] No checkpointed highwatermark is found for partition connect-offsets-14 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,394] INFO [Partition connect-offsets-14 broker=1] Log loaded for partition connect-offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,394] INFO [Broker id=1] Leader connect-offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,397] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-24 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,399] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-9 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,399] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-20 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,399] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-5 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,399] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-17 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,399] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,399] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-13 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,400] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,401] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,401] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-23 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,401] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,402] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,402] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,403] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-11 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,403] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-22 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,404] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-7 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,405] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,405] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-19 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,406] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-15 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,406] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,407] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-21 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,407] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-6 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,408] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-18 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,408] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,408] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 7 from controller 1 epoch 1 for the become-leader transition for partition connect-offsets-14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,411] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=connect-offsets,partition_index=10,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=8,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=14,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=12,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=6,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=4,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=24,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=18,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=16,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=22,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=20,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=9,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=7,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=13,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=11,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=5,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=23,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=17,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=15,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=21,error_code=0,_tagged_fields={}},{topic_name=connect-offsets,partition_index=19,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 7 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,416] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=8, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-8 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,416] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=12, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-12 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,416] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,416] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,416] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=24, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-24 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,416] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=16, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-16 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,416] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=20, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-20 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=9, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-9 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=13, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-13 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=5, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-5 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=17, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-17 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=21, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-21 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=10, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-10 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=14, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-14 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,417] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=6, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-6 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=18, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-18 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=22, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-22 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=7, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-7 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=11, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-11 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=23, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-23 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=15, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-15 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-offsets', partitionIndex=19, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-offsets-19 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,418] INFO [Broker id=1] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,422] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 8 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:27,433] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka1:9092 (org.apache.kafka.connect.util.TopicAdmin)
[36mconnect            |[0m [2021-01-30 15:46:27,448] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = producer-1
[36mconnect            |[0m 	compression.type = none
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 2147483647
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m 	linger.ms = 0
[36mconnect            |[0m 	max.block.ms = 60000
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 1048576
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,485] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,485] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,485] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,485] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,485] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,485] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,486] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,486] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,486] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,486] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,487] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,488] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,488] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,488] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,488] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,488] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,488] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,488] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,490] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,490] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,490] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,491] INFO Kafka startTimeMs: 1612021587490 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,505] INFO ConsumerConfig values: 
[36mconnect            |[0m 	allow.auto.create.topics = true
[36mconnect            |[0m 	auto.commit.interval.ms = 5000
[36mconnect            |[0m 	auto.offset.reset = earliest
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	check.crcs = true
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = consumer-sandbox-1
[36mconnect            |[0m 	client.rack = 
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	enable.auto.commit = false
[36mconnect            |[0m 	exclude.internal.topics = true
[36mconnect            |[0m 	fetch.max.bytes = 52428800
[36mconnect            |[0m 	fetch.max.wait.ms = 500
[36mconnect            |[0m 	fetch.min.bytes = 1
[36mconnect            |[0m 	group.id = sandbox
[36mconnect            |[0m 	group.instance.id = null
[36mconnect            |[0m 	heartbeat.interval.ms = 3000
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.leave.group.on.close = true
[36mconnect            |[0m 	internal.throw.on.fetch.stable.offset.unsupported = false
[36mconnect            |[0m 	isolation.level = read_uncommitted
[36mconnect            |[0m 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[36mconnect            |[0m 	max.partition.fetch.bytes = 1048576
[36mconnect            |[0m 	max.poll.interval.ms = 300000
[36mconnect            |[0m 	max.poll.records = 500
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	session.timeout.ms = 10000
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[36mconnect            |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,508] INFO [Producer clientId=producer-1] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:27,555] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,557] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,557] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,557] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,558] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,558] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,558] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,558] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,559] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,559] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,559] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,559] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,559] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,559] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,560] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,560] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,560] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,560] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,561] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,561] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,561] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,562] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,562] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,562] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,562] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,563] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,563] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,563] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,564] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,564] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,564] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,564] INFO Kafka startTimeMs: 1612021587564 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,579] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:27,596] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Subscribed to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.KafkaConsumer)
[36mconnect            |[0m [2021-01-30 15:46:27,604] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,605] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,605] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,605] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,605] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,605] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,605] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,606] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,606] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,606] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,606] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,606] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,606] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,607] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,607] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,607] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,608] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,608] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,608] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,608] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,608] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,608] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,609] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,609] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,609] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Seeking to EARLIEST offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,670] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-10 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,672] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-8 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,672] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-14 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,672] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-12 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,673] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,673] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,673] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-6 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,673] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,673] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-24 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,674] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-18 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,674] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-16 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,674] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-22 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,674] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-20 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,674] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-9 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,675] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-7 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,675] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-13 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,676] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-11 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,676] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,677] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-5 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,677] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,677] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-23 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,677] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-17 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,678] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-15 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,678] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-21 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,678] INFO [Consumer clientId=consumer-sandbox-1, groupId=sandbox] Resetting offset for partition connect-offsets-19 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,680] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:27,680] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:27,680] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore)
[36mconnect            |[0m [2021-01-30 15:46:27,688] INFO Worker started (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:27,689] INFO Starting KafkaBasedLog with topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:27,689] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,695] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,695] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,695] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,695] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,695] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,695] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,695] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,696] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,697] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,697] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,697] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,697] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,697] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,697] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,697] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,697] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,697] INFO Kafka startTimeMs: 1612021587697 (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka1             |[0m [2021-01-30 15:46:27,724] INFO Creating topic connect-status with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(1), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(1), 4 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:27,740] INFO [Controller id=1] New topics: [HashSet(connect-status)], deleted topics: [HashSet()], new partition replica assignment [HashMap(connect-status-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-status-3 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-status-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-status-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), connect-status-1 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:27,740] INFO [Controller id=1] New partition creation callback for connect-status-4,connect-status-3,connect-status-2,connect-status-0,connect-status-1 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:27,740] INFO [Controller id=1 epoch=1] Changed partition connect-status-4 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,741] INFO [Controller id=1 epoch=1] Changed partition connect-status-3 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,741] INFO [Controller id=1 epoch=1] Changed partition connect-status-2 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,741] INFO [Controller id=1 epoch=1] Changed partition connect-status-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,741] INFO [Controller id=1 epoch=1] Changed partition connect-status-1 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,741] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,742] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,742] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-1 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,742] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-3 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,742] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-4 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,742] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-2 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,742] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,760] INFO [Controller id=1 epoch=1] Changed partition connect-status-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,761] INFO [Controller id=1 epoch=1] Changed partition connect-status-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,761] INFO [Controller id=1 epoch=1] Changed partition connect-status-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,761] INFO [Controller id=1 epoch=1] Changed partition connect-status-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,761] INFO [Controller id=1 epoch=1] Changed partition connect-status-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,762] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-status-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,762] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-status-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,762] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-status-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,762] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-status-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,762] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-status-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,762] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 5 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,763] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 5 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,763] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,764] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-1 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,764] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-3 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,764] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-4 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,764] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-status-2 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,764] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,764] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 for 5 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,765] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,766] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-status', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 9 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,769] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,770] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,770] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,770] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,770] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 9 from controller 1 epoch 1 starting the become-leader transition for partition connect-status-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,770] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(connect-status-4, connect-status-3, connect-status-2, connect-status-0, connect-status-1) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,770] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 1 epoch 1 as part of the become-leader transition for 5 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,774] INFO [Log partition=connect-status-4, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,775] INFO Created log for partition connect-status-4 in /var/lib/kafka/data/connect-status-4 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,777] INFO [Partition connect-status-4 broker=1] No checkpointed highwatermark is found for partition connect-status-4 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,777] INFO [Partition connect-status-4 broker=1] Log loaded for partition connect-status-4 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,777] INFO [Broker id=1] Leader connect-status-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,785] INFO [Log partition=connect-status-1, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,786] INFO Created log for partition connect-status-1 in /var/lib/kafka/data/connect-status-1 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,786] INFO [Partition connect-status-1 broker=1] No checkpointed highwatermark is found for partition connect-status-1 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,786] INFO [Partition connect-status-1 broker=1] Log loaded for partition connect-status-1 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,786] INFO [Broker id=1] Leader connect-status-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,792] INFO [Log partition=connect-status-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,794] INFO Created log for partition connect-status-0 in /var/lib/kafka/data/connect-status-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,794] INFO [Partition connect-status-0 broker=1] No checkpointed highwatermark is found for partition connect-status-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,794] INFO [Partition connect-status-0 broker=1] Log loaded for partition connect-status-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,794] INFO [Broker id=1] Leader connect-status-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,800] INFO [Log partition=connect-status-3, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,802] INFO Created log for partition connect-status-3 in /var/lib/kafka/data/connect-status-3 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,802] INFO [Partition connect-status-3 broker=1] No checkpointed highwatermark is found for partition connect-status-3 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,802] INFO [Partition connect-status-3 broker=1] Log loaded for partition connect-status-3 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,802] INFO [Broker id=1] Leader connect-status-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,808] INFO [Log partition=connect-status-2, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:27,809] INFO Created log for partition connect-status-2 in /var/lib/kafka/data/connect-status-2 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:27,809] INFO [Partition connect-status-2 broker=1] No checkpointed highwatermark is found for partition connect-status-2 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,809] INFO [Partition connect-status-2 broker=1] Log loaded for partition connect-status-2 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:27,810] INFO [Broker id=1] Leader connect-status-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,813] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition connect-status-4 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,813] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition connect-status-1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,813] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition connect-status-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,813] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition connect-status-3 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,813] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 9 from controller 1 epoch 1 for the become-leader transition for partition connect-status-2 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,816] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=connect-status,partition_index=1,error_code=0,_tagged_fields={}},{topic_name=connect-status,partition_index=2,error_code=0,_tagged_fields={}},{topic_name=connect-status,partition_index=0,error_code=0,_tagged_fields={}},{topic_name=connect-status,partition_index=3,error_code=0,_tagged_fields={}},{topic_name=connect-status,partition_index=4,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 9 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,818] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-status', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-status-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,819] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-status', partitionIndex=4, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-status-4 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,819] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-status', partitionIndex=3, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-status-3 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,819] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-status', partitionIndex=2, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-status-2 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,819] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-status', partitionIndex=1, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-status-1 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,819] INFO [Broker id=1] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,822] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 10 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:27,824] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka1:9092 (org.apache.kafka.connect.util.TopicAdmin)
[36mconnect            |[0m [2021-01-30 15:46:27,829] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = producer-2
[36mconnect            |[0m 	compression.type = none
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 120000
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
[36mconnect            |[0m 	linger.ms = 0
[36mconnect            |[0m 	max.block.ms = 60000
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 1048576
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 0
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,837] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,838] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,838] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,838] INFO Kafka startTimeMs: 1612021587838 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,840] INFO ConsumerConfig values: 
[36mconnect            |[0m 	allow.auto.create.topics = true
[36mconnect            |[0m 	auto.commit.interval.ms = 5000
[36mconnect            |[0m 	auto.offset.reset = earliest
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	check.crcs = true
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = consumer-sandbox-2
[36mconnect            |[0m 	client.rack = 
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	enable.auto.commit = false
[36mconnect            |[0m 	exclude.internal.topics = true
[36mconnect            |[0m 	fetch.max.bytes = 52428800
[36mconnect            |[0m 	fetch.max.wait.ms = 500
[36mconnect            |[0m 	fetch.min.bytes = 1
[36mconnect            |[0m 	group.id = sandbox
[36mconnect            |[0m 	group.instance.id = null
[36mconnect            |[0m 	heartbeat.interval.ms = 3000
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.leave.group.on.close = true
[36mconnect            |[0m 	internal.throw.on.fetch.stable.offset.unsupported = false
[36mconnect            |[0m 	isolation.level = read_uncommitted
[36mconnect            |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[36mconnect            |[0m 	max.partition.fetch.bytes = 1048576
[36mconnect            |[0m 	max.poll.interval.ms = 300000
[36mconnect            |[0m 	max.poll.records = 500
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	session.timeout.ms = 10000
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[36mconnect            |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,850] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,851] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,852] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,852] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,852] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,852] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,852] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,852] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,852] INFO Kafka startTimeMs: 1612021587852 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,855] INFO [Producer clientId=producer-2] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:27,860] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:27,863] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Subscribed to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.KafkaConsumer)
[36mconnect            |[0m [2021-01-30 15:46:27,863] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Seeking to EARLIEST offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,863] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Seeking to EARLIEST offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,863] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Seeking to EARLIEST offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,863] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Seeking to EARLIEST offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,864] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Seeking to EARLIEST offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,882] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Resetting offset for partition connect-status-1 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,882] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Resetting offset for partition connect-status-2 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,883] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Resetting offset for partition connect-status-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,883] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Resetting offset for partition connect-status-3 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,883] INFO [Consumer clientId=consumer-sandbox-2, groupId=sandbox] Resetting offset for partition connect-status-4 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:27,884] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:27,884] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:27,891] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[36mconnect            |[0m [2021-01-30 15:46:27,892] INFO Starting KafkaBasedLog with topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:27,903] INFO AdminClientConfig values: 
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = 
[36mconnect            |[0m 	connections.max.idle.ms = 300000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m  (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,918] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.admin.AdminClientConfig)
[36mconnect            |[0m [2021-01-30 15:46:27,919] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,919] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:27,920] INFO Kafka startTimeMs: 1612021587919 (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka1             |[0m [2021-01-30 15:46:27,949] INFO Creating topic connect-configs with configuration {cleanup.policy=compact} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:27,980] INFO [Controller id=1] New topics: [HashSet(connect-configs)], deleted topics: [HashSet()], new partition replica assignment [Map(connect-configs-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:27,980] INFO [Controller id=1] New partition creation callback for connect-configs-0 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:27,980] INFO [Controller id=1 epoch=1] Changed partition connect-configs-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,980] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,981] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-configs-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,981] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,994] INFO [Controller id=1 epoch=1] Changed partition connect-configs-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,994] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-configs', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition connect-configs-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,994] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,995] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,996] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition connect-configs-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,996] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,999] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:27,999] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='connect-configs', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 11 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,001] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 11 from controller 1 epoch 1 starting the become-leader transition for partition connect-configs-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,001] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(connect-configs-0) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:28,001] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 11 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,006] INFO [Log partition=connect-configs-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:28,008] INFO Created log for partition connect-configs-0 in /var/lib/kafka/data/connect-configs-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> compact, flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:28,010] INFO [Partition connect-configs-0 broker=1] No checkpointed highwatermark is found for partition connect-configs-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:28,010] INFO [Partition connect-configs-0 broker=1] Log loaded for partition connect-configs-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:28,010] INFO [Broker id=1] Leader connect-configs-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,016] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 11 from controller 1 epoch 1 for the become-leader transition for partition connect-configs-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,021] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=connect-configs,partition_index=0,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 11 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,029] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='connect-configs', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition connect-configs-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,031] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:28,036] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 12 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:28,042] INFO Created topic (name=connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at kafka1:9092 (org.apache.kafka.connect.util.TopicAdmin)
[36mconnect            |[0m [2021-01-30 15:46:28,051] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = producer-3
[36mconnect            |[0m 	compression.type = none
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 2147483647
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
[36mconnect            |[0m 	linger.ms = 0
[36mconnect            |[0m 	max.block.ms = 60000
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 1048576
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,061] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,062] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:28,062] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:28,062] INFO Kafka startTimeMs: 1612021588062 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:28,064] INFO ConsumerConfig values: 
[36mconnect            |[0m 	allow.auto.create.topics = true
[36mconnect            |[0m 	auto.commit.interval.ms = 5000
[36mconnect            |[0m 	auto.offset.reset = earliest
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	check.crcs = true
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = consumer-sandbox-3
[36mconnect            |[0m 	client.rack = 
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	default.api.timeout.ms = 60000
[36mconnect            |[0m 	enable.auto.commit = false
[36mconnect            |[0m 	exclude.internal.topics = true
[36mconnect            |[0m 	fetch.max.bytes = 52428800
[36mconnect            |[0m 	fetch.max.wait.ms = 500
[36mconnect            |[0m 	fetch.min.bytes = 1
[36mconnect            |[0m 	group.id = sandbox
[36mconnect            |[0m 	group.instance.id = null
[36mconnect            |[0m 	heartbeat.interval.ms = 3000
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.leave.group.on.close = true
[36mconnect            |[0m 	internal.throw.on.fetch.stable.offset.unsupported = false
[36mconnect            |[0m 	isolation.level = read_uncommitted
[36mconnect            |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[36mconnect            |[0m 	max.partition.fetch.bytes = 1048576
[36mconnect            |[0m 	max.poll.interval.ms = 300000
[36mconnect            |[0m 	max.poll.records = 500
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[36mconnect            |[0m 	receive.buffer.bytes = 65536
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	session.timeout.ms = 10000
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[36mconnect            |[0m  (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,083] INFO [Producer clientId=producer-3] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:28,087] WARN The configuration 'log4j.loggers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'producer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'producer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'producer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'plugin.path' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'internal.key.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'producer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'status.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'offset.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'config.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'rest.advertised.host.name' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'status.storage.topic' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'consumer.request.timeout.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'consumer.retry.backoff.ms' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'consumer.bootstrap.servers' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'rest.port' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,088] WARN The configuration 'config.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'internal.key.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'value.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'internal.value.converter.schemas.enable' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'internal.value.converter' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'consumer.interceptor.classes' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'offset.storage.replication.factor' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'log4j.root.loglevel' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] WARN The configuration 'key.converter.schema.registry.url' was supplied but isn't a known config. (org.apache.kafka.clients.consumer.ConsumerConfig)
[36mconnect            |[0m [2021-01-30 15:46:28,089] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:28,089] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:28,089] INFO Kafka startTimeMs: 1612021588089 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:28,097] INFO [Consumer clientId=consumer-sandbox-3, groupId=sandbox] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:28,102] INFO [Consumer clientId=consumer-sandbox-3, groupId=sandbox] Subscribed to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.KafkaConsumer)
[36mconnect            |[0m [2021-01-30 15:46:28,102] INFO [Consumer clientId=consumer-sandbox-3, groupId=sandbox] Seeking to EARLIEST offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:28,129] INFO [Consumer clientId=consumer-sandbox-3, groupId=sandbox] Resetting offset for partition connect-configs-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState)
[36mconnect            |[0m [2021-01-30 15:46:28,131] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:28,131] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog)
[36mconnect            |[0m [2021-01-30 15:46:28,131] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore)
[36mconnect            |[0m [2021-01-30 15:46:28,131] INFO [Worker clientId=connect-1, groupId=sandbox] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:28,161] INFO [Worker clientId=connect-1, groupId=sandbox] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:28,162] INFO [Worker clientId=connect-1, groupId=sandbox] Discovered group coordinator kafka1:9092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:28,165] INFO [Worker clientId=connect-1, groupId=sandbox] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:28,165] INFO [Worker clientId=connect-1, groupId=sandbox] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:28,180] INFO [Worker clientId=connect-1, groupId=sandbox] Rebalance failed. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36mconnect            |[0m org.apache.kafka.common.errors.MemberIdRequiredException: The group member needs to have a valid member id before actually entering a consumer group.
[36mconnect            |[0m [2021-01-30 15:46:28,184] INFO [Worker clientId=connect-1, groupId=sandbox] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:28,189] INFO [GroupCoordinator 1]: Preparing to rebalance group sandbox in state PreparingRebalance with old generation 0 (__consumer_offsets-7) (reason: Adding new member connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad with group instance id None) (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:28,192] INFO [GroupCoordinator 1]: Stabilized group sandbox generation 1 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:28,194] INFO [Worker clientId=connect-1, groupId=sandbox] Successfully joined group with generation Generation{generationId=1, memberId='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36mconnect            |[0m Jan 30, 2021 3:46:28 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[36mconnect            |[0m WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.LoggingResource will be ignored. 
[36mconnect            |[0m Jan 30, 2021 3:46:28 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[36mconnect            |[0m WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource will be ignored. 
[36mconnect            |[0m Jan 30, 2021 3:46:28 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[36mconnect            |[0m WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.RootResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.RootResource will be ignored. 
[36mconnect            |[0m Jan 30, 2021 3:46:28 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
[36mconnect            |[0m WARNING: A provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource will be ignored. 
[32mkafka1             |[0m [2021-01-30 15:46:28,232] INFO [GroupCoordinator 1]: Assignment received from leader for group sandbox for generation 1 (kafka.coordinator.group.GroupCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:28,239] INFO [Worker clientId=connect-1, groupId=sandbox] Successfully synced group in generation Generation{generationId=1, memberId='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:28,241] INFO [Worker clientId=connect-1, groupId=sandbox] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', leaderUrl='http://connect:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:28,242] INFO [Worker clientId=connect-1, groupId=sandbox] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:28,242] INFO [Worker clientId=connect-1, groupId=sandbox] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:28,348] INFO [Worker clientId=connect-1, groupId=sandbox] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m Jan 30, 2021 3:46:28 PM org.glassfish.jersey.internal.Errors logErrors
[36mconnect            |[0m WARNING: The following warnings have been detected: WARNING: The (sub)resource method listLoggers in org.apache.kafka.connect.runtime.rest.resources.LoggingResource contains empty path annotation.
[36mconnect            |[0m WARNING: The (sub)resource method createConnector in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
[36mconnect            |[0m WARNING: The (sub)resource method listConnectors in org.apache.kafka.connect.runtime.rest.resources.ConnectorsResource contains empty path annotation.
[36mconnect            |[0m WARNING: The (sub)resource method listConnectorPlugins in org.apache.kafka.connect.runtime.rest.resources.ConnectorPluginsResource contains empty path annotation.
[36mconnect            |[0m WARNING: The (sub)resource method serverInfo in org.apache.kafka.connect.runtime.rest.resources.RootResource contains empty path annotation.
[36mconnect            |[0m 
[36mconnect            |[0m [2021-01-30 15:46:28,521] INFO Started o.e.j.s.ServletContextHandler@221a2068{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
[36mconnect            |[0m [2021-01-30 15:46:28,522] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer)
[36mconnect            |[0m [2021-01-30 15:46:28,522] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect)
[36mconnect            |[0m [2021-01-30 15:46:31,930] INFO AbstractConfig values: 
[36mconnect            |[0m  (org.apache.kafka.common.config.AbstractConfig)
[36mconnect            |[0m [2021-01-30 15:46:31,943] INFO [Worker clientId=connect-1, groupId=sandbox] Connector RssSourceConnectorSportv2 config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:32,450] INFO [Worker clientId=connect-1, groupId=sandbox] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:32,450] INFO [Worker clientId=connect-1, groupId=sandbox] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:32,452] INFO [GroupCoordinator 1]: Preparing to rebalance group sandbox in state PreparingRebalance with old generation 1 (__consumer_offsets-7) (reason: leader connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:32,459] INFO [GroupCoordinator 1]: Stabilized group sandbox generation 2 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:32,464] INFO [Worker clientId=connect-1, groupId=sandbox] Successfully joined group with generation Generation{generationId=2, memberId='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:32,470] INFO [GroupCoordinator 1]: Assignment received from leader for group sandbox for generation 2 (kafka.coordinator.group.GroupCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:32,475] INFO [Worker clientId=connect-1, groupId=sandbox] Successfully synced group in generation Generation{generationId=2, memberId='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:32,476] INFO [Worker clientId=connect-1, groupId=sandbox] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', leaderUrl='http://connect:8083/', offset=2, connectorIds=[RssSourceConnectorSportv2], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:32,476] INFO [Worker clientId=connect-1, groupId=sandbox] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:32,478] INFO [Worker clientId=connect-1, groupId=sandbox] Starting connector RssSourceConnectorSportv2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:32,487] INFO Creating connector RssSourceConnectorSportv2 of type org.kaliy.kafka.connect.rss.RssSourceConnector (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:32,490] INFO SourceConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:32,492] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:32,499] INFO Instantiated connector RssSourceConnectorSportv2 with version 0.1.0 of type class org.kaliy.kafka.connect.rss.RssSourceConnector (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:32,500] INFO Finished creating connector RssSourceConnectorSportv2 (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:32,501] INFO [Worker clientId=connect-1, groupId=sandbox] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:32,511] INFO RssSourceConnectorConfig values: 
[36mconnect            |[0m 	rss.urls = https://e00-marca.uecdn.es/rss/portada.xml https://www.sport.es/es/rss/barca/rss.xml https://www.mundodeportivo.com/mvc/feed/rss/
[36mconnect            |[0m 	sleep.seconds = 60
[36mconnect            |[0m 	topic = sports_topics_v2
[36mconnect            |[0m  (org.kaliy.kafka.connect.rss.config.RssSourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:32,531] INFO SourceConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:32,536] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:33,479] INFO [Worker clientId=connect-1, groupId=sandbox] Tasks [RssSourceConnectorSportv2-0, RssSourceConnectorSportv2-1, RssSourceConnectorSportv2-2] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:33,986] INFO [Worker clientId=connect-1, groupId=sandbox] Handling task config update by restarting tasks [] (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:33,987] INFO [Worker clientId=connect-1, groupId=sandbox] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:33,987] INFO [Worker clientId=connect-1, groupId=sandbox] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:33,989] INFO [GroupCoordinator 1]: Preparing to rebalance group sandbox in state PreparingRebalance with old generation 2 (__consumer_offsets-7) (reason: leader connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad re-joining group during Stable) (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:33,990] INFO [GroupCoordinator 1]: Stabilized group sandbox generation 3 (__consumer_offsets-7) (kafka.coordinator.group.GroupCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:33,991] INFO [Worker clientId=connect-1, groupId=sandbox] Successfully joined group with generation Generation{generationId=3, memberId='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[32mkafka1             |[0m [2021-01-30 15:46:33,995] INFO [GroupCoordinator 1]: Assignment received from leader for group sandbox for generation 3 (kafka.coordinator.group.GroupCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:33,999] INFO [Worker clientId=connect-1, groupId=sandbox] Successfully synced group in generation Generation{generationId=3, memberId='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', protocol='sessioned'} (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
[36mconnect            |[0m [2021-01-30 15:46:33,999] INFO [Worker clientId=connect-1, groupId=sandbox] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-1-7d69b72d-891a-4389-aeae-bf6da667d8ad', leaderUrl='http://connect:8083/', offset=6, connectorIds=[RssSourceConnectorSportv2], taskIds=[RssSourceConnectorSportv2-0, RssSourceConnectorSportv2-1, RssSourceConnectorSportv2-2], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:34,002] INFO [Worker clientId=connect-1, groupId=sandbox] Starting connectors and tasks using config offset 6 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:34,003] INFO [Worker clientId=connect-1, groupId=sandbox] Starting task RssSourceConnectorSportv2-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:34,005] INFO [Worker clientId=connect-1, groupId=sandbox] Starting task RssSourceConnectorSportv2-1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:34,005] INFO Creating task RssSourceConnectorSportv2-0 (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,005] INFO Creating task RssSourceConnectorSportv2-1 (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,007] INFO [Worker clientId=connect-1, groupId=sandbox] Starting task RssSourceConnectorSportv2-2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:34,007] INFO Creating task RssSourceConnectorSportv2-2 (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,009] INFO ConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,009] INFO ConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,010] INFO ConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,013] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,015] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,017] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,020] INFO TaskConfig values: 
[36mconnect            |[0m 	task.class = class org.kaliy.kafka.connect.rss.RssSourceTask
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.TaskConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,021] INFO TaskConfig values: 
[36mconnect            |[0m 	task.class = class org.kaliy.kafka.connect.rss.RssSourceTask
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.TaskConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,020] INFO TaskConfig values: 
[36mconnect            |[0m 	task.class = class org.kaliy.kafka.connect.rss.RssSourceTask
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.TaskConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,028] INFO Instantiated task RssSourceConnectorSportv2-2 with version 0.1.0 of type org.kaliy.kafka.connect.rss.RssSourceTask (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,029] INFO Instantiated task RssSourceConnectorSportv2-0 with version 0.1.0 of type org.kaliy.kafka.connect.rss.RssSourceTask (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,029] INFO Instantiated task RssSourceConnectorSportv2-1 with version 0.1.0 of type org.kaliy.kafka.connect.rss.RssSourceTask (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,030] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = key
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,029] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = key
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,030] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = value
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,030] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = key
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task RssSourceConnectorSportv2-2 using the connector config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,030] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = value
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task RssSourceConnectorSportv2-2 using the connector config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO JsonConverterConfig values: 
[36mconnect            |[0m 	converter.type = value
[36mconnect            |[0m 	decimal.format = BASE64
[36mconnect            |[0m 	schemas.cache.size = 1000
[36mconnect            |[0m 	schemas.enable = false
[36mconnect            |[0m  (org.apache.kafka.connect.json.JsonConverterConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task RssSourceConnectorSportv2-1 using the connector config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task RssSourceConnectorSportv2-1 using the connector config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task RssSourceConnectorSportv2-0 using the connector config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task RssSourceConnectorSportv2-0 using the connector config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task RssSourceConnectorSportv2-1 using the worker config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,031] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task RssSourceConnectorSportv2-0 using the worker config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,032] INFO Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task RssSourceConnectorSportv2-2 using the worker config (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,037] INFO SourceConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,037] INFO SourceConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,037] INFO SourceConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.SourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,039] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,041] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,044] INFO EnrichedConnectorConfig values: 
[36mconnect            |[0m 	config.action.reload = restart
[36mconnect            |[0m 	connector.class = org.kaliy.kafka.connect.rss.RssSourceConnector
[36mconnect            |[0m 	errors.log.enable = false
[36mconnect            |[0m 	errors.log.include.messages = false
[36mconnect            |[0m 	errors.retry.delay.max.ms = 60000
[36mconnect            |[0m 	errors.retry.timeout = 0
[36mconnect            |[0m 	errors.tolerance = none
[36mconnect            |[0m 	header.converter = null
[36mconnect            |[0m 	key.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m 	name = RssSourceConnectorSportv2
[36mconnect            |[0m 	predicates = []
[36mconnect            |[0m 	tasks.max = 3
[36mconnect            |[0m 	topic.creation.groups = []
[36mconnect            |[0m 	transforms = [createKey, extractFeed, extractTitle]
[36mconnect            |[0m 	transforms.createKey.fields = [feed]
[36mconnect            |[0m 	transforms.createKey.negate = false
[36mconnect            |[0m 	transforms.createKey.predicate = 
[36mconnect            |[0m 	transforms.createKey.type = class org.apache.kafka.connect.transforms.ValueToKey
[36mconnect            |[0m 	transforms.extractFeed.field = feed
[36mconnect            |[0m 	transforms.extractFeed.negate = false
[36mconnect            |[0m 	transforms.extractFeed.predicate = 
[36mconnect            |[0m 	transforms.extractFeed.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	transforms.extractTitle.field = url
[36mconnect            |[0m 	transforms.extractTitle.negate = false
[36mconnect            |[0m 	transforms.extractTitle.predicate = 
[36mconnect            |[0m 	transforms.extractTitle.type = class org.apache.kafka.connect.transforms.ExtractField$Key
[36mconnect            |[0m 	value.converter = class org.apache.kafka.connect.json.JsonConverter
[36mconnect            |[0m  (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,049] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,049] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,050] INFO Initializing: org.apache.kafka.connect.runtime.TransformationChain{org.apache.kafka.connect.transforms.ValueToKey, org.apache.kafka.connect.transforms.ExtractField$Key, org.apache.kafka.connect.transforms.ExtractField$Key} (org.apache.kafka.connect.runtime.Worker)
[36mconnect            |[0m [2021-01-30 15:46:34,055] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = connector-producer-RssSourceConnectorSportv2-1
[36mconnect            |[0m 	compression.type = none
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 2147483647
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = [io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor]
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m 	linger.ms = 0
[36mconnect            |[0m 	max.block.ms = 9223372036854775807
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 1048576
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,055] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = connector-producer-RssSourceConnectorSportv2-0
[36mconnect            |[0m 	compression.type = none
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 2147483647
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = [io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor]
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m 	linger.ms = 0
[36mconnect            |[0m 	max.block.ms = 9223372036854775807
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 1048576
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,055] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = connector-producer-RssSourceConnectorSportv2-2
[36mconnect            |[0m 	compression.type = none
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 2147483647
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = [io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor]
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m 	linger.ms = 0
[36mconnect            |[0m 	max.block.ms = 9223372036854775807
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 1048576
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 2000
[36mconnect            |[0m 	retries = 2147483647
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,067] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,067] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,067] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,067] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,067] INFO Kafka startTimeMs: 1612021594067 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,072] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,074] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,074] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,074] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,075] INFO Kafka startTimeMs: 1612021594074 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,083] WARN The configuration 'metrics.context.connect.group.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,083] WARN The configuration 'metrics.context.connect.kafka.cluster.id' was supplied but isn't a known config. (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,083] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,083] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,083] INFO Kafka startTimeMs: 1612021594083 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:34,088] INFO [Producer clientId=connector-producer-RssSourceConnectorSportv2-1] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:34,098] INFO [Producer clientId=connector-producer-RssSourceConnectorSportv2-0] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:34,098] INFO [Producer clientId=connector-producer-RssSourceConnectorSportv2-2] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:34,100] INFO [Worker clientId=connect-1, groupId=sandbox] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder)
[36mconnect            |[0m [2021-01-30 15:46:34,111] INFO RssSourceConnectorConfig values: 
[36mconnect            |[0m 	rss.urls = https://www.sport.es/es/rss/barca/rss.xml
[36mconnect            |[0m 	sleep.seconds = 60
[36mconnect            |[0m 	topic = sports_topics_v2
[36mconnect            |[0m  (org.kaliy.kafka.connect.rss.config.RssSourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,111] INFO Starting task with properties {connector.class=org.kaliy.kafka.connect.rss.RssSourceConnector, transforms.createKey.type=org.apache.kafka.connect.transforms.ValueToKey, transforms.extractFeed.type=org.apache.kafka.connect.transforms.ExtractField$Key, tasks.max=3, transforms=createKey, extractFeed, extractTitle, key.ignore=false, rss.urls=https://www.sport.es/es/rss/barca/rss.xml, key.converter.schemas.enable=false, task.class=org.kaliy.kafka.connect.rss.RssSourceTask, transforms.createKey.fields=feed, transforms.extractTitle.type=org.apache.kafka.connect.transforms.ExtractField$Key, value.converter.schemas.enable=false, name=RssSourceConnectorSportv2, topic=sports_topics_v2, value.converter=org.apache.kafka.connect.json.JsonConverter, transforms.extractTitle.field=url, transforms.extractFeed.field=feed, key.converter=org.apache.kafka.connect.json.JsonConverter} (org.kaliy.kafka.connect.rss.RssSourceTask)
[36mconnect            |[0m [2021-01-30 15:46:34,118] INFO RssSourceConnectorConfig values: 
[36mconnect            |[0m 	rss.urls = https://www.mundodeportivo.com/mvc/feed/rss/
[36mconnect            |[0m 	sleep.seconds = 60
[36mconnect            |[0m 	topic = sports_topics_v2
[36mconnect            |[0m  (org.kaliy.kafka.connect.rss.config.RssSourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,109] INFO RssSourceConnectorConfig values: 
[36mconnect            |[0m 	rss.urls = https://e00-marca.uecdn.es/rss/portada.xml
[36mconnect            |[0m 	sleep.seconds = 60
[36mconnect            |[0m 	topic = sports_topics_v2
[36mconnect            |[0m  (org.kaliy.kafka.connect.rss.config.RssSourceConnectorConfig)
[36mconnect            |[0m [2021-01-30 15:46:34,121] INFO Starting task with properties {connector.class=org.kaliy.kafka.connect.rss.RssSourceConnector, transforms.createKey.type=org.apache.kafka.connect.transforms.ValueToKey, transforms.extractFeed.type=org.apache.kafka.connect.transforms.ExtractField$Key, tasks.max=3, transforms=createKey, extractFeed, extractTitle, key.ignore=false, rss.urls=https://e00-marca.uecdn.es/rss/portada.xml, key.converter.schemas.enable=false, task.class=org.kaliy.kafka.connect.rss.RssSourceTask, transforms.createKey.fields=feed, transforms.extractTitle.type=org.apache.kafka.connect.transforms.ExtractField$Key, value.converter.schemas.enable=false, name=RssSourceConnectorSportv2, topic=sports_topics_v2, value.converter=org.apache.kafka.connect.json.JsonConverter, transforms.extractTitle.field=url, transforms.extractFeed.field=feed, key.converter=org.apache.kafka.connect.json.JsonConverter} (org.kaliy.kafka.connect.rss.RssSourceTask)
[36mconnect            |[0m [2021-01-30 15:46:34,118] INFO Starting task with properties {connector.class=org.kaliy.kafka.connect.rss.RssSourceConnector, transforms.createKey.type=org.apache.kafka.connect.transforms.ValueToKey, transforms.extractFeed.type=org.apache.kafka.connect.transforms.ExtractField$Key, tasks.max=3, transforms=createKey, extractFeed, extractTitle, key.ignore=false, rss.urls=https://www.mundodeportivo.com/mvc/feed/rss/, key.converter.schemas.enable=false, task.class=org.kaliy.kafka.connect.rss.RssSourceTask, transforms.createKey.fields=feed, transforms.extractTitle.type=org.apache.kafka.connect.transforms.ExtractField$Key, value.converter.schemas.enable=false, name=RssSourceConnectorSportv2, topic=sports_topics_v2, value.converter=org.apache.kafka.connect.json.JsonConverter, transforms.extractTitle.field=url, transforms.extractFeed.field=feed, key.converter=org.apache.kafka.connect.json.JsonConverter} (org.kaliy.kafka.connect.rss.RssSourceTask)
[36mconnect            |[0m [2021-01-30 15:46:34,353] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-1} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:46:34,353] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:46:34,354] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-2} Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask)
[32mkafka1             |[0m [2021-01-30 15:46:35,467] INFO Creating topic sports_topics_v2 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:35,467] INFO Creating topic sports_topics_v2 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:35,468] INFO Creating topic sports_topics_v2 with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:35,483] INFO [KafkaApi-1] Auto creation of topic sports_topics_v2 with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[36mconnect            |[0m [2021-01-30 15:46:35,485] WARN [Producer clientId=connector-producer-RssSourceConnectorSportv2-1] Error while fetching metadata with correlation id 3 : {sports_topics_v2=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[32mkafka1             |[0m [2021-01-30 15:46:35,490] INFO [Controller id=1] New topics: [HashSet(sports_topics_v2)], deleted topics: [HashSet()], new partition replica assignment [Map(sports_topics_v2-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:35,490] INFO [Controller id=1] New partition creation callback for sports_topics_v2-0 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:35,491] INFO [Controller id=1 epoch=1] Changed partition sports_topics_v2-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,491] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,491] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition sports_topics_v2-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,491] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:35,491] WARN [Producer clientId=connector-producer-RssSourceConnectorSportv2-2] Error while fetching metadata with correlation id 3 : {sports_topics_v2=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[36mconnect            |[0m [2021-01-30 15:46:35,494] WARN [Producer clientId=connector-producer-RssSourceConnectorSportv2-0] Error while fetching metadata with correlation id 3 : {sports_topics_v2=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[32mkafka1             |[0m [2021-01-30 15:46:35,504] INFO [Controller id=1 epoch=1] Changed partition sports_topics_v2-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,504] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='sports_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition sports_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,504] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,505] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,506] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition sports_topics_v2-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,506] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,508] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 13 from controller 1 for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,508] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='sports_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 13 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,510] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 13 from controller 1 epoch 1 starting the become-leader transition for partition sports_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,510] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(sports_topics_v2-0) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:35,510] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 13 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,515] INFO [Log partition=sports_topics_v2-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:35,516] INFO Created log for partition sports_topics_v2-0 in /var/lib/kafka/data/sports_topics_v2-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:35,517] INFO [Partition sports_topics_v2-0 broker=1] No checkpointed highwatermark is found for partition sports_topics_v2-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:35,517] INFO [Partition sports_topics_v2-0 broker=1] Log loaded for partition sports_topics_v2-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:35,517] INFO [Broker id=1] Leader sports_topics_v2-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,520] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 13 from controller 1 epoch 1 for the become-leader transition for partition sports_topics_v2-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,522] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=sports_topics_v2,partition_index=0,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 13 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,525] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='sports_topics_v2', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition sports_topics_v2-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,525] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 14 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:35,527] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 14 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:35,997] INFO creating interceptor (io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor)
[36mconnect            |[0m [2021-01-30 15:46:36,003] INFO creating interceptor (io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor)
[36mconnect            |[0m [2021-01-30 15:46:36,006] INFO creating interceptor (io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor)
[36mconnect            |[0m [2021-01-30 15:46:36,146] INFO MonitoringInterceptorConfig values: 
[36mconnect            |[0m 	confluent.monitoring.interceptor.publishMs = 15000
[36mconnect            |[0m 	confluent.monitoring.interceptor.topic = _confluent-monitoring
[36mconnect            |[0m  (io.confluent.monitoring.clients.interceptor.MonitoringInterceptorConfig)
[36mconnect            |[0m [2021-01-30 15:46:36,146] INFO MonitoringInterceptorConfig values: 
[36mconnect            |[0m 	confluent.monitoring.interceptor.publishMs = 15000
[36mconnect            |[0m 	confluent.monitoring.interceptor.topic = _confluent-monitoring
[36mconnect            |[0m  (io.confluent.monitoring.clients.interceptor.MonitoringInterceptorConfig)
[36mconnect            |[0m [2021-01-30 15:46:36,147] INFO MonitoringInterceptorConfig values: 
[36mconnect            |[0m 	confluent.monitoring.interceptor.publishMs = 15000
[36mconnect            |[0m 	confluent.monitoring.interceptor.topic = _confluent-monitoring
[36mconnect            |[0m  (io.confluent.monitoring.clients.interceptor.MonitoringInterceptorConfig)
[36mconnect            |[0m [2021-01-30 15:46:36,180] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-1
[36mconnect            |[0m 	compression.type = lz4
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 120000
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m 	linger.ms = 500
[36mconnect            |[0m 	max.block.ms = 60000
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 10485760
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 30000
[36mconnect            |[0m 	retries = 10
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:36,180] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-2
[36mconnect            |[0m 	compression.type = lz4
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 120000
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m 	linger.ms = 500
[36mconnect            |[0m 	max.block.ms = 60000
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 10485760
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 30000
[36mconnect            |[0m 	retries = 10
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:36,180] INFO ProducerConfig values: 
[36mconnect            |[0m 	acks = -1
[36mconnect            |[0m 	batch.size = 16384
[36mconnect            |[0m 	bootstrap.servers = [kafka1:9092]
[36mconnect            |[0m 	buffer.memory = 33554432
[36mconnect            |[0m 	client.dns.lookup = use_all_dns_ips
[36mconnect            |[0m 	client.id = confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-0
[36mconnect            |[0m 	compression.type = lz4
[36mconnect            |[0m 	connections.max.idle.ms = 540000
[36mconnect            |[0m 	delivery.timeout.ms = 120000
[36mconnect            |[0m 	enable.idempotence = false
[36mconnect            |[0m 	interceptor.classes = []
[36mconnect            |[0m 	internal.auto.downgrade.txn.commit = false
[36mconnect            |[0m 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m 	linger.ms = 500
[36mconnect            |[0m 	max.block.ms = 60000
[36mconnect            |[0m 	max.in.flight.requests.per.connection = 1
[36mconnect            |[0m 	max.request.size = 10485760
[36mconnect            |[0m 	metadata.max.age.ms = 300000
[36mconnect            |[0m 	metadata.max.idle.ms = 300000
[36mconnect            |[0m 	metric.reporters = []
[36mconnect            |[0m 	metrics.num.samples = 2
[36mconnect            |[0m 	metrics.recording.level = INFO
[36mconnect            |[0m 	metrics.sample.window.ms = 30000
[36mconnect            |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[36mconnect            |[0m 	receive.buffer.bytes = 32768
[36mconnect            |[0m 	reconnect.backoff.max.ms = 1000
[36mconnect            |[0m 	reconnect.backoff.ms = 50
[36mconnect            |[0m 	request.timeout.ms = 30000
[36mconnect            |[0m 	retries = 10
[36mconnect            |[0m 	retry.backoff.ms = 500
[36mconnect            |[0m 	sasl.client.callback.handler.class = null
[36mconnect            |[0m 	sasl.jaas.config = null
[36mconnect            |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[36mconnect            |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[36mconnect            |[0m 	sasl.kerberos.service.name = null
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[36mconnect            |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.callback.handler.class = null
[36mconnect            |[0m 	sasl.login.class = null
[36mconnect            |[0m 	sasl.login.refresh.buffer.seconds = 300
[36mconnect            |[0m 	sasl.login.refresh.min.period.seconds = 60
[36mconnect            |[0m 	sasl.login.refresh.window.factor = 0.8
[36mconnect            |[0m 	sasl.login.refresh.window.jitter = 0.05
[36mconnect            |[0m 	sasl.mechanism = GSSAPI
[36mconnect            |[0m 	security.protocol = PLAINTEXT
[36mconnect            |[0m 	security.providers = null
[36mconnect            |[0m 	send.buffer.bytes = 131072
[36mconnect            |[0m 	ssl.cipher.suites = null
[36mconnect            |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[36mconnect            |[0m 	ssl.endpoint.identification.algorithm = https
[36mconnect            |[0m 	ssl.engine.factory.class = null
[36mconnect            |[0m 	ssl.key.password = null
[36mconnect            |[0m 	ssl.keymanager.algorithm = SunX509
[36mconnect            |[0m 	ssl.keystore.location = null
[36mconnect            |[0m 	ssl.keystore.password = null
[36mconnect            |[0m 	ssl.keystore.type = JKS
[36mconnect            |[0m 	ssl.protocol = TLSv1.3
[36mconnect            |[0m 	ssl.provider = null
[36mconnect            |[0m 	ssl.secure.random.implementation = null
[36mconnect            |[0m 	ssl.trustmanager.algorithm = PKIX
[36mconnect            |[0m 	ssl.truststore.location = null
[36mconnect            |[0m 	ssl.truststore.password = null
[36mconnect            |[0m 	ssl.truststore.type = JKS
[36mconnect            |[0m 	transaction.timeout.ms = 60000
[36mconnect            |[0m 	transactional.id = null
[36mconnect            |[0m 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
[36mconnect            |[0m  (org.apache.kafka.clients.producer.ProducerConfig)
[36mconnect            |[0m [2021-01-30 15:46:36,199] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,201] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,202] INFO Kafka startTimeMs: 1612021596199 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,203] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,205] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,205] INFO Kafka startTimeMs: 1612021596203 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,206] INFO Kafka version: 6.0.1-ccs (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,206] INFO Kafka commitId: 9c1fbb3db1e0d69d (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,206] INFO Kafka startTimeMs: 1612021596206 (org.apache.kafka.common.utils.AppInfoParser)
[36mconnect            |[0m [2021-01-30 15:46:36,214] INFO interceptor=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-2 created for client_id=connector-producer-RssSourceConnectorSportv2-2 client_type=PRODUCER session= cluster=3-wYX40qRXev0Kr60NxjUw (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)
[36mconnect            |[0m [2021-01-30 15:46:36,214] INFO interceptor=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-1 created for client_id=connector-producer-RssSourceConnectorSportv2-1 client_type=PRODUCER session= cluster=3-wYX40qRXev0Kr60NxjUw (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)
[36mconnect            |[0m [2021-01-30 15:46:36,219] INFO interceptor=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-0 created for client_id=connector-producer-RssSourceConnectorSportv2-0 client_type=PRODUCER session= cluster=3-wYX40qRXev0Kr60NxjUw (io.confluent.monitoring.clients.interceptor.MonitoringInterceptor)
[36mconnect            |[0m [2021-01-30 15:46:36,227] INFO [Producer clientId=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-1] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:36,229] INFO [Producer clientId=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-2] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[36mconnect            |[0m [2021-01-30 15:46:36,232] INFO [Producer clientId=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-0] Cluster ID: 3-wYX40qRXev0Kr60NxjUw (org.apache.kafka.clients.Metadata)
[33mkafka-ui           |[0m 15:46:40.780 [parallel-2] DEBUG com.provectus.kafka.ui.cluster.service.MetricsUpdateService - Start getting metrics for kafkaCluster: Broker1
[33mkafka-ui           |[0m 15:46:40.782 [parallel-2] INFO  org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
[33mkafka-ui           |[0m 	bootstrap.servers = [kafka1:9092]
[33mkafka-ui           |[0m 	client.dns.lookup = default
[33mkafka-ui           |[0m 	client.id = 
[33mkafka-ui           |[0m 	connections.max.idle.ms = 300000
[33mkafka-ui           |[0m 	metadata.max.age.ms = 300000
[33mkafka-ui           |[0m 	metric.reporters = []
[33mkafka-ui           |[0m 	metrics.num.samples = 2
[33mkafka-ui           |[0m 	metrics.recording.level = INFO
[33mkafka-ui           |[0m 	metrics.sample.window.ms = 30000
[33mkafka-ui           |[0m 	receive.buffer.bytes = 65536
[33mkafka-ui           |[0m 	reconnect.backoff.max.ms = 1000
[33mkafka-ui           |[0m 	reconnect.backoff.ms = 50
[33mkafka-ui           |[0m 	request.timeout.ms = 5000
[33mkafka-ui           |[0m 	retries = 5
[33mkafka-ui           |[0m 	retry.backoff.ms = 100
[33mkafka-ui           |[0m 	sasl.client.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.jaas.config = null
[33mkafka-ui           |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mkafka-ui           |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mkafka-ui           |[0m 	sasl.kerberos.service.name = null
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.login.class = null
[33mkafka-ui           |[0m 	sasl.login.refresh.buffer.seconds = 300
[33mkafka-ui           |[0m 	sasl.login.refresh.min.period.seconds = 60
[33mkafka-ui           |[0m 	sasl.login.refresh.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.refresh.window.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.mechanism = GSSAPI
[33mkafka-ui           |[0m 	security.protocol = PLAINTEXT
[33mkafka-ui           |[0m 	security.providers = null
[33mkafka-ui           |[0m 	send.buffer.bytes = 131072
[33mkafka-ui           |[0m 	ssl.cipher.suites = null
[33mkafka-ui           |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mkafka-ui           |[0m 	ssl.endpoint.identification.algorithm = https
[33mkafka-ui           |[0m 	ssl.key.password = null
[33mkafka-ui           |[0m 	ssl.keymanager.algorithm = SunX509
[33mkafka-ui           |[0m 	ssl.keystore.location = null
[33mkafka-ui           |[0m 	ssl.keystore.password = null
[33mkafka-ui           |[0m 	ssl.keystore.type = JKS
[33mkafka-ui           |[0m 	ssl.protocol = TLS
[33mkafka-ui           |[0m 	ssl.provider = null
[33mkafka-ui           |[0m 	ssl.secure.random.implementation = null
[33mkafka-ui           |[0m 	ssl.trustmanager.algorithm = PKIX
[33mkafka-ui           |[0m 	ssl.truststore.location = null
[33mkafka-ui           |[0m 	ssl.truststore.password = null
[33mkafka-ui           |[0m 	ssl.truststore.type = JKS
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m 15:46:40.792 [parallel-2] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.4.0
[33mkafka-ui           |[0m 15:46:40.792 [parallel-2] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 77a89fcf8d7fa018
[33mkafka-ui           |[0m 15:46:40.793 [parallel-2] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021600792
[33mkafka-ui           |[0m 15:46:40.876 [ZkClient-EventThread-99-null] INFO  org.I0Itec.zkclient.ZkEventThread - Starting ZkClient event thread.
[33mkafka-ui           |[0m 15:46:40.877 [kafka-admin-client-thread | adminclient-1] ERROR com.provectus.kafka.ui.zookeeper.ZookeeperService - Error while creating zookeeper client for cluster Broker1
[33mkafka-ui           |[0m 15:46:40.877 [kafka-admin-client-thread | adminclient-1] DEBUG com.provectus.kafka.ui.zookeeper.ZookeeperService - Start getting Zookeeper metrics for kafkaCluster: Broker1
[33mkafka-ui           |[0m 15:46:46.828 [boundedElastic-3] DEBUG org.springframework.http.codec.json.Jackson2JsonEncoder - [16ec81a4] Encoding [[class Cluster {
[33mkafka-ui           |[0m     name: Broker1
[33mkafka-ui           |[0m     defaultCluster: null
[33mkafka-ui           |[0m     status: online
[33mkafka-ui           |[0m     brokerCount: 1
[33mkafka-ui           |[0m    (truncated)...]
[33mkafka-ui           |[0m 15:46:46.915 [boundedElastic-3] DEBUG org.springframework.http.codec.json.Jackson2JsonEncoder - [16ec81a4] Encoding [[class Topic {
[33mkafka-ui           |[0m     name: mundo_topics_v2
[33mkafka-ui           |[0m     internal: false
[33mkafka-ui           |[0m     partitionCount: 1
[33mkafka-ui           |[0m     replicationFa (truncated)...]
[33mkafka-ui           |[0m 15:46:48.400 [boundedElastic-3] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
[33mkafka-ui           |[0m 	allow.auto.create.topics = true
[33mkafka-ui           |[0m 	auto.commit.interval.ms = 5000
[33mkafka-ui           |[0m 	auto.offset.reset = earliest
[33mkafka-ui           |[0m 	bootstrap.servers = [kafka1:9092]
[33mkafka-ui           |[0m 	check.crcs = true
[33mkafka-ui           |[0m 	client.dns.lookup = default
[33mkafka-ui           |[0m 	client.id = kafka-ui
[33mkafka-ui           |[0m 	client.rack = 
[33mkafka-ui           |[0m 	connections.max.idle.ms = 540000
[33mkafka-ui           |[0m 	default.api.timeout.ms = 60000
[33mkafka-ui           |[0m 	enable.auto.commit = true
[33mkafka-ui           |[0m 	exclude.internal.topics = true
[33mkafka-ui           |[0m 	fetch.max.bytes = 52428800
[33mkafka-ui           |[0m 	fetch.max.wait.ms = 500
[33mkafka-ui           |[0m 	fetch.min.bytes = 1
[33mkafka-ui           |[0m 	group.id = null
[33mkafka-ui           |[0m 	group.instance.id = null
[33mkafka-ui           |[0m 	heartbeat.interval.ms = 3000
[33mkafka-ui           |[0m 	interceptor.classes = []
[33mkafka-ui           |[0m 	internal.leave.group.on.close = true
[33mkafka-ui           |[0m 	isolation.level = read_uncommitted
[33mkafka-ui           |[0m 	key.deserializer = class org.apache.kafka.common.serialization.BytesDeserializer
[33mkafka-ui           |[0m 	max.partition.fetch.bytes = 1048576
[33mkafka-ui           |[0m 	max.poll.interval.ms = 300000
[33mkafka-ui           |[0m 	max.poll.records = 500
[33mkafka-ui           |[0m 	metadata.max.age.ms = 300000
[33mkafka-ui           |[0m 	metric.reporters = []
[33mkafka-ui           |[0m 	metrics.num.samples = 2
[33mkafka-ui           |[0m 	metrics.recording.level = INFO
[33mkafka-ui           |[0m 	metrics.sample.window.ms = 30000
[33mkafka-ui           |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[33mkafka-ui           |[0m 	receive.buffer.bytes = 65536
[33mkafka-ui           |[0m 	reconnect.backoff.max.ms = 1000
[33mkafka-ui           |[0m 	reconnect.backoff.ms = 50
[33mkafka-ui           |[0m 	request.timeout.ms = 30000
[33mkafka-ui           |[0m 	retry.backoff.ms = 100
[33mkafka-ui           |[0m 	sasl.client.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.jaas.config = null
[33mkafka-ui           |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mkafka-ui           |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mkafka-ui           |[0m 	sasl.kerberos.service.name = null
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.login.class = null
[33mkafka-ui           |[0m 	sasl.login.refresh.buffer.seconds = 300
[33mkafka-ui           |[0m 	sasl.login.refresh.min.period.seconds = 60
[33mkafka-ui           |[0m 	sasl.login.refresh.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.refresh.window.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.mechanism = GSSAPI
[33mkafka-ui           |[0m 	security.protocol = PLAINTEXT
[33mkafka-ui           |[0m 	security.providers = null
[33mkafka-ui           |[0m 	send.buffer.bytes = 131072
[33mkafka-ui           |[0m 	session.timeout.ms = 10000
[33mkafka-ui           |[0m 	ssl.cipher.suites = null
[33mkafka-ui           |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mkafka-ui           |[0m 	ssl.endpoint.identification.algorithm = https
[33mkafka-ui           |[0m 	ssl.key.password = null
[33mkafka-ui           |[0m 	ssl.keymanager.algorithm = SunX509
[33mkafka-ui           |[0m 	ssl.keystore.location = null
[33mkafka-ui           |[0m 	ssl.keystore.password = null
[33mkafka-ui           |[0m 	ssl.keystore.type = JKS
[33mkafka-ui           |[0m 	ssl.protocol = TLS
[33mkafka-ui           |[0m 	ssl.provider = null
[33mkafka-ui           |[0m 	ssl.secure.random.implementation = null
[33mkafka-ui           |[0m 	ssl.trustmanager.algorithm = PKIX
[33mkafka-ui           |[0m 	ssl.truststore.location = null
[33mkafka-ui           |[0m 	ssl.truststore.password = null
[33mkafka-ui           |[0m 	ssl.truststore.type = JKS
[33mkafka-ui           |[0m 	value.deserializer = class org.apache.kafka.common.serialization.BytesDeserializer
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m 15:46:48.498 [boundedElastic-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.4.0
[33mkafka-ui           |[0m 15:46:48.498 [boundedElastic-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 77a89fcf8d7fa018
[33mkafka-ui           |[0m 15:46:48.498 [boundedElastic-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021608498
[33mkafka-ui           |[0m 15:46:48.542 [boundedElastic-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=kafka-ui, groupId=null] Cluster ID: 3-wYX40qRXev0Kr60NxjUw
[33mkafka-ui           |[0m 15:46:48.592 [boundedElastic-3] DEBUG org.springframework.http.codec.json.Jackson2JsonEncoder - [16ec81a4] Encoding [class TopicDetails {
[33mkafka-ui           |[0m     name: sports_topics_v2
[33mkafka-ui           |[0m     internal: false
[33mkafka-ui           |[0m     partitions: [class Partition (truncated)...]
[34mzookeeper          |[0m [2021-01-30 15:46:49,433] INFO Processing ruok command from /127.0.0.1:50724 (org.apache.zookeeper.server.NIOServerCnxn)
[35mschema-registry    |[0m [2021-01-30 15:46:50,931] INFO 127.0.0.1 - - [30/Jan/2021:15:46:50 +0000] "GET / HTTP/1.1" 200 2  8 (io.confluent.rest-utils.requests)
[33mkafka-ui           |[0m 15:46:51.096 [boundedElastic-3] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
[33mkafka-ui           |[0m 	allow.auto.create.topics = true
[33mkafka-ui           |[0m 	auto.commit.interval.ms = 5000
[33mkafka-ui           |[0m 	auto.offset.reset = earliest
[33mkafka-ui           |[0m 	bootstrap.servers = [kafka1:9092]
[33mkafka-ui           |[0m 	check.crcs = true
[33mkafka-ui           |[0m 	client.dns.lookup = default
[33mkafka-ui           |[0m 	client.id = kafka-ui
[33mkafka-ui           |[0m 	client.rack = 
[33mkafka-ui           |[0m 	connections.max.idle.ms = 540000
[33mkafka-ui           |[0m 	default.api.timeout.ms = 60000
[33mkafka-ui           |[0m 	enable.auto.commit = true
[33mkafka-ui           |[0m 	exclude.internal.topics = true
[33mkafka-ui           |[0m 	fetch.max.bytes = 52428800
[33mkafka-ui           |[0m 	fetch.max.wait.ms = 500
[33mkafka-ui           |[0m 	fetch.min.bytes = 1
[33mkafka-ui           |[0m 	group.id = null
[33mkafka-ui           |[0m 	group.instance.id = null
[33mkafka-ui           |[0m 	heartbeat.interval.ms = 3000
[33mkafka-ui           |[0m 	interceptor.classes = []
[33mkafka-ui           |[0m 	internal.leave.group.on.close = true
[33mkafka-ui           |[0m 	isolation.level = read_uncommitted
[33mkafka-ui           |[0m 	key.deserializer = class org.apache.kafka.common.serialization.BytesDeserializer
[33mkafka-ui           |[0m 	max.partition.fetch.bytes = 1048576
[33mkafka-ui           |[0m 	max.poll.interval.ms = 300000
[33mkafka-ui           |[0m 	max.poll.records = 500
[33mkafka-ui           |[0m 	metadata.max.age.ms = 300000
[33mkafka-ui           |[0m 	metric.reporters = []
[33mkafka-ui           |[0m 	metrics.num.samples = 2
[33mkafka-ui           |[0m 	metrics.recording.level = INFO
[33mkafka-ui           |[0m 	metrics.sample.window.ms = 30000
[33mkafka-ui           |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[33mkafka-ui           |[0m 	receive.buffer.bytes = 65536
[33mkafka-ui           |[0m 	reconnect.backoff.max.ms = 1000
[33mkafka-ui           |[0m 	reconnect.backoff.ms = 50
[33mkafka-ui           |[0m 	request.timeout.ms = 30000
[33mkafka-ui           |[0m 	retry.backoff.ms = 100
[33mkafka-ui           |[0m 	sasl.client.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.jaas.config = null
[33mkafka-ui           |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mkafka-ui           |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mkafka-ui           |[0m 	sasl.kerberos.service.name = null
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.login.class = null
[33mkafka-ui           |[0m 	sasl.login.refresh.buffer.seconds = 300
[33mkafka-ui           |[0m 	sasl.login.refresh.min.period.seconds = 60
[33mkafka-ui           |[0m 	sasl.login.refresh.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.refresh.window.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.mechanism = GSSAPI
[33mkafka-ui           |[0m 	security.protocol = PLAINTEXT
[33mkafka-ui           |[0m 	security.providers = null
[33mkafka-ui           |[0m 	send.buffer.bytes = 131072
[33mkafka-ui           |[0m 	session.timeout.ms = 10000
[33mkafka-ui           |[0m 	ssl.cipher.suites = null
[33mkafka-ui           |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mkafka-ui           |[0m 	ssl.endpoint.identification.algorithm = https
[33mkafka-ui           |[0m 	ssl.key.password = null
[33mkafka-ui           |[0m 	ssl.keymanager.algorithm = SunX509
[33mkafka-ui           |[0m 	ssl.keystore.location = null
[33mkafka-ui           |[0m 	ssl.keystore.password = null
[33mkafka-ui           |[0m 	ssl.keystore.type = JKS
[33mkafka-ui           |[0m 	ssl.protocol = TLS
[33mkafka-ui           |[0m 	ssl.provider = null
[33mkafka-ui           |[0m 	ssl.secure.random.implementation = null
[33mkafka-ui           |[0m 	ssl.trustmanager.algorithm = PKIX
[33mkafka-ui           |[0m 	ssl.truststore.location = null
[33mkafka-ui           |[0m 	ssl.truststore.password = null
[33mkafka-ui           |[0m 	ssl.truststore.type = JKS
[33mkafka-ui           |[0m 	value.deserializer = class org.apache.kafka.common.serialization.BytesDeserializer
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m 15:46:51.106 [boundedElastic-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.4.0
[33mkafka-ui           |[0m 15:46:51.106 [boundedElastic-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 77a89fcf8d7fa018
[33mkafka-ui           |[0m 15:46:51.106 [boundedElastic-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021611106
[33mkafka-ui           |[0m 15:46:51.119 [boundedElastic-3] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=kafka-ui, groupId=null] Cluster ID: 3-wYX40qRXev0Kr60NxjUw
[33mkafka-ui           |[0m 15:46:51.139 [boundedElastic-3] DEBUG org.springframework.http.codec.json.Jackson2JsonEncoder - [16ec81a4] Encoding [class TopicDetails {
[33mkafka-ui           |[0m     name: mundo_topics_v2
[33mkafka-ui           |[0m     internal: false
[33mkafka-ui           |[0m     partitions: [class Partition  (truncated)...]
[32mkafka1             |[0m [2021-01-30 15:46:51,237] INFO Creating topic _confluent-monitoring with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:51,239] INFO Creating topic _confluent-monitoring with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:51,240] INFO Creating topic _confluent-monitoring with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1)) (kafka.zk.AdminZkClient)
[32mkafka1             |[0m [2021-01-30 15:46:51,248] INFO [KafkaApi-1] Auto creation of topic _confluent-monitoring with 1 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[36mconnect            |[0m [2021-01-30 15:46:51,250] WARN [Producer clientId=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-2] Error while fetching metadata with correlation id 3 : {_confluent-monitoring=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[36mconnect            |[0m [2021-01-30 15:46:51,251] WARN [Producer clientId=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-0] Error while fetching metadata with correlation id 3 : {_confluent-monitoring=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[32mkafka1             |[0m [2021-01-30 15:46:51,267] INFO [Controller id=1] New topics: [HashSet(_confluent-monitoring)], deleted topics: [HashSet()], new partition replica assignment [Map(_confluent-monitoring-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=))] (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:51,268] INFO [Controller id=1] New partition creation callback for _confluent-monitoring-0 (kafka.controller.KafkaController)
[32mkafka1             |[0m [2021-01-30 15:46:51,268] INFO [Controller id=1 epoch=1] Changed partition _confluent-monitoring-0 state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,268] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,270] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-monitoring-0 from NonExistentReplica to NewReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,270] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[36mconnect            |[0m [2021-01-30 15:46:51,273] WARN [Producer clientId=confluent.monitoring.interceptor.connector-producer-RssSourceConnectorSportv2-1] Error while fetching metadata with correlation id 3 : {_confluent-monitoring=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[32mkafka1             |[0m [2021-01-30 15:46:51,283] INFO [Controller id=1 epoch=1] Changed partition _confluent-monitoring-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isr=List(1), zkVersion=0) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,283] TRACE [Controller id=1 epoch=1] Sending become-leader LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-monitoring', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) to broker 1 for partition _confluent-monitoring-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,283] INFO [Controller id=1 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,284] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,285] TRACE [Controller id=1 epoch=1] Changed state of replica 1 for partition _confluent-monitoring-0 from NewReplica to OnlineReplica (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,285] INFO [Controller id=1 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,287] INFO [Broker id=1] Handling LeaderAndIsr request correlationId 15 from controller 1 for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,287] TRACE [Broker id=1] Received LeaderAndIsr request LeaderAndIsrPartitionState(topicName='_confluent-monitoring', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], addingReplicas=[], removingReplicas=[], isNew=true) correlation id 15 from controller 1 epoch 1 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,289] TRACE [Broker id=1] Handling LeaderAndIsr request correlationId 15 from controller 1 epoch 1 starting the become-leader transition for partition _confluent-monitoring-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,289] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-monitoring-0) (kafka.server.ReplicaFetcherManager)
[32mkafka1             |[0m [2021-01-30 15:46:51,289] INFO [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 15 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,296] INFO [Log partition=_confluent-monitoring-0, dir=/var/lib/kafka/data] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka1             |[0m [2021-01-30 15:46:51,297] INFO Created log for partition _confluent-monitoring-0 in /var/lib/kafka/data/_confluent-monitoring-0 with properties {compression.type -> producer, min.insync.replicas -> 1, message.downconversion.enable -> true, segment.jitter.ms -> 0, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, retention.ms -> 604800000, segment.bytes -> 1073741824, flush.messages -> 9223372036854775807, message.format.version -> 2.6-IV0, max.compaction.lag.ms -> 9223372036854775807, file.delete.delay.ms -> 60000, max.message.bytes -> 1048588, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, preallocate -> false, index.interval.bytes -> 4096, min.cleanable.dirty.ratio -> 0.5, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, segment.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760}. (kafka.log.LogManager)
[32mkafka1             |[0m [2021-01-30 15:46:51,300] INFO [Partition _confluent-monitoring-0 broker=1] No checkpointed highwatermark is found for partition _confluent-monitoring-0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:51,300] INFO [Partition _confluent-monitoring-0 broker=1] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0 (kafka.cluster.Partition)
[32mkafka1             |[0m [2021-01-30 15:46:51,300] INFO [Broker id=1] Leader _confluent-monitoring-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [1] addingReplicas [] removingReplicas []. Previous leader epoch was -1. (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,303] TRACE [Broker id=1] Completed LeaderAndIsr request correlationId 15 from controller 1 epoch 1 for the become-leader transition for partition _confluent-monitoring-0 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,305] TRACE [Controller id=1 epoch=1] Received response {error_code=0,partition_errors=[{topic_name=_confluent-monitoring,partition_index=0,error_code=0,_tagged_fields={}}],_tagged_fields={}} for request LEADER_AND_ISR with correlation id 15 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,307] TRACE [Broker id=1] Cached leader info UpdateMetadataPartitionState(topicName='_confluent-monitoring', partitionIndex=0, controllerEpoch=1, leader=1, leaderEpoch=0, isr=[1], zkVersion=0, replicas=[1], offlineReplicas=[]) for partition _confluent-monitoring-0 in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,307] INFO [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 16 (state.change.logger)
[32mkafka1             |[0m [2021-01-30 15:46:51,308] TRACE [Controller id=1 epoch=1] Received response {error_code=0,_tagged_fields={}} for request UPDATE_METADATA with correlation id 16 sent to broker kafka1:9092 (id: 1 rack: null) (state.change.logger)
[33mkafka-ui           |[0m 15:46:52.362 [boundedElastic-2] INFO  org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
[33mkafka-ui           |[0m 	allow.auto.create.topics = true
[33mkafka-ui           |[0m 	auto.commit.interval.ms = 5000
[33mkafka-ui           |[0m 	auto.offset.reset = earliest
[33mkafka-ui           |[0m 	bootstrap.servers = [kafka1:9092]
[33mkafka-ui           |[0m 	check.crcs = true
[33mkafka-ui           |[0m 	client.dns.lookup = default
[33mkafka-ui           |[0m 	client.id = kafka-ui
[33mkafka-ui           |[0m 	client.rack = 
[33mkafka-ui           |[0m 	connections.max.idle.ms = 540000
[33mkafka-ui           |[0m 	default.api.timeout.ms = 60000
[33mkafka-ui           |[0m 	enable.auto.commit = true
[33mkafka-ui           |[0m 	exclude.internal.topics = true
[33mkafka-ui           |[0m 	fetch.max.bytes = 52428800
[33mkafka-ui           |[0m 	fetch.max.wait.ms = 500
[33mkafka-ui           |[0m 	fetch.min.bytes = 1
[33mkafka-ui           |[0m 	group.id = null
[33mkafka-ui           |[0m 	group.instance.id = null
[33mkafka-ui           |[0m 	heartbeat.interval.ms = 3000
[33mkafka-ui           |[0m 	interceptor.classes = []
[33mkafka-ui           |[0m 	internal.leave.group.on.close = true
[33mkafka-ui           |[0m 	isolation.level = read_uncommitted
[33mkafka-ui           |[0m 	key.deserializer = class org.apache.kafka.common.serialization.BytesDeserializer
[33mkafka-ui           |[0m 	max.partition.fetch.bytes = 1048576
[33mkafka-ui           |[0m 	max.poll.interval.ms = 300000
[33mkafka-ui           |[0m 	max.poll.records = 500
[33mkafka-ui           |[0m 	metadata.max.age.ms = 300000
[33mkafka-ui           |[0m 	metric.reporters = []
[33mkafka-ui           |[0m 	metrics.num.samples = 2
[33mkafka-ui           |[0m 	metrics.recording.level = INFO
[33mkafka-ui           |[0m 	metrics.sample.window.ms = 30000
[33mkafka-ui           |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[33mkafka-ui           |[0m 	receive.buffer.bytes = 65536
[33mkafka-ui           |[0m 	reconnect.backoff.max.ms = 1000
[33mkafka-ui           |[0m 	reconnect.backoff.ms = 50
[33mkafka-ui           |[0m 	request.timeout.ms = 30000
[33mkafka-ui           |[0m 	retry.backoff.ms = 100
[33mkafka-ui           |[0m 	sasl.client.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.jaas.config = null
[33mkafka-ui           |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mkafka-ui           |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mkafka-ui           |[0m 	sasl.kerberos.service.name = null
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.login.class = null
[33mkafka-ui           |[0m 	sasl.login.refresh.buffer.seconds = 300
[33mkafka-ui           |[0m 	sasl.login.refresh.min.period.seconds = 60
[33mkafka-ui           |[0m 	sasl.login.refresh.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.refresh.window.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.mechanism = GSSAPI
[33mkafka-ui           |[0m 	security.protocol = PLAINTEXT
[33mkafka-ui           |[0m 	security.providers = null
[33mkafka-ui           |[0m 	send.buffer.bytes = 131072
[33mkafka-ui           |[0m 	session.timeout.ms = 10000
[33mkafka-ui           |[0m 	ssl.cipher.suites = null
[33mkafka-ui           |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mkafka-ui           |[0m 	ssl.endpoint.identification.algorithm = https
[33mkafka-ui           |[0m 	ssl.key.password = null
[33mkafka-ui           |[0m 	ssl.keymanager.algorithm = SunX509
[33mkafka-ui           |[0m 	ssl.keystore.location = null
[33mkafka-ui           |[0m 	ssl.keystore.password = null
[33mkafka-ui           |[0m 	ssl.keystore.type = JKS
[33mkafka-ui           |[0m 	ssl.protocol = TLS
[33mkafka-ui           |[0m 	ssl.provider = null
[33mkafka-ui           |[0m 	ssl.secure.random.implementation = null
[33mkafka-ui           |[0m 	ssl.trustmanager.algorithm = PKIX
[33mkafka-ui           |[0m 	ssl.truststore.location = null
[33mkafka-ui           |[0m 	ssl.truststore.password = null
[33mkafka-ui           |[0m 	ssl.truststore.type = JKS
[33mkafka-ui           |[0m 	value.deserializer = class org.apache.kafka.common.serialization.BytesDeserializer
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m 15:46:52.372 [boundedElastic-2] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.4.0
[33mkafka-ui           |[0m 15:46:52.372 [boundedElastic-2] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 77a89fcf8d7fa018
[33mkafka-ui           |[0m 15:46:52.372 [boundedElastic-2] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021612372
[33mkafka-ui           |[0m 15:46:52.376 [boundedElastic-2] INFO  org.apache.kafka.clients.consumer.KafkaConsumer - [Consumer clientId=kafka-ui, groupId=null] Subscribed to partition(s): mundo_topics_v2-0
[33mkafka-ui           |[0m 15:46:52.384 [boundedElastic-2] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=kafka-ui, groupId=null] Seeking to EARLIEST offset of partition mundo_topics_v2-0
[33mkafka-ui           |[0m 15:46:52.386 [boundedElastic-2] INFO  com.provectus.kafka.ui.cluster.service.ConsumingService - assignment: [mundo_topics_v2-0]
[33mkafka-ui           |[0m 15:46:52.409 [boundedElastic-2] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=kafka-ui, groupId=null] Cluster ID: 3-wYX40qRXev0Kr60NxjUw
[33mkafka-ui           |[0m 15:46:52.422 [boundedElastic-2] INFO  org.apache.kafka.clients.consumer.internals.SubscriptionState - [Consumer clientId=kafka-ui, groupId=null] Resetting offset for partition mundo_topics_v2-0 to offset 0.
[33mkafka-ui           |[0m 15:46:53.388 [boundedElastic-2] INFO  com.provectus.kafka.ui.cluster.service.ConsumingService - 0 records polled
[33mkafka-ui           |[0m 15:46:54.392 [boundedElastic-2] INFO  com.provectus.kafka.ui.cluster.service.ConsumingService - 0 records polled
[33mkafka-ui           |[0m 15:46:55.393 [boundedElastic-2] INFO  com.provectus.kafka.ui.cluster.service.ConsumingService - 0 records polled
[33mkafka-ui           |[0m 15:46:56.396 [boundedElastic-2] INFO  com.provectus.kafka.ui.cluster.service.ConsumingService - 0 records polled
[33mkafka-ui           |[0m 15:46:57.398 [boundedElastic-2] INFO  com.provectus.kafka.ui.cluster.service.ConsumingService - 0 records polled
[33mkafka-ui           |[0m 15:46:57.398 [boundedElastic-2] DEBUG org.springframework.http.codec.json.Jackson2JsonEncoder - [16ec81a4] Encoding [[]]
[32mkafka1             |[0m [2021-01-30 15:47:10,416] INFO [GroupCoordinator 1]: Member cons-string-id2-51aab293-671c-438d-851b-c743b7fab175 in group group-string-id2 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:47:10,417] INFO [GroupCoordinator 1]: Preparing to rebalance group group-string-id2 in state PreparingRebalance with old generation 1 (__consumer_offsets-17) (reason: removing member cons-string-id2-51aab293-671c-438d-851b-c743b7fab175 on heartbeat expiration) (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:47:10,420] INFO [GroupCoordinator 1]: Member cons-string-id2-36da344c-20f5-4a2e-8cfc-663897445d0a in group group-string-id2 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:47:10,432] INFO [GroupCoordinator 1]: Member cons-string-id2-615ce07a-e262-4377-9069-840914dced6e in group group-string-id2 has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[32mkafka1             |[0m [2021-01-30 15:47:10,433] INFO [GroupCoordinator 1]: Group group-string-id2 with generation 2 is now empty (__consumer_offsets-17) (kafka.coordinator.group.GroupCoordinator)
[33mkafka-ui           |[0m 15:47:10.745 [parallel-3] DEBUG com.provectus.kafka.ui.cluster.service.MetricsUpdateService - Start getting metrics for kafkaCluster: Broker1
[33mkafka-ui           |[0m 15:47:10.747 [parallel-3] INFO  org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
[33mkafka-ui           |[0m 	bootstrap.servers = [kafka1:9092]
[33mkafka-ui           |[0m 	client.dns.lookup = default
[33mkafka-ui           |[0m 	client.id = 
[33mkafka-ui           |[0m 	connections.max.idle.ms = 300000
[33mkafka-ui           |[0m 	metadata.max.age.ms = 300000
[33mkafka-ui           |[0m 	metric.reporters = []
[33mkafka-ui           |[0m 	metrics.num.samples = 2
[33mkafka-ui           |[0m 	metrics.recording.level = INFO
[33mkafka-ui           |[0m 	metrics.sample.window.ms = 30000
[33mkafka-ui           |[0m 	receive.buffer.bytes = 65536
[33mkafka-ui           |[0m 	reconnect.backoff.max.ms = 1000
[33mkafka-ui           |[0m 	reconnect.backoff.ms = 50
[33mkafka-ui           |[0m 	request.timeout.ms = 5000
[33mkafka-ui           |[0m 	retries = 5
[33mkafka-ui           |[0m 	retry.backoff.ms = 100
[33mkafka-ui           |[0m 	sasl.client.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.jaas.config = null
[33mkafka-ui           |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mkafka-ui           |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mkafka-ui           |[0m 	sasl.kerberos.service.name = null
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.login.class = null
[33mkafka-ui           |[0m 	sasl.login.refresh.buffer.seconds = 300
[33mkafka-ui           |[0m 	sasl.login.refresh.min.period.seconds = 60
[33mkafka-ui           |[0m 	sasl.login.refresh.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.refresh.window.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.mechanism = GSSAPI
[33mkafka-ui           |[0m 	security.protocol = PLAINTEXT
[33mkafka-ui           |[0m 	security.providers = null
[33mkafka-ui           |[0m 	send.buffer.bytes = 131072
[33mkafka-ui           |[0m 	ssl.cipher.suites = null
[33mkafka-ui           |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mkafka-ui           |[0m 	ssl.endpoint.identification.algorithm = https
[33mkafka-ui           |[0m 	ssl.key.password = null
[33mkafka-ui           |[0m 	ssl.keymanager.algorithm = SunX509
[33mkafka-ui           |[0m 	ssl.keystore.location = null
[33mkafka-ui           |[0m 	ssl.keystore.password = null
[33mkafka-ui           |[0m 	ssl.keystore.type = JKS
[33mkafka-ui           |[0m 	ssl.protocol = TLS
[33mkafka-ui           |[0m 	ssl.provider = null
[33mkafka-ui           |[0m 	ssl.secure.random.implementation = null
[33mkafka-ui           |[0m 	ssl.trustmanager.algorithm = PKIX
[33mkafka-ui           |[0m 	ssl.truststore.location = null
[33mkafka-ui           |[0m 	ssl.truststore.password = null
[33mkafka-ui           |[0m 	ssl.truststore.type = JKS
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m 15:47:10.755 [parallel-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.4.0
[33mkafka-ui           |[0m 15:47:10.755 [parallel-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 77a89fcf8d7fa018
[33mkafka-ui           |[0m 15:47:10.755 [parallel-3] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021630754
[33mkafka-ui           |[0m 15:47:10.819 [ZkClient-EventThread-111-null] INFO  org.I0Itec.zkclient.ZkEventThread - Starting ZkClient event thread.
[33mkafka-ui           |[0m 15:47:10.820 [kafka-admin-client-thread | adminclient-1] ERROR com.provectus.kafka.ui.zookeeper.ZookeeperService - Error while creating zookeeper client for cluster Broker1
[33mkafka-ui           |[0m 15:47:10.820 [kafka-admin-client-thread | adminclient-1] DEBUG com.provectus.kafka.ui.zookeeper.ZookeeperService - Start getting Zookeeper metrics for kafkaCluster: Broker1
[34mzookeeper          |[0m [2021-01-30 15:47:19,503] INFO Processing ruok command from /127.0.0.1:50764 (org.apache.zookeeper.server.NIOServerCnxn)
[35mschema-registry    |[0m [2021-01-30 15:47:21,001] INFO 127.0.0.1 - - [30/Jan/2021:15:47:20 +0000] "GET / HTTP/1.1" 200 2  4 (io.confluent.rest-utils.requests)
[36mconnect            |[0m [2021-01-30 15:47:34,031] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-2} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,032] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-2} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,050] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-2} Finished commitOffsets successfully in 19 ms (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,051] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-0} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,052] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-0} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,058] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-0} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,058] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-1} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,059] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-1} flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask)
[36mconnect            |[0m [2021-01-30 15:47:34,065] INFO WorkerSourceTask{id=RssSourceConnectorSportv2-1} Finished commitOffsets successfully in 7 ms (org.apache.kafka.connect.runtime.WorkerSourceTask)
[33mkafka-ui           |[0m 15:47:40.712 [parallel-4] DEBUG com.provectus.kafka.ui.cluster.service.MetricsUpdateService - Start getting metrics for kafkaCluster: Broker1
[33mkafka-ui           |[0m 15:47:40.712 [parallel-4] INFO  org.apache.kafka.clients.admin.AdminClientConfig - AdminClientConfig values: 
[33mkafka-ui           |[0m 	bootstrap.servers = [kafka1:9092]
[33mkafka-ui           |[0m 	client.dns.lookup = default
[33mkafka-ui           |[0m 	client.id = 
[33mkafka-ui           |[0m 	connections.max.idle.ms = 300000
[33mkafka-ui           |[0m 	metadata.max.age.ms = 300000
[33mkafka-ui           |[0m 	metric.reporters = []
[33mkafka-ui           |[0m 	metrics.num.samples = 2
[33mkafka-ui           |[0m 	metrics.recording.level = INFO
[33mkafka-ui           |[0m 	metrics.sample.window.ms = 30000
[33mkafka-ui           |[0m 	receive.buffer.bytes = 65536
[33mkafka-ui           |[0m 	reconnect.backoff.max.ms = 1000
[33mkafka-ui           |[0m 	reconnect.backoff.ms = 50
[33mkafka-ui           |[0m 	request.timeout.ms = 5000
[33mkafka-ui           |[0m 	retries = 5
[33mkafka-ui           |[0m 	retry.backoff.ms = 100
[33mkafka-ui           |[0m 	sasl.client.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.jaas.config = null
[33mkafka-ui           |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[33mkafka-ui           |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[33mkafka-ui           |[0m 	sasl.kerberos.service.name = null
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.callback.handler.class = null
[33mkafka-ui           |[0m 	sasl.login.class = null
[33mkafka-ui           |[0m 	sasl.login.refresh.buffer.seconds = 300
[33mkafka-ui           |[0m 	sasl.login.refresh.min.period.seconds = 60
[33mkafka-ui           |[0m 	sasl.login.refresh.window.factor = 0.8
[33mkafka-ui           |[0m 	sasl.login.refresh.window.jitter = 0.05
[33mkafka-ui           |[0m 	sasl.mechanism = GSSAPI
[33mkafka-ui           |[0m 	security.protocol = PLAINTEXT
[33mkafka-ui           |[0m 	security.providers = null
[33mkafka-ui           |[0m 	send.buffer.bytes = 131072
[33mkafka-ui           |[0m 	ssl.cipher.suites = null
[33mkafka-ui           |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[33mkafka-ui           |[0m 	ssl.endpoint.identification.algorithm = https
[33mkafka-ui           |[0m 	ssl.key.password = null
[33mkafka-ui           |[0m 	ssl.keymanager.algorithm = SunX509
[33mkafka-ui           |[0m 	ssl.keystore.location = null
[33mkafka-ui           |[0m 	ssl.keystore.password = null
[33mkafka-ui           |[0m 	ssl.keystore.type = JKS
[33mkafka-ui           |[0m 	ssl.protocol = TLS
[33mkafka-ui           |[0m 	ssl.provider = null
[33mkafka-ui           |[0m 	ssl.secure.random.implementation = null
[33mkafka-ui           |[0m 	ssl.trustmanager.algorithm = PKIX
[33mkafka-ui           |[0m 	ssl.truststore.location = null
[33mkafka-ui           |[0m 	ssl.truststore.password = null
[33mkafka-ui           |[0m 	ssl.truststore.type = JKS
[33mkafka-ui           |[0m 
[33mkafka-ui           |[0m 15:47:40.720 [parallel-4] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.4.0
[33mkafka-ui           |[0m 15:47:40.720 [parallel-4] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 77a89fcf8d7fa018
[33mkafka-ui           |[0m 15:47:40.720 [parallel-4] INFO  org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1612021660719
[33mkafka-ui           |[0m 15:47:40.795 [ZkClient-EventThread-114-null] INFO  org.I0Itec.zkclient.ZkEventThread - Starting ZkClient event thread.
[33mkafka-ui           |[0m 15:47:40.796 [kafka-admin-client-thread | adminclient-1] ERROR com.provectus.kafka.ui.zookeeper.ZookeeperService - Error while creating zookeeper client for cluster Broker1
[33mkafka-ui           |[0m 15:47:40.797 [kafka-admin-client-thread | adminclient-1] DEBUG com.provectus.kafka.ui.zookeeper.ZookeeperService - Start getting Zookeeper metrics for kafkaCluster: Broker1
[34mzookeeper          |[0m [2021-01-30 15:47:49,534] INFO Processing ruok command from /127.0.0.1:50798 (org.apache.zookeeper.server.NIOServerCnxn)
[35mschema-registry    |[0m [2021-01-30 15:47:51,063] INFO 127.0.0.1 - - [30/Jan/2021:15:47:51 +0000] "GET / HTTP/1.1" 200 2  4 (io.confluent.rest-utils.requests)
Stopping connect         ... 
Stopping kafka-ui        ... 
Stopping schema-registry ... 
Stopping kafka1          ... 
Stopping zookeeper       ... 
Killing connect          ... 
Killing kafka-ui         ... 
Killing schema-registry  ... 
Killing kafka1           ... 
Killing zookeeper        ... 
Aborting.
Gracefully stopping... (press Ctrl+C again to force)
